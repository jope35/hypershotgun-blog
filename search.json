[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hyper-shotgun",
    "section": "",
    "text": "intro to partial dependence plots\n\n\nvisualizing model relationships\n\n\n \n\n\n\n\n\n2024-03-14\n\n\nJoost de Theije + LLM\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nbaby steps: tuneing big model one step at a time\n\n\nstepwise tuning of gradient boosting models\n\n\n\n\n\n\n\n\n2024-03-14\n\n\nJoost de Theije + LLM\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nBoring linear forecast\n\n\nImproving Performance by Adding Some Dummies\n\n\nDiscover how the simple addition of dummy variables can transform a linear forecast from mundane to insightful. This article explores the significant impact of encoding seasonality into your models and provides a step-by-step guide on improving forecast accuracy.\n\n\n\n\n\n2023-03-5\n\n\nJoost de Theije + LLM\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html",
    "href": "posts/boring_forecast/boring_linear_forecast.html",
    "title": "Boring linear forecast",
    "section": "",
    "text": "Linear regression in itself is not performant for longer time-scales\nMost systems have some capabilities for linear regression built-in\nAdding dummy variables for datetime features(i.e. month, weekday etc.) adds predictive power"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#tldr",
    "href": "posts/boring_forecast/boring_linear_forecast.html#tldr",
    "title": "Boring linear forecast",
    "section": "",
    "text": "Linear regression in itself is not performant for longer time-scales\nMost systems have some capabilities for linear regression built-in\nAdding dummy variables for datetime features(i.e. month, weekday etc.) adds predictive power"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#introduction",
    "href": "posts/boring_forecast/boring_linear_forecast.html#introduction",
    "title": "Boring linear forecast",
    "section": "2 Introduction",
    "text": "2 Introduction\nLinear regression is a fundamental statistical model for determining linear relationships between variables, commonly used in Economics, Finance, and Social Sciences. Despite being considered basic, its ability to estimate direction and magnitude makes it valuable. The popularity of linear regression arises from its widespread availability, affordability, and ease of identifying optimal parameters.\nFor time series forecasting, dummy variables can be employed to expand the model’s capabilities. These dummies offer insights into temporal relationships by capturing seasonality and identifying one-off events such as price reductions or natural disasters. For instance, we can create dummies for specific datetime features like month or weekday/weekend status."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#imports",
    "href": "posts/boring_forecast/boring_linear_forecast.html#imports",
    "title": "Boring linear forecast",
    "section": "3 Imports",
    "text": "3 Imports\nFirst we import all the libraries, the default data science libs and the linear model and metrics from sklearn.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#reading-in-the-data",
    "href": "posts/boring_forecast/boring_linear_forecast.html#reading-in-the-data",
    "title": "Boring linear forecast",
    "section": "4 Reading in the data",
    "text": "4 Reading in the data\nFor this example, we’ll utilize the dataset from the Prophet package. The dataset is described in the Prophet docs as follows:\n\nAs an illustration, let’s examine a time series of the log daily page views for the Wikipedia page about Peyton Manning. We obtained this data using the Wikipedia trend package in R. Peyton Manning offers an excellent example because it demonstrates some of Prophet’s features such as multiple seasonality, changing growth rates, and the ability to model special days (like his playoff and Superbowl appearances).\n\nLink to the documentation\n\n\nCode\ndf_in = pd.read_csv(\n    \"https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv\"\n)\ndf_in = df_in.assign(ds=pd.to_datetime(df_in[\"ds\"]))\ndf_in = df_in[(df_in[\"ds\"] &gt; \"2012\")]  # selecting data after 2012\n\n\nTo gain a better understanding of our dataset, let’s visualize it over time and identify any observable trends or patterns. This visualization provides insights into the patterns and trends in the data, which we can then try to incorporate into our model using seasonal dummies.\n\n\nCode\nplt.plot_date(\n    x=df_in[\"ds\"],\n    y=df_in[\"y\"],\n    label=\"input timeseries\",\n    fmt=\"-\",\n)\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"target variable - $y$\")\nplt.title(\"daily visits ot Peyton Manning wiki on a daily basis (log)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nI’ve selected data spanning from 2012 for analyzing seasonal dummies. The initial observation is that there exists a dip around months 6 and 7. Early August hosts NFL exhibition games, preceding the start of the regular football season.\nAlso, we can observe a pattern over the year, it starts high then dips, and then and high again. This can be seen for the other years as well, so there is some repeating seasonality. Let us continue and train our first models. Starting with a simple ordinary linear regression and then adding dummies to see if they improve the performance of the model."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#train-test-split",
    "href": "posts/boring_forecast/boring_linear_forecast.html#train-test-split",
    "title": "Boring linear forecast",
    "section": "5 Train-test split",
    "text": "5 Train-test split\n\n\nCode\n# train test split\ndf_train = df_in[(df_in[\"ds\"] &gt; \"2012\") & (df_in[\"ds\"] &lt; \"2015\")]\ndf_test = df_in[(df_in[\"ds\"] &gt; \"2015\")]\n\n\nTo evaluate the performance of the model, let’s split the dataset into two parts: training data (from 2012 to 2015) and testing data (everything after 2015). The model will only be exposed to the training data and expected to generate predictions for the testing data. Once the predictions are obtained, we’ll calculate the performance using these predictions and the true observations.\n\n\nCode\n# visually inspect the train test split\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\"data is splitted, everything before 2015 is train data after 2015 test\")\nplt.show()"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#setting-up-the-regression",
    "href": "posts/boring_forecast/boring_linear_forecast.html#setting-up-the-regression",
    "title": "Boring linear forecast",
    "section": "6 Setting up the regression",
    "text": "6 Setting up the regression\n\n\nCode\nX_train = df_train[\"ds\"].astype(int).to_numpy().reshape(-1, 1)\ny_train = df_train[\"y\"].to_numpy()\n\nX_test = df_test[\"ds\"].astype(int).to_numpy().reshape(-1, 1)\ny_test = df_test[\"y\"].to_numpy()\n\n\nPreprocessing the data to prepare it for fitting the linear model: In this instance, we convert the date columns into a sequence of ever-increasing integers.\n\n\nCode\n# creating, fit, and inference\nlinear = LinearRegression()\nlinear.fit(X=X_train, y=y_train)\ny_pred = linear.predict(X=X_test)\n\n\nThe linear model is swift to fit, taking approximately 4 milliseconds on my machine. Given the modest data size (though it’s significant for time series), this enables us to fit 1000 models within the span of a single coffee sip☕️.\nLet’s visualize the model results by plotting all relevant components: train data, test data, and predictions. Additionally, we will calculate two error metrics – Mean Squared Error (mse) and Mean Absolute Error (mae) – to quantify the performance of the model.\n\n\nCode\n# calc error metrics\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\n\n# visually inspect the prediction\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred,\n    label=\"prediction\",\n    fmt=\"--\",\n)\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(f\"linear regression applied (MSE= {mse:.3}, MAE={mae:.3})\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe linear model is represented by a plain straight line, as expected from a simple linear regression model. The green line passes through the orange data points roughly, moving in an intuitively correct direction. However, it fails to capture seasonality or other patterns present in the training set. With Mean Squared Error (MSE) and Mean Absolute Error (MAE) values around 0.6, we’ll use this as a baseline for further improvement by incorporating dummies.\n\n\nmean squared error  = 0.617\nmean absolute error = 0.615"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#adding-dummies",
    "href": "posts/boring_forecast/boring_linear_forecast.html#adding-dummies",
    "title": "Boring linear forecast",
    "section": "7 Adding dummies",
    "text": "7 Adding dummies\nLet’s introduce dummy variables for the months to see whether we can enhance our model’s performance, both visually and numerically. We’ll create a separate column for each month, where a 1 indicates the occurrence of that month, and a 0 signifies its absence. By doing so, we embed temporal information directly into the model, allowing it to discern the specific impacts of each month and adjust future predictions accordingly. That’s the theory—now, let’s put it to the test!\n\n\nCode\n# creating dummies for the months\ndf_dummies = df_in.assign(\n    month=df_in[\"ds\"].dt.month.astype(\"category\"),\n    ds_int=df_in[\"ds\"].astype(int),\n)\n\nnot_dummy = {\"y\", \"ds\", \"ds_int\"}\nto_dummy = list(set(df_dummies.columns) - not_dummy)\n\ndf_dummies = pd.get_dummies(data=df_dummies, columns=to_dummy)\nall_features = list(set(df_dummies.columns) - {\"y\", \"ds\"})\n\n# slicing the input in train test\ndf_train_dummies = df_dummies[(df_dummies[\"ds\"] &gt; \"2012\") & (df_dummies[\"ds\"] &lt; \"2015\")]\ndf_test_dummies = df_dummies[(df_dummies[\"ds\"] &gt; \"2015\")]\n\nX_train = df_train_dummies.loc[:, all_features]\ny_train = df_train_dummies[[\"y\"]]\n\nX_test = df_test_dummies.loc[:, all_features]\ny_test = df_test_dummies[[\"y\"]]\n\ndf_dummies.drop(columns=[\"ds_int\", \"y\"]).sample(\n    n=6,\n    ignore_index=True,\n    random_state=42,\n)\n\n\n\n\n\n\n\n\n\n\nds\nmonth_1\nmonth_2\nmonth_3\nmonth_4\nmonth_5\nmonth_6\nmonth_7\nmonth_8\nmonth_9\nmonth_10\nmonth_11\nmonth_12\n\n\n\n\n0\n2014-05-06\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n2012-07-05\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n2015-06-21\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n2012-03-09\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n2012-08-10\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n5\n2013-05-11\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\nWe’ve added month dummies to our data—month_1 for January through month_12 for December. Next, we’ll train our model with these dummies and compare performance against the original model without them.\n\n\nCode\n# create the pipeline and fit pipeline\n# scaler is there so that the coefs can be interpeted later\n# pipeline = make_pipeline(StandardScaler(), LinearRegression())\npipeline = make_pipeline(MinMaxScaler(), LinearRegression())\n\npipeline.fit(X=X_train, y=y_train)\ny_pred_dummies = pipeline.predict(X=X_test)\n\nmse_dummies = mean_squared_error(y_test, y_pred_dummies)\nmae_dummies = mean_absolute_error(y_test, y_pred_dummies)\n\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred,\n    label=\"prediction\",\n    fmt=\"--\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies,\n    label=\"prediction with dummies\",\n    fmt=\"--\",\n)\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\n    f\"linear regression with dummies applied (mse= {mse_dummies:.3}, mae={mae_dummies:.3})\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nVisually, our forecast has improved significantly. It now mirrors the ups and downs of the time series more closely and captures the overall trend more accurately. These enhancements are also evident in the error metrics, with the mean squared error improving by a factor of 1.89 and the mean absolute error by a factor of 1.54. Such substantial gains from simply incorporating ones and zeros are indeed impressive.\n\n\nCode\nprint(f\"mean squared error  = {mse_dummies:.3}\")\nprint(f\"improvement factor mse month dummies -&gt; {mse/mse_dummies:.3}x\")\n\nprint(\"-\" * 79)\n\nprint(f\"mean absolute error = {mae_dummies:.3}\")\nprint(f\"improvement factor mea month dummies -&gt; {mae/mae_dummies:.3}x\")\n\n\nmean squared error  = 0.323\nimprovement factor mse month dummies -&gt; 1.91x\n-------------------------------------------------------------------------------\nmean absolute error = 0.397\nimprovement factor mea month dummies -&gt; 1.55x"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#inspecting-the-seasonality",
    "href": "posts/boring_forecast/boring_linear_forecast.html#inspecting-the-seasonality",
    "title": "Boring linear forecast",
    "section": "8 Inspecting the seasonality",
    "text": "8 Inspecting the seasonality\nNow that we have encoded the information about the seasonality in the model, we can inspect that seasonality on its own. This will give us some insight into the inner workings of the underlying time series model. First, we access the coefficients of the linear model and put them into a separate DataFrame. Then, we need to scale them so that the relative difference is more apparent. Looking at the raw coefficients would not yield any meaningful information, as the scale is not relatable to the original problem.\n\n\nCode\n# pull coefs into a seperate df, to inspect the seasonality\nlin_reg_coefs = (\n    pd.DataFrame(\n        data=pipeline[\"linearregression\"].coef_,\n        columns=X_train.columns,\n    )\n    .T.reset_index()\n    .rename(columns={\"index\": \"month\", 0: \"coefficient\"})\n)\n# exclude the time col\nlin_reg_coefs = lin_reg_coefs[lin_reg_coefs[\"month\"] != \"ds_int\"]\n\n# subtract mean to get the relative difference between the coefs\nlin_reg_coefs[\"coefficient\"] = (\n    lin_reg_coefs[\"coefficient\"] - lin_reg_coefs[\"coefficient\"].mean()\n)\n\n\n\n\nCode\nchart = sns.barplot(\n    data=lin_reg_coefs,\n    x=\"month\",\n    y=\"coefficient\",\n    color=sns.color_palette()[0],\n    order=[f\"month_{i}\" for i in range(1, 13)],\n)\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"\")\nplt.title(\"yearly seasonality\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe have a clear picture of the seasonality throughout the year. There is a dip in the middle of the year, followed by a significant uptick in January. This can be attributed to the Super Bowl, which is played in the first week of February and drives a lot of traffic to the wiki page. The dip in the middle of the year is also evident at the month_6 mark."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#recap",
    "href": "posts/boring_forecast/boring_linear_forecast.html#recap",
    "title": "Boring linear forecast",
    "section": "9 Recap",
    "text": "9 Recap\nIn this blog post, I have demonstrated that the performance of a simple linear regression for time series forecasting can be improved by a factor of 1.54 up to 1.89 by simply adding dummy variables for the months. The nice thing about this approach is that the linear regression is available in most systems with analytical capabilities (yes, even in Excel), and adding the dummy variables is so simple that you can do it in a SQL server. The added benefit is that the model fitting is quick, allowing you to retrain the model monthly, weekly, daily, or even hourly."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#encore",
    "href": "posts/boring_forecast/boring_linear_forecast.html#encore",
    "title": "Boring linear forecast",
    "section": "10 Encore",
    "text": "10 Encore\nWhat if we were to dummies not just for the months, but also for other datetime features and really turn it up to eleven\n\nSure, let’s create the dummy variables for the given datetime features and then apply the Elastic Net linear model.\nFirst, let’s create the dummy variables for the datetime features:\n\nMonth: This feature will create 12 dummy variables, one for each month.\nWeek: This feature will create 52 dummy variables, one for each week of the year.\nDay of the Week: This feature will create 7 dummy variables, one for each day of the week.\nIs Weekend: This feature will create 2 dummy variables, one for weekdays and one for weekends.\nQuarter: This feature will create 4 dummy variables, one for each quarter of the year.\n\nBy creating these dummy variables, we will end up with a large number of features, which can lead to overfitting. This is where the Elastic Net linear model comes in handy.\nThe Elastic Net is a regularized linear regression model that combines the L1 (Lasso) and L2 (Ridge) regularization techniques. This helps to handle the large number of features and prevent overfitting. The Elastic Net model will automatically select the most important features and shrink the coefficients of the less important ones towards zero.\nAs an exercise, you can use the sklearn.linear_model.ElasticNetCV class to train the Elastic Net model on your dataset. This class will automatically tune the hyperparameters (alpha and l1_ratio) using cross-validation, which can help to find the optimal balance between the L1 and L2 regularization.\nRemember to split your dataset into training and testing sets, and evaluate the performance of the Elastic Net model on the test set. This will give you a good idea of how well the model can handle the large number of features created by the dummy variables.\n\n\nCode\n# creating dummies for the months\ndf_dummies_all = df_in.assign(\n    month=df_in[\"ds\"].dt.month.astype(\"category\"),\n    week=df_in[\"ds\"].dt.isocalendar().week.astype(\"category\"),\n    dayofweek=df_in[\"ds\"].dt.dayofweek.astype(\"category\"),\n    is_weekend=(df_in[\"ds\"].dt.dayofweek) &gt;= 5,\n    quarter=df_in[\"ds\"].dt.quarter.astype(\"category\"),\n    ds_int=df_in[\"ds\"].astype(int),\n)\n\nnot_dummy = {\"y\", \"ds\", \"ds_int\"}\nto_dummy = list(set(df_dummies_all.columns) - not_dummy)\n\ndf_dummies_all = pd.get_dummies(\n    data=df_dummies_all,\n    columns=to_dummy,\n    drop_first=True,  # reduce the amount of cols with no additional info\n)\nall_features = list(set(df_dummies_all.columns) - {\"y\", \"ds\"})\n\n# slicing the input in train test\ndf_train_dummies_all = df_dummies_all[\n    (df_dummies_all[\"ds\"] &gt; \"2012\") & (df_dummies_all[\"ds\"] &lt; \"2015\")\n]\ndf_test_dummies_all = df_dummies_all[(df_dummies_all[\"ds\"] &gt; \"2015\")]\n\nX_train = df_train_dummies_all.loc[:, all_features]\ny_train = df_train_dummies_all[[\"y\"]]\n\nX_test = df_test_dummies_all.loc[:, all_features]\ny_test = df_test_dummies_all[[\"y\"]]\n\ndf_dummies_all.drop(columns=[\"ds_int\", \"y\"]).sample(\n    n=6,\n    ignore_index=True,\n    random_state=42,\n)\n\n\n\n\n\n\n\n\n\n\nds\nquarter_2\nquarter_3\nquarter_4\ndayofweek_1\ndayofweek_2\ndayofweek_3\ndayofweek_4\ndayofweek_5\ndayofweek_6\n...\nmonth_3\nmonth_4\nmonth_5\nmonth_6\nmonth_7\nmonth_8\nmonth_9\nmonth_10\nmonth_11\nmonth_12\n\n\n\n\n0\n2014-05-06\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n2012-07-05\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n2015-06-21\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n2012-03-09\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n2012-08-10\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n5\n2013-05-11\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n6 rows × 74 columns\n\n\n\n\n\n\nCode\n# utilzing an elasticnet linear model to compensate for the amount of features\nelastic_params = {\n    \"l1_ratio\": np.linspace(start=0.000001, stop=1, num=100),\n    \"cv\": 7,\n    \"n_alphas\": 1_00,\n    \"n_jobs\": -1,\n}\n\npipeline_all = make_pipeline(MinMaxScaler(), ElasticNetCV(**elastic_params))\n\npipeline_all.fit(X=X_train, y=y_train.to_numpy().ravel())\ny_pred_dummies_all = pipeline_all.predict(X=X_test)\n\nmse_dummies_all = mean_squared_error(y_test, y_pred_dummies_all)\nmae_dummies_all = mean_absolute_error(y_test, y_pred_dummies_all)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n    alpha=0.7,\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies,\n    label=\"prediction with dummies\",\n    fmt=\"--\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies_all,\n    label=\"prediction with all dummies\",\n    fmt=\"--\",\n)\n\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\n    f\"linear regression with dummies applied (mse= {mse_dummies_all:.3}, mae={mae_dummies_all:.3})\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nImmediately, it becomes evident that the model captures more of the fine-grained movement of the time series. This is also reflected in the fact that both error metrics have improved.\n\n\nCode\nprint(f\"mean squared error = {mse_dummies_all:.3}\")\nprint(f\"improvement factor mse month dummies -&gt; {mse/mse_dummies:.3}x\")\nprint(f\"improvement factor mse all dummies   -&gt; {mse/mse_dummies_all:.3}x\")\nprint(\"-\" * 79)\n\nprint(f\"mean absolute error = {mae_dummies_all:.3}\")\nprint(f\"improvement factor mea month dummies -&gt; {mae/mae_dummies:.3}x\")\nprint(f\"improvement factor mea all dummies   -&gt; {mae/mae_dummies_all:.3}x\")\n\n\nmean squared error = 0.262\nimprovement factor mse month dummies -&gt; 1.91x\nimprovement factor mse all dummies   -&gt; 2.35x\n-------------------------------------------------------------------------------\nmean absolute error = 0.356\nimprovement factor mea month dummies -&gt; 1.55x\nimprovement factor mea all dummies   -&gt; 1.73x"
  },
  {
    "objectID": "posts/DPD/dependency.html",
    "href": "posts/DPD/dependency.html",
    "title": "intro to partial dependence plots",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nfrom sklearn.datasets import make_friedman1\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import PartialDependenceDisplay, partial_dependence\nCode\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nCode\n# setting global plotting settings\n# roudning all floats to two digits\npd.options.display.float_format = \"{:.2f}\".format\n\nset_matplotlib_formats(\"svg\")\nsns.set_context(context=\"notebook\", font_scale=1.5)\nsns.set_palette(\"tab10\")\nsns.set_style(\"darkgrid\")\nFIGSIZE = (12, 6)\nRANDOM_STATE = 35"
  },
  {
    "objectID": "posts/DPD/dependency.html#intro",
    "href": "posts/DPD/dependency.html#intro",
    "title": "intro to partial dependence plots",
    "section": "1 Intro",
    "text": "1 Intro\nToday, we’re examining partial dependence plots, a tool for visualizing the average influence of a feature on a model’s predictions.\n#TODO why would we want to see a partial dependece plot\nTo demonstrate this, we’ll utilize a dataset from Scikit-learn, which can be found here.\nThe formula generating the synthetic dataset is as follows: \\(y(X) = 10\\sin(\\pi \\cdot X_0 \\cdot X_1) + 20 \\cdot (X_2 - 0.5)^2 + 10 \\cdot X_3 + 5 \\cdot X_4 + \\text{noise} \\cdot N(0, 1)\\).\nI will generate a dataset comprising 7 features, but only 5 will actually influence the output—meaning the remaining two have no predictive value. the generate dataset contains 2000 samples and has a noise factor of 2.\n\n\nCode\nX_reg, y_reg = make_friedman1(\n    n_samples=2_000, n_features=7, noise=2, random_state=RANDOM_STATE\n)\n\n# stick it into a dataframe\ndf_reg = pd.concat(\n    [\n        pd.DataFrame(\n            data=X_reg, columns=[\"x_0\", \"x_1\", \"x_2\", \"x_3\", \"x_4\", \"x_5\", \"x_6\"]\n        ),\n        pd.DataFrame(data=y_reg, columns=[\"target\"]),\n    ],\n    axis=1,\n)\n\n# display descriptive stats\ndf_reg.describe().T\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nx_0\n2000.00\n0.49\n0.29\n0.00\n0.25\n0.49\n0.74\n1.00\n\n\nx_1\n2000.00\n0.50\n0.29\n0.00\n0.25\n0.51\n0.76\n1.00\n\n\nx_2\n2000.00\n0.50\n0.29\n0.00\n0.25\n0.52\n0.75\n1.00\n\n\nx_3\n2000.00\n0.50\n0.28\n0.00\n0.26\n0.51\n0.75\n1.00\n\n\nx_4\n2000.00\n0.50\n0.29\n0.00\n0.25\n0.50\n0.75\n1.00\n\n\nx_5\n2000.00\n0.50\n0.29\n0.00\n0.24\n0.49\n0.75\n1.00\n\n\nx_6\n2000.00\n0.50\n0.28\n0.00\n0.26\n0.51\n0.75\n1.00\n\n\ntarget\n2000.00\n14.48\n5.26\n-1.56\n10.72\n14.46\n18.15\n30.51\n\n\n\n\n\n\n\n\nall input features are uniform distributed \\(U(0,1)\\) -&gt; the mean is ~0.5 the target feature has a higher mean of ~14\nThe data is fed into a random-forest model, it is my favorite go to model, no feature scaling required can handle missing values, can capture non-linearity, has ability to model non-linearity, and fitting can be quite fast.\n\n\nCode\nX = df_reg.drop(columns=\"target\")\ny = df_reg[\"target\"]\n\nreg = RandomForestRegressor(\n    n_estimators=32,\n    max_depth=9,\n    min_samples_split=2,\n    random_state=42,\n)\n_ = reg.fit(X, y)\n\n\nto determine the partial dependence (PD) of each feature on the target we need to do some dataset wrangling. lets start with a (very very small) subset of our data taken at random.\n\n\nCode\ndf_sample = df_reg.sample(3, random_state=1)\ndf_sample\n\n\n\n\n\n\n\n\n\n\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\ntarget\n\n\n\n\n674\n0.36\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n\n\n1699\n0.32\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n\n\n1282\n0.31\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57"
  },
  {
    "objectID": "posts/DPD/dependency.html#how-to-calculate-the-pdp",
    "href": "posts/DPD/dependency.html#how-to-calculate-the-pdp",
    "title": "intro to partial dependence plots",
    "section": "2 how to calculate the PDP",
    "text": "2 how to calculate the PDP\nIn order to calculate the PD of lets say x_0 we would need to follow these steps: 1. determine the range of x_0 2. set a grid size, i.e the sampling ratio over the range 3. apply the grid over the data, this will increase the data size by a lot 4. predict with the model using the newly created dataset 5. average out the individual predictions to get a singular result\n\nrange of x_0 is between 0 and 1\nlets use a grid size of 7, that means that we create a list of 7 equaly space values between 0 and 1\nfor each sample in our dataset we apply the grid of 7 values which means that if we have 3 datapoints to begin we end up with 3*7=21 datapoints. that is an increase in data and for bigger data sets and finer grids this increases very fast\nnow we ask the model to make predictions, this is inference and not training so the computational cost is reasonable\nperform a group by on the sampled grid and apply a average aggregation function\nthe result is the partial dependence of x_0 and the target variable\n\n\n2.1 determine the range of x_0\n\n\nCode\ndf_reg[\"x_0\"].agg([\"min\", \"max\"])\n\n\nmin   0.00\nmax   1.00\nName: x_0, dtype: float64\n\n\nthe range of x_0 is between zero and one, in this case we are taking the entire region but you could also restrict it to the 90% percentile.\n\n\n2.2 set a grid size, i.e the sampling ratio over the range\nIn this case a rather small grid size of 7 is chosen. that means over the range of 0 to 1 we select 7 equaly spaced datapoints\n\n\n2.3 apply the grid over the data\nby utilizing the np.linspace() function we can easily create the grid, and create the required dataset with a cross join. for the people that know cross joins blow up your data pretty fast as it scales quadraticly. so our initial 3 datapoints together with the grid size increase the data count to $ 3 * 7 = 21 $ the newly created column is the feature for which we want to calculate the pd, in this case x_0_sample\n\n\nCode\ndf_pdp = df_sample.drop(columns=\"x_0\")\ndf_sample_grid = pd.Series(np.linspace(0, 1, 7), name=\"x_0_sample\").round(2)\ndf_pdp = df_pdp.join(other=df_sample_grid, how=\"cross\")\ndf_pdp\n\n\n\n\n\n\n\n\n\n\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\ntarget\nx_0_sample\n\n\n\n\n0\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.00\n\n\n1\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.17\n\n\n2\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.33\n\n\n3\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.50\n\n\n4\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.67\n\n\n5\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.83\n\n\n6\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n1.00\n\n\n7\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.00\n\n\n8\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.17\n\n\n9\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.33\n\n\n10\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.50\n\n\n11\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.67\n\n\n12\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.83\n\n\n13\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n1.00\n\n\n14\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.00\n\n\n15\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.17\n\n\n16\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.33\n\n\n17\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.50\n\n\n18\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.67\n\n\n19\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.83\n\n\n20\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n1.00\n\n\n\n\n\n\n\n\n\n\n2.4 predict with the model\nfor our new dataset we ask the model to make a prediction for each observations, so the original model is asked to create a predictions for each of the newly created 21 data points.\n\n\nCode\ndf_pdp = df_pdp.assign(\n    y_pred=reg.predict(\n        df_pdp.loc[\n            :, [\"x_0_sample\", \"x_1\", \"x_2\", \"x_3\", \"x_4\", \"x_5\", \"x_6\"]\n        ].to_numpy()\n    )\n)\ndf_pdp\n\n\n\n\n\n\n\n\n\n\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\ntarget\nx_0_sample\ny_pred\n\n\n\n\n0\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.00\n5.23\n\n\n1\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.17\n9.61\n\n\n2\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.33\n12.19\n\n\n3\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.50\n12.58\n\n\n4\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.67\n12.72\n\n\n5\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.83\n12.66\n\n\n6\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n1.00\n12.79\n\n\n7\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.00\n5.76\n\n\n8\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.17\n9.31\n\n\n9\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.33\n9.89\n\n\n10\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.50\n11.27\n\n\n11\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.67\n11.67\n\n\n12\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.83\n11.60\n\n\n13\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n1.00\n11.86\n\n\n14\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.00\n6.65\n\n\n15\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.17\n6.97\n\n\n16\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.33\n7.73\n\n\n17\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.50\n9.19\n\n\n18\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.67\n9.91\n\n\n19\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.83\n10.83\n\n\n20\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n1.00\n10.82\n\n\n\n\n\n\n\n\n\n\n2.5 average out the individual predictions\nthe next step is the aggregate the results by taking the average. this is achieved by taking the columns x_0_sample , y_pred and grouping by x_0_sample and applying mean aggregation.\n\n\nCode\ndf_pdp_plot = (\n    df_pdp[[\"x_0_sample\", \"y_pred\"]].groupby(\"x_0_sample\", as_index=False).mean()\n)\ndf_pdp_plot\n\n\n\n\n\n\n\n\n\n\nx_0_sample\ny_pred\n\n\n\n\n0\n0.00\n5.88\n\n\n1\n0.17\n8.63\n\n\n2\n0.33\n9.94\n\n\n3\n0.50\n11.01\n\n\n4\n0.67\n11.43\n\n\n5\n0.83\n11.70\n\n\n6\n1.00\n11.82\n\n\n\n\n\n\n\n\nthis is the dataframe that shows the average dependence of the target variable with respect to the feature x_0 in this case. increasing the value of x_0 also yields an increase of the target variable. as can be seen in the plot below. however keep in mind that this is done with a subset of the data on a very coarse grid. if more samples are included and the size of the grid is increased a more detailed dependence will emerge.\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=FIGSIZE)\nax.plot(df_pdp_plot[\"x_0_sample\"], df_pdp_plot[\"y_pred\"])\nax.set_xlabel(\"$x_0$\")\nax.set_ylabel(\"target\")\nax.set_title(\"partial dependence of $x_0$ on the target variable\")\nplt.show()\n\n\n\n\n\n\n\n\n\nthe plot above shows that the response of the model is postive with respect to an increase in the value of feature x_0 however this is done with just three datapoints,If we use the full dataset of 2000 samples and a sampling grid of 127 that would result in a final dataset of \\(2000 * 128 = 256.000\\) samples. yikes that explodes fast and this is just for one feature. however we do have the ability to get a more accurate picture of the partial dependece of the x_0 variable.\nbelow you can see the code, i placed two comments to indicate where the changes have been made.\n\n\nCode\ndf_pdp = df_reg.drop(columns=\"x_0\")  # &lt;- the full dataset\ndf_sample_grid = pd.Series(\n    np.linspace(0, 1, 128),  # &lt;- sampling is 128\n    name=\"x_0_sample\",\n)\ndf_pdp = df_pdp.join(other=df_sample_grid, how=\"cross\")\n\ndf_pdp = df_pdp.assign(\n    y_pred=reg.predict(\n        df_pdp.loc[\n            :, [\"x_0_sample\", \"x_1\", \"x_2\", \"x_3\", \"x_4\", \"x_5\", \"x_6\"]\n        ].to_numpy()\n    )\n)\n\ndf_pdp_plot_full = (\n    df_pdp[[\"x_0_sample\", \"y_pred\"]].groupby(\"x_0_sample\", as_index=False).mean()\n)\n\n_, ax = plt.subplots(1, 1, figsize=FIGSIZE)\nax.plot(df_pdp_plot_full[\"x_0_sample\"], df_pdp_plot_full[\"y_pred\"])\nax.set_xlabel(\"$x_0$\")\nax.set_ylabel(\"target\")\nax.set_title(\"partial dependence of $x_0$ on the target variable\")\nplt.show()\n\n\n\n\n\n\n\n\n\nthe same picture as before emerges however this time it includes a little more detail, an increaase in x_0 also yields an increase in the target variable to an almost monotonic level."
  },
  {
    "objectID": "posts/DPD/dependency.html#scikit-learn-implementation",
    "href": "posts/DPD/dependency.html#scikit-learn-implementation",
    "title": "intro to partial dependence plots",
    "section": "3 Scikit-learn implementation",
    "text": "3 Scikit-learn implementation\nin practise we would not code our own PDP routinge but first look if there already is a lib that does the job and scikit-learn has an implementation of a partial dependence and also the SHAP packages has a PDP option, super handy if you are already using it to get shapley values.\n\n3.1 one-way partial dependence\nby using the scikit-learn function we can easily create plots for our entire dataset. it is just with a one-liner 😊.\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=(12, 16))\n\nPartialDependenceDisplay.from_estimator(\n    estimator=reg, X=X, features=range(0, 7), grid_resolution=128, kind=\"average\", ax=ax\n)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfor the plot above we can clearly see that x_6 and x_5 do not meaningfully contribute to the output of the model. so changes one of those two will not change the predicted target value. this is also logical because these two features are random noise. x_2 has the shape of a bathtub and the mean of 0.5 yields the lowest target outcome. x_0 and x_1 show a steady monotonic increase and plateau for the higher input values, after which the impact on the target is diminished. for the remaining two features x_3 and x_4 show that an increase in the feature yields a higher outcome of the target, also a big jump in output can be seen at around 0.4 for x_3 adn 0.6 for x_4\n\n\n3.2 two-way partial dependence\nIt is possible to adapt this approach and get the PDP of two features with respect to the target variable. the trick is to create a sample grid that is the product of the two features. but again this will increase the number of datapoints that need to be inferenced. however now you can obtain insights into the feature interactions and the results it has on the target variable.\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n\nPartialDependenceDisplay.from_estimator(\n    reg, X, features=[(1, 2)], grid_resolution=128, ax=ax\n)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/DPD/dependency.html#bonus-individual-conditional-expectation",
    "href": "posts/DPD/dependency.html#bonus-individual-conditional-expectation",
    "title": "intro to partial dependence plots",
    "section": "4 Bonus: Individual Conditional Expectation",
    "text": "4 Bonus: Individual Conditional Expectation\ninstead of performing a aggregation and plotting the result, theinndividual observations can alos be used. in the plot each blue line is an original observation were the value of x_3 has taken over by the sampling grid. this allows you to inspect the individual datapoints. in this case we have a well beheaved dataset were all points are in agreement however this plot might give you some insights if performance is lacking for a couple of datapoints. it well may be that if the feature values is increased the target value goes up for one group and goes down for another group.\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=(12, 9))\n\nidx = np.random.randint(len(X), size=int(len(X) * 0.1), dtype=int)\n\n\nPartialDependenceDisplay.from_estimator(\n    reg, X.loc[idx, :], features=[3], kind=\"both\", grid_resolution=128, ax=ax\n)\nax.set_title(\"Individual Condition Expectation\")\nplt.show()"
  },
  {
    "objectID": "posts/stepcat/stepcat.html",
    "href": "posts/stepcat/stepcat.html",
    "title": "baby steps: tuneing big model one step at a time",
    "section": "",
    "text": "hello world 👋\n\n\nCode\nimport optuna\nfrom catboost import CatBoostClassifier\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import cr, train_test_split\n\n\n\n\nCode\nX, y = fetch_openml(data_id=1597, data_home=\"openml_download\", return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n\n\n\n\nCode\nX.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 29 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   V1      284807 non-null  float64\n 1   V2      284807 non-null  float64\n 2   V3      284807 non-null  float64\n 3   V4      284807 non-null  float64\n 4   V5      284807 non-null  float64\n 5   V6      284807 non-null  float64\n 6   V7      284807 non-null  float64\n 7   V8      284807 non-null  float64\n 8   V9      284807 non-null  float64\n 9   V10     284807 non-null  float64\n 10  V11     284807 non-null  float64\n 11  V12     284807 non-null  float64\n 12  V13     284807 non-null  float64\n 13  V14     284807 non-null  float64\n 14  V15     284807 non-null  float64\n 15  V16     284807 non-null  float64\n 16  V17     284807 non-null  float64\n 17  V18     284807 non-null  float64\n 18  V19     284807 non-null  float64\n 19  V20     284807 non-null  float64\n 20  V21     284807 non-null  float64\n 21  V22     284807 non-null  float64\n 22  V23     284807 non-null  float64\n 23  V24     284807 non-null  float64\n 24  V25     284807 non-null  float64\n 25  V26     284807 non-null  float64\n 26  V27     284807 non-null  float64\n 27  V28     284807 non-null  float64\n 28  Amount  284807 non-null  float64\ndtypes: float64(29)\nmemory usage: 63.0 MB\n\n\n\n\nCode\nclassifier = CatBoostClassifier()\n\n\n\n\nCode\nclassifier.fit(X_train, y_train, eval_set=(X_test, y_test))\n\n\nLearning rate set to 0.121128\n0:  learn: 0.3071897    test: 0.3072926 best: 0.3072926 (0) total: 14.2ms   remaining: 14.2s\n1:  learn: 0.1536840    test: 0.1535857 best: 0.1535857 (1) total: 27.5ms   remaining: 13.7s\n2:  learn: 0.0736323    test: 0.0736258 best: 0.0736258 (2) total: 40ms remaining: 13.3s\n3:  learn: 0.0381991    test: 0.0381996 best: 0.0381996 (3) total: 53.2ms   remaining: 13.3s\n4:  learn: 0.0219160    test: 0.0219192 best: 0.0219192 (4) total: 65.9ms   remaining: 13.1s\n5:  learn: 0.0138181    test: 0.0138475 best: 0.0138475 (5) total: 78.2ms   remaining: 13s\n6:  learn: 0.0094805    test: 0.0095179 best: 0.0095179 (6) total: 91ms remaining: 12.9s\n7:  learn: 0.0071271    test: 0.0071443 best: 0.0071443 (7) total: 104ms    remaining: 12.9s\n8:  learn: 0.0056565    test: 0.0056889 best: 0.0056889 (8) total: 116ms    remaining: 12.8s\n9:  learn: 0.0046348    test: 0.0046960 best: 0.0046960 (9) total: 129ms    remaining: 12.7s\n10: learn: 0.0040311    test: 0.0041042 best: 0.0041042 (10)    total: 141ms    remaining: 12.7s\n11: learn: 0.0036231    test: 0.0037168 best: 0.0037168 (11)    total: 154ms    remaining: 12.7s\n12: learn: 0.0033432    test: 0.0034531 best: 0.0034531 (12)    total: 167ms    remaining: 12.7s\n13: learn: 0.0031533    test: 0.0032756 best: 0.0032756 (13)    total: 180ms    remaining: 12.7s\n14: learn: 0.0030034    test: 0.0031459 best: 0.0031459 (14)    total: 193ms    remaining: 12.7s\n15: learn: 0.0028765    test: 0.0030329 best: 0.0030329 (15)    total: 207ms    remaining: 12.7s\n16: learn: 0.0027834    test: 0.0029590 best: 0.0029590 (16)    total: 224ms    remaining: 13s\n17: learn: 0.0027061    test: 0.0028855 best: 0.0028855 (17)    total: 238ms    remaining: 13s\n18: learn: 0.0026491    test: 0.0028620 best: 0.0028620 (18)    total: 251ms    remaining: 13s\n19: learn: 0.0025919    test: 0.0028216 best: 0.0028216 (19)    total: 265ms    remaining: 13s\n20: learn: 0.0025402    test: 0.0027778 best: 0.0027778 (20)    total: 278ms    remaining: 12.9s\n21: learn: 0.0024994    test: 0.0027584 best: 0.0027584 (21)    total: 290ms    remaining: 12.9s\n22: learn: 0.0024764    test: 0.0027386 best: 0.0027386 (22)    total: 303ms    remaining: 12.8s\n23: learn: 0.0024554    test: 0.0027307 best: 0.0027307 (23)    total: 315ms    remaining: 12.8s\n24: learn: 0.0024284    test: 0.0027176 best: 0.0027176 (24)    total: 328ms    remaining: 12.8s\n25: learn: 0.0024141    test: 0.0027158 best: 0.0027158 (25)    total: 342ms    remaining: 12.8s\n26: learn: 0.0024026    test: 0.0027113 best: 0.0027113 (26)    total: 355ms    remaining: 12.8s\n27: learn: 0.0023428    test: 0.0026957 best: 0.0026957 (27)    total: 368ms    remaining: 12.8s\n28: learn: 0.0023269    test: 0.0026878 best: 0.0026878 (28)    total: 381ms    remaining: 12.7s\n29: learn: 0.0022890    test: 0.0026565 best: 0.0026565 (29)    total: 393ms    remaining: 12.7s\n30: learn: 0.0022731    test: 0.0026552 best: 0.0026552 (30)    total: 406ms    remaining: 12.7s\n31: learn: 0.0022589    test: 0.0026429 best: 0.0026429 (31)    total: 419ms    remaining: 12.7s\n32: learn: 0.0022151    test: 0.0026387 best: 0.0026387 (32)    total: 434ms    remaining: 12.7s\n33: learn: 0.0021662    test: 0.0026285 best: 0.0026285 (33)    total: 447ms    remaining: 12.7s\n34: learn: 0.0021507    test: 0.0026217 best: 0.0026217 (34)    total: 459ms    remaining: 12.6s\n35: learn: 0.0021239    test: 0.0026082 best: 0.0026082 (35)    total: 472ms    remaining: 12.6s\n36: learn: 0.0021046    test: 0.0026054 best: 0.0026054 (36)    total: 484ms    remaining: 12.6s\n37: learn: 0.0020925    test: 0.0026007 best: 0.0026007 (37)    total: 497ms    remaining: 12.6s\n38: learn: 0.0020581    test: 0.0026047 best: 0.0026007 (37)    total: 509ms    remaining: 12.5s\n39: learn: 0.0020465    test: 0.0025977 best: 0.0025977 (39)    total: 521ms    remaining: 12.5s\n40: learn: 0.0020412    test: 0.0025931 best: 0.0025931 (40)    total: 534ms    remaining: 12.5s\n41: learn: 0.0020307    test: 0.0025855 best: 0.0025855 (41)    total: 547ms    remaining: 12.5s\n42: learn: 0.0019997    test: 0.0025830 best: 0.0025830 (42)    total: 560ms    remaining: 12.5s\n43: learn: 0.0019881    test: 0.0025740 best: 0.0025740 (43)    total: 571ms    remaining: 12.4s\n44: learn: 0.0019676    test: 0.0025715 best: 0.0025715 (44)    total: 584ms    remaining: 12.4s\n45: learn: 0.0019562    test: 0.0025702 best: 0.0025702 (45)    total: 596ms    remaining: 12.4s\n46: learn: 0.0019462    test: 0.0025710 best: 0.0025702 (45)    total: 609ms    remaining: 12.4s\n47: learn: 0.0019359    test: 0.0025656 best: 0.0025656 (47)    total: 623ms    remaining: 12.4s\n48: learn: 0.0019078    test: 0.0025733 best: 0.0025656 (47)    total: 640ms    remaining: 12.4s\n49: learn: 0.0018843    test: 0.0025796 best: 0.0025656 (47)    total: 653ms    remaining: 12.4s\n50: learn: 0.0018726    test: 0.0025829 best: 0.0025656 (47)    total: 667ms    remaining: 12.4s\n51: learn: 0.0018621    test: 0.0025811 best: 0.0025656 (47)    total: 679ms    remaining: 12.4s\n52: learn: 0.0018434    test: 0.0025780 best: 0.0025656 (47)    total: 692ms    remaining: 12.4s\n53: learn: 0.0018340    test: 0.0025789 best: 0.0025656 (47)    total: 704ms    remaining: 12.3s\n54: learn: 0.0018274    test: 0.0025811 best: 0.0025656 (47)    total: 716ms    remaining: 12.3s\n55: learn: 0.0018131    test: 0.0025755 best: 0.0025656 (47)    total: 729ms    remaining: 12.3s\n56: learn: 0.0018005    test: 0.0025829 best: 0.0025656 (47)    total: 741ms    remaining: 12.3s\n57: learn: 0.0017945    test: 0.0025834 best: 0.0025656 (47)    total: 754ms    remaining: 12.3s\n58: learn: 0.0017709    test: 0.0025735 best: 0.0025656 (47)    total: 767ms    remaining: 12.2s\n59: learn: 0.0017417    test: 0.0025683 best: 0.0025656 (47)    total: 779ms    remaining: 12.2s\n60: learn: 0.0017334    test: 0.0025717 best: 0.0025656 (47)    total: 792ms    remaining: 12.2s\n61: learn: 0.0017263    test: 0.0025727 best: 0.0025656 (47)    total: 804ms    remaining: 12.2s\n62: learn: 0.0017155    test: 0.0025686 best: 0.0025656 (47)    total: 817ms    remaining: 12.1s\n63: learn: 0.0016956    test: 0.0025672 best: 0.0025656 (47)    total: 830ms    remaining: 12.1s\n64: learn: 0.0016879    test: 0.0025670 best: 0.0025656 (47)    total: 844ms    remaining: 12.1s\n65: learn: 0.0016819    test: 0.0025685 best: 0.0025656 (47)    total: 857ms    remaining: 12.1s\n66: learn: 0.0016717    test: 0.0025665 best: 0.0025656 (47)    total: 870ms    remaining: 12.1s\n67: learn: 0.0016628    test: 0.0025657 best: 0.0025656 (47)    total: 882ms    remaining: 12.1s\n68: learn: 0.0016538    test: 0.0025626 best: 0.0025626 (68)    total: 896ms    remaining: 12.1s\n69: learn: 0.0016442    test: 0.0025551 best: 0.0025551 (69)    total: 909ms    remaining: 12.1s\n70: learn: 0.0016404    test: 0.0025549 best: 0.0025549 (70)    total: 923ms    remaining: 12.1s\n71: learn: 0.0016291    test: 0.0025580 best: 0.0025549 (70)    total: 938ms    remaining: 12.1s\n72: learn: 0.0016191    test: 0.0025551 best: 0.0025549 (70)    total: 951ms    remaining: 12.1s\n73: learn: 0.0016113    test: 0.0025502 best: 0.0025502 (73)    total: 964ms    remaining: 12.1s\n74: learn: 0.0016073    test: 0.0025525 best: 0.0025502 (73)    total: 976ms    remaining: 12s\n75: learn: 0.0015994    test: 0.0025575 best: 0.0025502 (73)    total: 988ms    remaining: 12s\n76: learn: 0.0015939    test: 0.0025550 best: 0.0025502 (73)    total: 1s   remaining: 12s\n77: learn: 0.0015847    test: 0.0025566 best: 0.0025502 (73)    total: 1.01s    remaining: 12s\n78: learn: 0.0015737    test: 0.0025493 best: 0.0025493 (78)    total: 1.03s    remaining: 12s\n79: learn: 0.0015667    test: 0.0025555 best: 0.0025493 (78)    total: 1.04s    remaining: 11.9s\n80: learn: 0.0015593    test: 0.0025574 best: 0.0025493 (78)    total: 1.05s    remaining: 11.9s\n81: learn: 0.0015485    test: 0.0025523 best: 0.0025493 (78)    total: 1.06s    remaining: 11.9s\n82: learn: 0.0015406    test: 0.0025472 best: 0.0025472 (82)    total: 1.08s    remaining: 11.9s\n83: learn: 0.0015351    test: 0.0025465 best: 0.0025465 (83)    total: 1.09s    remaining: 11.9s\n84: learn: 0.0015237    test: 0.0025359 best: 0.0025359 (84)    total: 1.1s remaining: 11.9s\n85: learn: 0.0015106    test: 0.0025275 best: 0.0025275 (85)    total: 1.12s    remaining: 11.9s\n86: learn: 0.0015050    test: 0.0025272 best: 0.0025272 (86)    total: 1.13s    remaining: 11.9s\n87: learn: 0.0015024    test: 0.0025261 best: 0.0025261 (87)    total: 1.15s    remaining: 11.9s\n88: learn: 0.0014909    test: 0.0025237 best: 0.0025237 (88)    total: 1.16s    remaining: 11.9s\n89: learn: 0.0014864    test: 0.0025206 best: 0.0025206 (89)    total: 1.17s    remaining: 11.8s\n90: learn: 0.0014818    test: 0.0025175 best: 0.0025175 (90)    total: 1.18s    remaining: 11.8s\n91: learn: 0.0014740    test: 0.0025232 best: 0.0025175 (90)    total: 1.2s remaining: 11.8s\n92: learn: 0.0014663    test: 0.0025247 best: 0.0025175 (90)    total: 1.21s    remaining: 11.8s\n93: learn: 0.0014575    test: 0.0025322 best: 0.0025175 (90)    total: 1.22s    remaining: 11.8s\n94: learn: 0.0014489    test: 0.0025293 best: 0.0025175 (90)    total: 1.23s    remaining: 11.8s\n95: learn: 0.0014433    test: 0.0025268 best: 0.0025175 (90)    total: 1.25s    remaining: 11.8s\n96: learn: 0.0014358    test: 0.0025234 best: 0.0025175 (90)    total: 1.26s    remaining: 11.7s\n97: learn: 0.0014218    test: 0.0025102 best: 0.0025102 (97)    total: 1.27s    remaining: 11.7s\n98: learn: 0.0014155    test: 0.0025184 best: 0.0025102 (97)    total: 1.29s    remaining: 11.7s\n99: learn: 0.0014118    test: 0.0025188 best: 0.0025102 (97)    total: 1.3s remaining: 11.7s\n100:    learn: 0.0014061    test: 0.0025171 best: 0.0025102 (97)    total: 1.31s    remaining: 11.7s\n101:    learn: 0.0014019    test: 0.0025175 best: 0.0025102 (97)    total: 1.32s    remaining: 11.7s\n102:    learn: 0.0013915    test: 0.0025170 best: 0.0025102 (97)    total: 1.34s    remaining: 11.7s\n103:    learn: 0.0013722    test: 0.0025178 best: 0.0025102 (97)    total: 1.35s    remaining: 11.6s\n104:    learn: 0.0013680    test: 0.0025161 best: 0.0025102 (97)    total: 1.36s    remaining: 11.6s\n105:    learn: 0.0013634    test: 0.0025155 best: 0.0025102 (97)    total: 1.38s    remaining: 11.6s\n106:    learn: 0.0013524    test: 0.0025083 best: 0.0025083 (106)   total: 1.39s    remaining: 11.6s\n107:    learn: 0.0013485    test: 0.0025107 best: 0.0025083 (106)   total: 1.4s remaining: 11.6s\n108:    learn: 0.0013336    test: 0.0025095 best: 0.0025083 (106)   total: 1.42s    remaining: 11.6s\n109:    learn: 0.0013223    test: 0.0025131 best: 0.0025083 (106)   total: 1.43s    remaining: 11.6s\n110:    learn: 0.0013203    test: 0.0025156 best: 0.0025083 (106)   total: 1.44s    remaining: 11.6s\n111:    learn: 0.0013179    test: 0.0025198 best: 0.0025083 (106)   total: 1.46s    remaining: 11.5s\n112:    learn: 0.0013110    test: 0.0025201 best: 0.0025083 (106)   total: 1.47s    remaining: 11.5s\n113:    learn: 0.0012994    test: 0.0025159 best: 0.0025083 (106)   total: 1.48s    remaining: 11.5s\n114:    learn: 0.0012956    test: 0.0025149 best: 0.0025083 (106)   total: 1.5s remaining: 11.5s\n115:    learn: 0.0012929    test: 0.0025140 best: 0.0025083 (106)   total: 1.51s    remaining: 11.5s\n116:    learn: 0.0012757    test: 0.0025016 best: 0.0025016 (116)   total: 1.52s    remaining: 11.5s\n117:    learn: 0.0012667    test: 0.0024982 best: 0.0024982 (117)   total: 1.53s    remaining: 11.5s\n118:    learn: 0.0012632    test: 0.0024992 best: 0.0024982 (117)   total: 1.55s    remaining: 11.5s\n119:    learn: 0.0012604    test: 0.0024987 best: 0.0024982 (117)   total: 1.56s    remaining: 11.4s\n120:    learn: 0.0012543    test: 0.0024967 best: 0.0024967 (120)   total: 1.57s    remaining: 11.4s\n121:    learn: 0.0012494    test: 0.0024963 best: 0.0024963 (121)   total: 1.58s    remaining: 11.4s\n122:    learn: 0.0012349    test: 0.0024863 best: 0.0024863 (122)   total: 1.6s remaining: 11.4s\n123:    learn: 0.0012317    test: 0.0024852 best: 0.0024852 (123)   total: 1.61s    remaining: 11.4s\n124:    learn: 0.0012245    test: 0.0024816 best: 0.0024816 (124)   total: 1.62s    remaining: 11.4s\n125:    learn: 0.0012210    test: 0.0024811 best: 0.0024811 (125)   total: 1.64s    remaining: 11.4s\n126:    learn: 0.0012181    test: 0.0024802 best: 0.0024802 (126)   total: 1.65s    remaining: 11.3s\n127:    learn: 0.0012152    test: 0.0024826 best: 0.0024802 (126)   total: 1.66s    remaining: 11.3s\n128:    learn: 0.0012123    test: 0.0024852 best: 0.0024802 (126)   total: 1.68s    remaining: 11.3s\n129:    learn: 0.0012102    test: 0.0024831 best: 0.0024802 (126)   total: 1.69s    remaining: 11.3s\n130:    learn: 0.0012059    test: 0.0024827 best: 0.0024802 (126)   total: 1.7s remaining: 11.3s\n131:    learn: 0.0012020    test: 0.0024811 best: 0.0024802 (126)   total: 1.72s    remaining: 11.3s\n132:    learn: 0.0011993    test: 0.0024817 best: 0.0024802 (126)   total: 1.73s    remaining: 11.3s\n133:    learn: 0.0011874    test: 0.0024843 best: 0.0024802 (126)   total: 1.74s    remaining: 11.3s\n134:    learn: 0.0011783    test: 0.0024797 best: 0.0024797 (134)   total: 1.75s    remaining: 11.2s\n135:    learn: 0.0011618    test: 0.0024859 best: 0.0024797 (134)   total: 1.77s    remaining: 11.2s\n136:    learn: 0.0011596    test: 0.0024880 best: 0.0024797 (134)   total: 1.78s    remaining: 11.2s\n137:    learn: 0.0011475    test: 0.0024900 best: 0.0024797 (134)   total: 1.79s    remaining: 11.2s\n138:    learn: 0.0011445    test: 0.0024892 best: 0.0024797 (134)   total: 1.81s    remaining: 11.2s\n139:    learn: 0.0011424    test: 0.0024875 best: 0.0024797 (134)   total: 1.82s    remaining: 11.2s\n140:    learn: 0.0011343    test: 0.0024858 best: 0.0024797 (134)   total: 1.83s    remaining: 11.2s\n141:    learn: 0.0011325    test: 0.0024865 best: 0.0024797 (134)   total: 1.84s    remaining: 11.2s\n142:    learn: 0.0011283    test: 0.0024848 best: 0.0024797 (134)   total: 1.86s    remaining: 11.1s\n143:    learn: 0.0011246    test: 0.0024826 best: 0.0024797 (134)   total: 1.87s    remaining: 11.1s\n144:    learn: 0.0011225    test: 0.0024820 best: 0.0024797 (134)   total: 1.89s    remaining: 11.1s\n145:    learn: 0.0011181    test: 0.0024826 best: 0.0024797 (134)   total: 1.9s remaining: 11.1s\n146:    learn: 0.0011156    test: 0.0024823 best: 0.0024797 (134)   total: 1.91s    remaining: 11.1s\n147:    learn: 0.0011126    test: 0.0024819 best: 0.0024797 (134)   total: 1.93s    remaining: 11.1s\n148:    learn: 0.0011045    test: 0.0024839 best: 0.0024797 (134)   total: 1.94s    remaining: 11.1s\n149:    learn: 0.0011002    test: 0.0024816 best: 0.0024797 (134)   total: 1.96s    remaining: 11.1s\n150:    learn: 0.0010965    test: 0.0024782 best: 0.0024782 (150)   total: 1.98s    remaining: 11.2s\n151:    learn: 0.0010847    test: 0.0024774 best: 0.0024774 (151)   total: 2s   remaining: 11.2s\n152:    learn: 0.0010783    test: 0.0024666 best: 0.0024666 (152)   total: 2.01s    remaining: 11.1s\n153:    learn: 0.0010753    test: 0.0024666 best: 0.0024666 (152)   total: 2.02s    remaining: 11.1s\n154:    learn: 0.0010663    test: 0.0024691 best: 0.0024666 (152)   total: 2.04s    remaining: 11.1s\n155:    learn: 0.0010628    test: 0.0024671 best: 0.0024666 (152)   total: 2.05s    remaining: 11.1s\n156:    learn: 0.0010607    test: 0.0024667 best: 0.0024666 (152)   total: 2.06s    remaining: 11.1s\n157:    learn: 0.0010567    test: 0.0024663 best: 0.0024663 (157)   total: 2.08s    remaining: 11.1s\n158:    learn: 0.0010470    test: 0.0024675 best: 0.0024663 (157)   total: 2.09s    remaining: 11.1s\n159:    learn: 0.0010449    test: 0.0024625 best: 0.0024625 (159)   total: 2.1s remaining: 11s\n160:    learn: 0.0010404    test: 0.0024658 best: 0.0024625 (159)   total: 2.12s    remaining: 11s\n161:    learn: 0.0010322    test: 0.0024637 best: 0.0024625 (159)   total: 2.13s    remaining: 11s\n162:    learn: 0.0010252    test: 0.0024615 best: 0.0024615 (162)   total: 2.14s    remaining: 11s\n163:    learn: 0.0010224    test: 0.0024614 best: 0.0024614 (163)   total: 2.15s    remaining: 11s\n164:    learn: 0.0010203    test: 0.0024574 best: 0.0024574 (164)   total: 2.17s    remaining: 11s\n165:    learn: 0.0010146    test: 0.0024507 best: 0.0024507 (165)   total: 2.18s    remaining: 10.9s\n166:    learn: 0.0010118    test: 0.0024533 best: 0.0024507 (165)   total: 2.19s    remaining: 10.9s\n167:    learn: 0.0010065    test: 0.0024512 best: 0.0024507 (165)   total: 2.2s remaining: 10.9s\n168:    learn: 0.0010049    test: 0.0024521 best: 0.0024507 (165)   total: 2.22s    remaining: 10.9s\n169:    learn: 0.0010013    test: 0.0024530 best: 0.0024507 (165)   total: 2.23s    remaining: 10.9s\n170:    learn: 0.0009958    test: 0.0024540 best: 0.0024507 (165)   total: 2.24s    remaining: 10.9s\n171:    learn: 0.0009930    test: 0.0024532 best: 0.0024507 (165)   total: 2.25s    remaining: 10.9s\n172:    learn: 0.0009902    test: 0.0024550 best: 0.0024507 (165)   total: 2.27s    remaining: 10.8s\n173:    learn: 0.0009883    test: 0.0024517 best: 0.0024507 (165)   total: 2.28s    remaining: 10.8s\n174:    learn: 0.0009861    test: 0.0024516 best: 0.0024507 (165)   total: 2.29s    remaining: 10.8s\n175:    learn: 0.0009840    test: 0.0024515 best: 0.0024507 (165)   total: 2.31s    remaining: 10.8s\n176:    learn: 0.0009823    test: 0.0024514 best: 0.0024507 (165)   total: 2.32s    remaining: 10.8s\n177:    learn: 0.0009802    test: 0.0024559 best: 0.0024507 (165)   total: 2.33s    remaining: 10.8s\n178:    learn: 0.0009684    test: 0.0024496 best: 0.0024496 (178)   total: 2.34s    remaining: 10.8s\n179:    learn: 0.0009667    test: 0.0024491 best: 0.0024491 (179)   total: 2.36s    remaining: 10.7s\n180:    learn: 0.0009605    test: 0.0024482 best: 0.0024482 (180)   total: 2.37s    remaining: 10.7s\n181:    learn: 0.0009588    test: 0.0024481 best: 0.0024481 (181)   total: 2.38s    remaining: 10.7s\n182:    learn: 0.0009491    test: 0.0024526 best: 0.0024481 (181)   total: 2.4s remaining: 10.7s\n183:    learn: 0.0009442    test: 0.0024529 best: 0.0024481 (181)   total: 2.41s    remaining: 10.7s\n184:    learn: 0.0009426    test: 0.0024481 best: 0.0024481 (181)   total: 2.42s    remaining: 10.7s\n185:    learn: 0.0009323    test: 0.0024524 best: 0.0024481 (181)   total: 2.44s    remaining: 10.7s\n186:    learn: 0.0009285    test: 0.0024560 best: 0.0024481 (181)   total: 2.45s    remaining: 10.6s\n187:    learn: 0.0009253    test: 0.0024524 best: 0.0024481 (181)   total: 2.46s    remaining: 10.6s\n188:    learn: 0.0009225    test: 0.0024542 best: 0.0024481 (181)   total: 2.47s    remaining: 10.6s\n189:    learn: 0.0009197    test: 0.0024585 best: 0.0024481 (181)   total: 2.49s    remaining: 10.6s\n190:    learn: 0.0009188    test: 0.0024582 best: 0.0024481 (181)   total: 2.5s remaining: 10.6s\n191:    learn: 0.0009110    test: 0.0024526 best: 0.0024481 (181)   total: 2.51s    remaining: 10.6s\n192:    learn: 0.0009089    test: 0.0024550 best: 0.0024481 (181)   total: 2.52s    remaining: 10.6s\n193:    learn: 0.0009006    test: 0.0024593 best: 0.0024481 (181)   total: 2.54s    remaining: 10.5s\n194:    learn: 0.0008989    test: 0.0024591 best: 0.0024481 (181)   total: 2.55s    remaining: 10.5s\n195:    learn: 0.0008981    test: 0.0024597 best: 0.0024481 (181)   total: 2.56s    remaining: 10.5s\n196:    learn: 0.0008915    test: 0.0024583 best: 0.0024481 (181)   total: 2.58s    remaining: 10.5s\n197:    learn: 0.0008842    test: 0.0024623 best: 0.0024481 (181)   total: 2.59s    remaining: 10.5s\n198:    learn: 0.0008791    test: 0.0024630 best: 0.0024481 (181)   total: 2.6s remaining: 10.5s\n199:    learn: 0.0008763    test: 0.0024632 best: 0.0024481 (181)   total: 2.61s    remaining: 10.5s\n200:    learn: 0.0008737    test: 0.0024641 best: 0.0024481 (181)   total: 2.63s    remaining: 10.4s\n201:    learn: 0.0008720    test: 0.0024637 best: 0.0024481 (181)   total: 2.64s    remaining: 10.4s\n202:    learn: 0.0008632    test: 0.0024596 best: 0.0024481 (181)   total: 2.65s    remaining: 10.4s\n203:    learn: 0.0008604    test: 0.0024626 best: 0.0024481 (181)   total: 2.66s    remaining: 10.4s\n204:    learn: 0.0008575    test: 0.0024578 best: 0.0024481 (181)   total: 2.68s    remaining: 10.4s\n205:    learn: 0.0008550    test: 0.0024504 best: 0.0024481 (181)   total: 2.69s    remaining: 10.4s\n206:    learn: 0.0008521    test: 0.0024488 best: 0.0024481 (181)   total: 2.7s remaining: 10.4s\n207:    learn: 0.0008492    test: 0.0024487 best: 0.0024481 (181)   total: 2.72s    remaining: 10.3s\n208:    learn: 0.0008476    test: 0.0024459 best: 0.0024459 (208)   total: 2.73s    remaining: 10.3s\n209:    learn: 0.0008453    test: 0.0024460 best: 0.0024459 (208)   total: 2.75s    remaining: 10.3s\n210:    learn: 0.0008439    test: 0.0024493 best: 0.0024459 (208)   total: 2.76s    remaining: 10.3s\n211:    learn: 0.0008425    test: 0.0024469 best: 0.0024459 (208)   total: 2.78s    remaining: 10.3s\n212:    learn: 0.0008377    test: 0.0024508 best: 0.0024459 (208)   total: 2.79s    remaining: 10.3s\n213:    learn: 0.0008347    test: 0.0024482 best: 0.0024459 (208)   total: 2.8s remaining: 10.3s\n214:    learn: 0.0008319    test: 0.0024440 best: 0.0024440 (214)   total: 2.81s    remaining: 10.3s\n215:    learn: 0.0008246    test: 0.0024414 best: 0.0024414 (215)   total: 2.83s    remaining: 10.3s\n216:    learn: 0.0008219    test: 0.0024426 best: 0.0024414 (215)   total: 2.84s    remaining: 10.3s\n217:    learn: 0.0008207    test: 0.0024406 best: 0.0024406 (217)   total: 2.85s    remaining: 10.2s\n218:    learn: 0.0008190    test: 0.0024400 best: 0.0024400 (218)   total: 2.87s    remaining: 10.2s\n219:    learn: 0.0008122    test: 0.0024469 best: 0.0024400 (218)   total: 2.88s    remaining: 10.2s\n220:    learn: 0.0008085    test: 0.0024455 best: 0.0024400 (218)   total: 2.89s    remaining: 10.2s\n221:    learn: 0.0007994    test: 0.0024482 best: 0.0024400 (218)   total: 2.91s    remaining: 10.2s\n222:    learn: 0.0007963    test: 0.0024492 best: 0.0024400 (218)   total: 2.92s    remaining: 10.2s\n223:    learn: 0.0007898    test: 0.0024449 best: 0.0024400 (218)   total: 2.93s    remaining: 10.2s\n224:    learn: 0.0007890    test: 0.0024449 best: 0.0024400 (218)   total: 2.95s    remaining: 10.2s\n225:    learn: 0.0007871    test: 0.0024447 best: 0.0024400 (218)   total: 2.96s    remaining: 10.1s\n226:    learn: 0.0007852    test: 0.0024450 best: 0.0024400 (218)   total: 2.97s    remaining: 10.1s\n227:    learn: 0.0007829    test: 0.0024430 best: 0.0024400 (218)   total: 2.98s    remaining: 10.1s\n228:    learn: 0.0007813    test: 0.0024428 best: 0.0024400 (218)   total: 3s   remaining: 10.1s\n229:    learn: 0.0007801    test: 0.0024427 best: 0.0024400 (218)   total: 3.01s    remaining: 10.1s\n230:    learn: 0.0007791    test: 0.0024426 best: 0.0024400 (218)   total: 3.02s    remaining: 10.1s\n231:    learn: 0.0007742    test: 0.0024418 best: 0.0024400 (218)   total: 3.04s    remaining: 10.1s\n232:    learn: 0.0007728    test: 0.0024415 best: 0.0024400 (218)   total: 3.05s    remaining: 10.1s\n233:    learn: 0.0007715    test: 0.0024418 best: 0.0024400 (218)   total: 3.06s    remaining: 10s\n234:    learn: 0.0007704    test: 0.0024434 best: 0.0024400 (218)   total: 3.08s    remaining: 10s\n235:    learn: 0.0007678    test: 0.0024401 best: 0.0024400 (218)   total: 3.09s    remaining: 10s\n236:    learn: 0.0007639    test: 0.0024312 best: 0.0024312 (236)   total: 3.1s remaining: 10s\n237:    learn: 0.0007590    test: 0.0024317 best: 0.0024312 (236)   total: 3.12s    remaining: 9.98s\n238:    learn: 0.0007532    test: 0.0024353 best: 0.0024312 (236)   total: 3.13s    remaining: 9.97s\n239:    learn: 0.0007482    test: 0.0024386 best: 0.0024312 (236)   total: 3.14s    remaining: 9.96s\n240:    learn: 0.0007461    test: 0.0024358 best: 0.0024312 (236)   total: 3.16s    remaining: 9.94s\n241:    learn: 0.0007396    test: 0.0024356 best: 0.0024312 (236)   total: 3.17s    remaining: 9.93s\n242:    learn: 0.0007378    test: 0.0024349 best: 0.0024312 (236)   total: 3.18s    remaining: 9.91s\n243:    learn: 0.0007375    test: 0.0024355 best: 0.0024312 (236)   total: 3.19s    remaining: 9.9s\n244:    learn: 0.0007312    test: 0.0024312 best: 0.0024312 (244)   total: 3.21s    remaining: 9.88s\n245:    learn: 0.0007292    test: 0.0024343 best: 0.0024312 (244)   total: 3.22s    remaining: 9.87s\n246:    learn: 0.0007272    test: 0.0024304 best: 0.0024304 (246)   total: 3.23s    remaining: 9.86s\n247:    learn: 0.0007261    test: 0.0024305 best: 0.0024304 (246)   total: 3.25s    remaining: 9.84s\n248:    learn: 0.0007243    test: 0.0024283 best: 0.0024283 (248)   total: 3.26s    remaining: 9.82s\n249:    learn: 0.0007233    test: 0.0024277 best: 0.0024277 (249)   total: 3.27s    remaining: 9.81s\n250:    learn: 0.0007150    test: 0.0024364 best: 0.0024277 (249)   total: 3.28s    remaining: 9.79s\n251:    learn: 0.0007142    test: 0.0024382 best: 0.0024277 (249)   total: 3.29s    remaining: 9.78s\n252:    learn: 0.0007104    test: 0.0024405 best: 0.0024277 (249)   total: 3.31s    remaining: 9.76s\n253:    learn: 0.0007082    test: 0.0024392 best: 0.0024277 (249)   total: 3.32s    remaining: 9.75s\n254:    learn: 0.0007063    test: 0.0024335 best: 0.0024277 (249)   total: 3.33s    remaining: 9.74s\n255:    learn: 0.0007035    test: 0.0024397 best: 0.0024277 (249)   total: 3.35s    remaining: 9.72s\n256:    learn: 0.0007026    test: 0.0024385 best: 0.0024277 (249)   total: 3.36s    remaining: 9.71s\n257:    learn: 0.0006986    test: 0.0024418 best: 0.0024277 (249)   total: 3.37s    remaining: 9.7s\n258:    learn: 0.0006922    test: 0.0024442 best: 0.0024277 (249)   total: 3.38s    remaining: 9.68s\n259:    learn: 0.0006898    test: 0.0024452 best: 0.0024277 (249)   total: 3.4s remaining: 9.67s\n260:    learn: 0.0006886    test: 0.0024466 best: 0.0024277 (249)   total: 3.41s    remaining: 9.65s\n261:    learn: 0.0006873    test: 0.0024451 best: 0.0024277 (249)   total: 3.42s    remaining: 9.64s\n262:    learn: 0.0006824    test: 0.0024516 best: 0.0024277 (249)   total: 3.43s    remaining: 9.62s\n263:    learn: 0.0006798    test: 0.0024510 best: 0.0024277 (249)   total: 3.45s    remaining: 9.61s\n264:    learn: 0.0006722    test: 0.0024499 best: 0.0024277 (249)   total: 3.46s    remaining: 9.6s\n265:    learn: 0.0006698    test: 0.0024492 best: 0.0024277 (249)   total: 3.47s    remaining: 9.58s\n266:    learn: 0.0006672    test: 0.0024500 best: 0.0024277 (249)   total: 3.48s    remaining: 9.57s\n267:    learn: 0.0006659    test: 0.0024468 best: 0.0024277 (249)   total: 3.5s remaining: 9.55s\n268:    learn: 0.0006652    test: 0.0024465 best: 0.0024277 (249)   total: 3.51s    remaining: 9.54s\n269:    learn: 0.0006615    test: 0.0024538 best: 0.0024277 (249)   total: 3.53s    remaining: 9.54s\n270:    learn: 0.0006551    test: 0.0024618 best: 0.0024277 (249)   total: 3.54s    remaining: 9.53s\n271:    learn: 0.0006528    test: 0.0024622 best: 0.0024277 (249)   total: 3.55s    remaining: 9.51s\n272:    learn: 0.0006503    test: 0.0024615 best: 0.0024277 (249)   total: 3.57s    remaining: 9.5s\n273:    learn: 0.0006496    test: 0.0024612 best: 0.0024277 (249)   total: 3.58s    remaining: 9.49s\n274:    learn: 0.0006488    test: 0.0024609 best: 0.0024277 (249)   total: 3.59s    remaining: 9.47s\n275:    learn: 0.0006429    test: 0.0024559 best: 0.0024277 (249)   total: 3.61s    remaining: 9.46s\n276:    learn: 0.0006417    test: 0.0024532 best: 0.0024277 (249)   total: 3.62s    remaining: 9.45s\n277:    learn: 0.0006403    test: 0.0024534 best: 0.0024277 (249)   total: 3.63s    remaining: 9.43s\n278:    learn: 0.0006353    test: 0.0024536 best: 0.0024277 (249)   total: 3.64s    remaining: 9.42s\n279:    learn: 0.0006342    test: 0.0024565 best: 0.0024277 (249)   total: 3.66s    remaining: 9.4s\n280:    learn: 0.0006325    test: 0.0024511 best: 0.0024277 (249)   total: 3.67s    remaining: 9.39s\n281:    learn: 0.0006283    test: 0.0024499 best: 0.0024277 (249)   total: 3.68s    remaining: 9.37s\n282:    learn: 0.0006266    test: 0.0024540 best: 0.0024277 (249)   total: 3.69s    remaining: 9.36s\n283:    learn: 0.0006236    test: 0.0024528 best: 0.0024277 (249)   total: 3.71s    remaining: 9.35s\n284:    learn: 0.0006221    test: 0.0024493 best: 0.0024277 (249)   total: 3.72s    remaining: 9.33s\n285:    learn: 0.0006202    test: 0.0024489 best: 0.0024277 (249)   total: 3.73s    remaining: 9.32s\n286:    learn: 0.0006174    test: 0.0024564 best: 0.0024277 (249)   total: 3.75s    remaining: 9.31s\n287:    learn: 0.0006148    test: 0.0024592 best: 0.0024277 (249)   total: 3.76s    remaining: 9.29s\n288:    learn: 0.0006120    test: 0.0024630 best: 0.0024277 (249)   total: 3.77s    remaining: 9.28s\n289:    learn: 0.0006113    test: 0.0024625 best: 0.0024277 (249)   total: 3.78s    remaining: 9.26s\n290:    learn: 0.0006103    test: 0.0024608 best: 0.0024277 (249)   total: 3.79s    remaining: 9.25s\n291:    learn: 0.0006100    test: 0.0024611 best: 0.0024277 (249)   total: 3.81s    remaining: 9.23s\n292:    learn: 0.0006080    test: 0.0024672 best: 0.0024277 (249)   total: 3.82s    remaining: 9.21s\n293:    learn: 0.0006049    test: 0.0024709 best: 0.0024277 (249)   total: 3.83s    remaining: 9.2s\n294:    learn: 0.0006017    test: 0.0024714 best: 0.0024277 (249)   total: 3.85s    remaining: 9.19s\n295:    learn: 0.0005975    test: 0.0024716 best: 0.0024277 (249)   total: 3.86s    remaining: 9.18s\n296:    learn: 0.0005949    test: 0.0024726 best: 0.0024277 (249)   total: 3.88s    remaining: 9.17s\n297:    learn: 0.0005933    test: 0.0024730 best: 0.0024277 (249)   total: 3.89s    remaining: 9.16s\n298:    learn: 0.0005916    test: 0.0024726 best: 0.0024277 (249)   total: 3.9s remaining: 9.14s\n299:    learn: 0.0005899    test: 0.0024725 best: 0.0024277 (249)   total: 3.91s    remaining: 9.13s\n300:    learn: 0.0005869    test: 0.0024729 best: 0.0024277 (249)   total: 3.93s    remaining: 9.12s\n301:    learn: 0.0005832    test: 0.0024736 best: 0.0024277 (249)   total: 3.94s    remaining: 9.11s\n302:    learn: 0.0005815    test: 0.0024730 best: 0.0024277 (249)   total: 3.95s    remaining: 9.09s\n303:    learn: 0.0005801    test: 0.0024721 best: 0.0024277 (249)   total: 3.96s    remaining: 9.08s\n304:    learn: 0.0005774    test: 0.0024726 best: 0.0024277 (249)   total: 3.98s    remaining: 9.07s\n305:    learn: 0.0005759    test: 0.0024725 best: 0.0024277 (249)   total: 4s   remaining: 9.06s\n306:    learn: 0.0005750    test: 0.0024733 best: 0.0024277 (249)   total: 4.01s    remaining: 9.06s\n307:    learn: 0.0005729    test: 0.0024745 best: 0.0024277 (249)   total: 4.03s    remaining: 9.05s\n308:    learn: 0.0005686    test: 0.0024791 best: 0.0024277 (249)   total: 4.04s    remaining: 9.04s\n309:    learn: 0.0005682    test: 0.0024787 best: 0.0024277 (249)   total: 4.05s    remaining: 9.02s\n310:    learn: 0.0005660    test: 0.0024789 best: 0.0024277 (249)   total: 4.07s    remaining: 9.01s\n311:    learn: 0.0005633    test: 0.0024792 best: 0.0024277 (249)   total: 4.08s    remaining: 9s\n312:    learn: 0.0005618    test: 0.0024789 best: 0.0024277 (249)   total: 4.1s remaining: 8.99s\n313:    learn: 0.0005594    test: 0.0024795 best: 0.0024277 (249)   total: 4.11s    remaining: 8.98s\n314:    learn: 0.0005569    test: 0.0024814 best: 0.0024277 (249)   total: 4.12s    remaining: 8.96s\n315:    learn: 0.0005567    test: 0.0024817 best: 0.0024277 (249)   total: 4.13s    remaining: 8.95s\n316:    learn: 0.0005536    test: 0.0024816 best: 0.0024277 (249)   total: 4.15s    remaining: 8.94s\n317:    learn: 0.0005500    test: 0.0024819 best: 0.0024277 (249)   total: 4.16s    remaining: 8.92s\n318:    learn: 0.0005480    test: 0.0024823 best: 0.0024277 (249)   total: 4.17s    remaining: 8.91s\n319:    learn: 0.0005422    test: 0.0024817 best: 0.0024277 (249)   total: 4.19s    remaining: 8.9s\n320:    learn: 0.0005387    test: 0.0024917 best: 0.0024277 (249)   total: 4.2s remaining: 8.88s\n321:    learn: 0.0005364    test: 0.0024933 best: 0.0024277 (249)   total: 4.21s    remaining: 8.87s\n322:    learn: 0.0005338    test: 0.0024894 best: 0.0024277 (249)   total: 4.22s    remaining: 8.85s\n323:    learn: 0.0005297    test: 0.0024916 best: 0.0024277 (249)   total: 4.24s    remaining: 8.84s\n324:    learn: 0.0005268    test: 0.0024902 best: 0.0024277 (249)   total: 4.25s    remaining: 8.82s\n325:    learn: 0.0005248    test: 0.0024909 best: 0.0024277 (249)   total: 4.26s    remaining: 8.81s\n326:    learn: 0.0005183    test: 0.0024946 best: 0.0024277 (249)   total: 4.28s    remaining: 8.8s\n327:    learn: 0.0005155    test: 0.0024922 best: 0.0024277 (249)   total: 4.29s    remaining: 8.79s\n328:    learn: 0.0005137    test: 0.0024921 best: 0.0024277 (249)   total: 4.3s remaining: 8.77s\n329:    learn: 0.0005121    test: 0.0024927 best: 0.0024277 (249)   total: 4.31s    remaining: 8.76s\n330:    learn: 0.0005108    test: 0.0024923 best: 0.0024277 (249)   total: 4.32s    remaining: 8.74s\n331:    learn: 0.0005087    test: 0.0024930 best: 0.0024277 (249)   total: 4.34s    remaining: 8.72s\n332:    learn: 0.0005037    test: 0.0024921 best: 0.0024277 (249)   total: 4.35s    remaining: 8.71s\n333:    learn: 0.0005025    test: 0.0024902 best: 0.0024277 (249)   total: 4.36s    remaining: 8.7s\n334:    learn: 0.0005003    test: 0.0024911 best: 0.0024277 (249)   total: 4.38s    remaining: 8.69s\n335:    learn: 0.0004967    test: 0.0024887 best: 0.0024277 (249)   total: 4.39s    remaining: 8.68s\n336:    learn: 0.0004938    test: 0.0024870 best: 0.0024277 (249)   total: 4.41s    remaining: 8.67s\n337:    learn: 0.0004927    test: 0.0024866 best: 0.0024277 (249)   total: 4.42s    remaining: 8.65s\n338:    learn: 0.0004900    test: 0.0024840 best: 0.0024277 (249)   total: 4.43s    remaining: 8.63s\n339:    learn: 0.0004883    test: 0.0024865 best: 0.0024277 (249)   total: 4.44s    remaining: 8.63s\n340:    learn: 0.0004861    test: 0.0024846 best: 0.0024277 (249)   total: 4.46s    remaining: 8.61s\n341:    learn: 0.0004842    test: 0.0024853 best: 0.0024277 (249)   total: 4.47s    remaining: 8.6s\n342:    learn: 0.0004831    test: 0.0024843 best: 0.0024277 (249)   total: 4.48s    remaining: 8.59s\n343:    learn: 0.0004813    test: 0.0024849 best: 0.0024277 (249)   total: 4.49s    remaining: 8.57s\n344:    learn: 0.0004800    test: 0.0024843 best: 0.0024277 (249)   total: 4.51s    remaining: 8.56s\n345:    learn: 0.0004780    test: 0.0024826 best: 0.0024277 (249)   total: 4.52s    remaining: 8.54s\n346:    learn: 0.0004763    test: 0.0024833 best: 0.0024277 (249)   total: 4.53s    remaining: 8.53s\n347:    learn: 0.0004747    test: 0.0024840 best: 0.0024277 (249)   total: 4.54s    remaining: 8.51s\n348:    learn: 0.0004735    test: 0.0024816 best: 0.0024277 (249)   total: 4.56s    remaining: 8.5s\n349:    learn: 0.0004720    test: 0.0024823 best: 0.0024277 (249)   total: 4.57s    remaining: 8.49s\n350:    learn: 0.0004709    test: 0.0024840 best: 0.0024277 (249)   total: 4.58s    remaining: 8.47s\n351:    learn: 0.0004698    test: 0.0024856 best: 0.0024277 (249)   total: 4.59s    remaining: 8.46s\n352:    learn: 0.0004691    test: 0.0024839 best: 0.0024277 (249)   total: 4.61s    remaining: 8.45s\n353:    learn: 0.0004685    test: 0.0024839 best: 0.0024277 (249)   total: 4.62s    remaining: 8.43s\n354:    learn: 0.0004672    test: 0.0024847 best: 0.0024277 (249)   total: 4.63s    remaining: 8.42s\n355:    learn: 0.0004647    test: 0.0024864 best: 0.0024277 (249)   total: 4.65s    remaining: 8.41s\n356:    learn: 0.0004626    test: 0.0024894 best: 0.0024277 (249)   total: 4.66s    remaining: 8.39s\n357:    learn: 0.0004623    test: 0.0024892 best: 0.0024277 (249)   total: 4.67s    remaining: 8.38s\n358:    learn: 0.0004611    test: 0.0024899 best: 0.0024277 (249)   total: 4.69s    remaining: 8.37s\n359:    learn: 0.0004600    test: 0.0024875 best: 0.0024277 (249)   total: 4.7s remaining: 8.35s\n360:    learn: 0.0004579    test: 0.0024898 best: 0.0024277 (249)   total: 4.71s    remaining: 8.34s\n361:    learn: 0.0004575    test: 0.0024909 best: 0.0024277 (249)   total: 4.72s    remaining: 8.33s\n362:    learn: 0.0004547    test: 0.0024912 best: 0.0024277 (249)   total: 4.74s    remaining: 8.31s\n363:    learn: 0.0004532    test: 0.0024968 best: 0.0024277 (249)   total: 4.75s    remaining: 8.3s\n364:    learn: 0.0004512    test: 0.0024953 best: 0.0024277 (249)   total: 4.76s    remaining: 8.29s\n365:    learn: 0.0004501    test: 0.0024960 best: 0.0024277 (249)   total: 4.78s    remaining: 8.27s\n366:    learn: 0.0004495    test: 0.0024945 best: 0.0024277 (249)   total: 4.79s    remaining: 8.26s\n367:    learn: 0.0004475    test: 0.0024966 best: 0.0024277 (249)   total: 4.8s remaining: 8.25s\n368:    learn: 0.0004464    test: 0.0024968 best: 0.0024277 (249)   total: 4.81s    remaining: 8.23s\n369:    learn: 0.0004459    test: 0.0024940 best: 0.0024277 (249)   total: 4.83s    remaining: 8.22s\n370:    learn: 0.0004438    test: 0.0024955 best: 0.0024277 (249)   total: 4.84s    remaining: 8.21s\n371:    learn: 0.0004434    test: 0.0024949 best: 0.0024277 (249)   total: 4.85s    remaining: 8.19s\n372:    learn: 0.0004417    test: 0.0024935 best: 0.0024277 (249)   total: 4.87s    remaining: 8.18s\n373:    learn: 0.0004415    test: 0.0024940 best: 0.0024277 (249)   total: 4.88s    remaining: 8.16s\n374:    learn: 0.0004406    test: 0.0024948 best: 0.0024277 (249)   total: 4.89s    remaining: 8.15s\n375:    learn: 0.0004401    test: 0.0024942 best: 0.0024277 (249)   total: 4.9s remaining: 8.14s\n376:    learn: 0.0004381    test: 0.0024948 best: 0.0024277 (249)   total: 4.92s    remaining: 8.12s\n377:    learn: 0.0004373    test: 0.0024945 best: 0.0024277 (249)   total: 4.93s    remaining: 8.11s\n378:    learn: 0.0004322    test: 0.0024991 best: 0.0024277 (249)   total: 4.94s    remaining: 8.1s\n379:    learn: 0.0004303    test: 0.0024993 best: 0.0024277 (249)   total: 4.95s    remaining: 8.08s\n380:    learn: 0.0004273    test: 0.0024981 best: 0.0024277 (249)   total: 4.96s    remaining: 8.07s\n381:    learn: 0.0004257    test: 0.0024962 best: 0.0024277 (249)   total: 4.98s    remaining: 8.06s\n382:    learn: 0.0004242    test: 0.0024977 best: 0.0024277 (249)   total: 4.99s    remaining: 8.04s\n383:    learn: 0.0004219    test: 0.0024972 best: 0.0024277 (249)   total: 5s   remaining: 8.03s\n384:    learn: 0.0004210    test: 0.0024963 best: 0.0024277 (249)   total: 5.02s    remaining: 8.02s\n385:    learn: 0.0004190    test: 0.0024973 best: 0.0024277 (249)   total: 5.03s    remaining: 8s\n386:    learn: 0.0004177    test: 0.0024987 best: 0.0024277 (249)   total: 5.04s    remaining: 7.99s\n387:    learn: 0.0004161    test: 0.0024968 best: 0.0024277 (249)   total: 5.05s    remaining: 7.97s\n388:    learn: 0.0004156    test: 0.0024946 best: 0.0024277 (249)   total: 5.07s    remaining: 7.96s\n389:    learn: 0.0004135    test: 0.0024964 best: 0.0024277 (249)   total: 5.08s    remaining: 7.95s\n390:    learn: 0.0004115    test: 0.0025013 best: 0.0024277 (249)   total: 5.09s    remaining: 7.93s\n391:    learn: 0.0004092    test: 0.0024981 best: 0.0024277 (249)   total: 5.11s    remaining: 7.92s\n392:    learn: 0.0004067    test: 0.0024972 best: 0.0024277 (249)   total: 5.12s    remaining: 7.91s\n393:    learn: 0.0004047    test: 0.0025012 best: 0.0024277 (249)   total: 5.14s    remaining: 7.9s\n394:    learn: 0.0004033    test: 0.0025025 best: 0.0024277 (249)   total: 5.15s    remaining: 7.89s\n395:    learn: 0.0004029    test: 0.0025010 best: 0.0024277 (249)   total: 5.16s    remaining: 7.87s\n396:    learn: 0.0003982    test: 0.0025010 best: 0.0024277 (249)   total: 5.17s    remaining: 7.86s\n397:    learn: 0.0003972    test: 0.0025035 best: 0.0024277 (249)   total: 5.19s    remaining: 7.85s\n398:    learn: 0.0003957    test: 0.0025018 best: 0.0024277 (249)   total: 5.2s remaining: 7.83s\n399:    learn: 0.0003949    test: 0.0025012 best: 0.0024277 (249)   total: 5.21s    remaining: 7.82s\n400:    learn: 0.0003932    test: 0.0025029 best: 0.0024277 (249)   total: 5.22s    remaining: 7.81s\n401:    learn: 0.0003917    test: 0.0025063 best: 0.0024277 (249)   total: 5.24s    remaining: 7.79s\n402:    learn: 0.0003905    test: 0.0025049 best: 0.0024277 (249)   total: 5.25s    remaining: 7.78s\n403:    learn: 0.0003891    test: 0.0025064 best: 0.0024277 (249)   total: 5.26s    remaining: 7.76s\n404:    learn: 0.0003874    test: 0.0025097 best: 0.0024277 (249)   total: 5.28s    remaining: 7.75s\n405:    learn: 0.0003854    test: 0.0025104 best: 0.0024277 (249)   total: 5.29s    remaining: 7.74s\n406:    learn: 0.0003845    test: 0.0025128 best: 0.0024277 (249)   total: 5.3s remaining: 7.72s\n407:    learn: 0.0003829    test: 0.0025121 best: 0.0024277 (249)   total: 5.32s    remaining: 7.71s\n408:    learn: 0.0003819    test: 0.0025111 best: 0.0024277 (249)   total: 5.33s    remaining: 7.7s\n409:    learn: 0.0003808    test: 0.0025145 best: 0.0024277 (249)   total: 5.34s    remaining: 7.69s\n410:    learn: 0.0003788    test: 0.0025140 best: 0.0024277 (249)   total: 5.36s    remaining: 7.67s\n411:    learn: 0.0003771    test: 0.0025155 best: 0.0024277 (249)   total: 5.37s    remaining: 7.66s\n412:    learn: 0.0003767    test: 0.0025157 best: 0.0024277 (249)   total: 5.38s    remaining: 7.65s\n413:    learn: 0.0003754    test: 0.0025192 best: 0.0024277 (249)   total: 5.4s remaining: 7.64s\n414:    learn: 0.0003742    test: 0.0025232 best: 0.0024277 (249)   total: 5.41s    remaining: 7.63s\n415:    learn: 0.0003720    test: 0.0025226 best: 0.0024277 (249)   total: 5.42s    remaining: 7.61s\n416:    learn: 0.0003694    test: 0.0025281 best: 0.0024277 (249)   total: 5.44s    remaining: 7.6s\n417:    learn: 0.0003677    test: 0.0025301 best: 0.0024277 (249)   total: 5.45s    remaining: 7.59s\n418:    learn: 0.0003659    test: 0.0025249 best: 0.0024277 (249)   total: 5.46s    remaining: 7.57s\n419:    learn: 0.0003636    test: 0.0025289 best: 0.0024277 (249)   total: 5.47s    remaining: 7.56s\n420:    learn: 0.0003592    test: 0.0025293 best: 0.0024277 (249)   total: 5.49s    remaining: 7.55s\n421:    learn: 0.0003588    test: 0.0025258 best: 0.0024277 (249)   total: 5.5s remaining: 7.53s\n422:    learn: 0.0003584    test: 0.0025256 best: 0.0024277 (249)   total: 5.51s    remaining: 7.52s\n423:    learn: 0.0003574    test: 0.0025275 best: 0.0024277 (249)   total: 5.53s    remaining: 7.51s\n424:    learn: 0.0003564    test: 0.0025261 best: 0.0024277 (249)   total: 5.54s    remaining: 7.49s\n425:    learn: 0.0003552    test: 0.0025277 best: 0.0024277 (249)   total: 5.55s    remaining: 7.48s\n426:    learn: 0.0003525    test: 0.0025304 best: 0.0024277 (249)   total: 5.56s    remaining: 7.46s\n427:    learn: 0.0003510    test: 0.0025319 best: 0.0024277 (249)   total: 5.58s    remaining: 7.45s\n428:    learn: 0.0003494    test: 0.0025286 best: 0.0024277 (249)   total: 5.59s    remaining: 7.44s\n429:    learn: 0.0003491    test: 0.0025286 best: 0.0024277 (249)   total: 5.6s remaining: 7.43s\n430:    learn: 0.0003487    test: 0.0025266 best: 0.0024277 (249)   total: 5.62s    remaining: 7.41s\n431:    learn: 0.0003444    test: 0.0025261 best: 0.0024277 (249)   total: 5.63s    remaining: 7.4s\n432:    learn: 0.0003438    test: 0.0025258 best: 0.0024277 (249)   total: 5.64s    remaining: 7.39s\n433:    learn: 0.0003418    test: 0.0025273 best: 0.0024277 (249)   total: 5.65s    remaining: 7.37s\n434:    learn: 0.0003407    test: 0.0025230 best: 0.0024277 (249)   total: 5.67s    remaining: 7.36s\n435:    learn: 0.0003398    test: 0.0025271 best: 0.0024277 (249)   total: 5.68s    remaining: 7.34s\n436:    learn: 0.0003396    test: 0.0025270 best: 0.0024277 (249)   total: 5.69s    remaining: 7.33s\n437:    learn: 0.0003374    test: 0.0025255 best: 0.0024277 (249)   total: 5.7s remaining: 7.32s\n438:    learn: 0.0003349    test: 0.0025247 best: 0.0024277 (249)   total: 5.72s    remaining: 7.31s\n439:    learn: 0.0003330    test: 0.0025238 best: 0.0024277 (249)   total: 5.73s    remaining: 7.29s\n440:    learn: 0.0003324    test: 0.0025250 best: 0.0024277 (249)   total: 5.74s    remaining: 7.28s\n441:    learn: 0.0003313    test: 0.0025264 best: 0.0024277 (249)   total: 5.75s    remaining: 7.26s\n442:    learn: 0.0003305    test: 0.0025231 best: 0.0024277 (249)   total: 5.77s    remaining: 7.25s\n443:    learn: 0.0003285    test: 0.0025274 best: 0.0024277 (249)   total: 5.78s    remaining: 7.24s\n444:    learn: 0.0003284    test: 0.0025275 best: 0.0024277 (249)   total: 5.79s    remaining: 7.22s\n445:    learn: 0.0003276    test: 0.0025264 best: 0.0024277 (249)   total: 5.8s remaining: 7.21s\n446:    learn: 0.0003269    test: 0.0025289 best: 0.0024277 (249)   total: 5.82s    remaining: 7.2s\n447:    learn: 0.0003220    test: 0.0025350 best: 0.0024277 (249)   total: 5.83s    remaining: 7.18s\n448:    learn: 0.0003206    test: 0.0025337 best: 0.0024277 (249)   total: 5.84s    remaining: 7.17s\n449:    learn: 0.0003199    test: 0.0025322 best: 0.0024277 (249)   total: 5.86s    remaining: 7.16s\n450:    learn: 0.0003178    test: 0.0025328 best: 0.0024277 (249)   total: 5.87s    remaining: 7.14s\n451:    learn: 0.0003172    test: 0.0025334 best: 0.0024277 (249)   total: 5.88s    remaining: 7.13s\n452:    learn: 0.0003166    test: 0.0025328 best: 0.0024277 (249)   total: 5.9s remaining: 7.12s\n453:    learn: 0.0003152    test: 0.0025293 best: 0.0024277 (249)   total: 5.91s    remaining: 7.11s\n454:    learn: 0.0003148    test: 0.0025282 best: 0.0024277 (249)   total: 5.93s    remaining: 7.1s\n455:    learn: 0.0003142    test: 0.0025287 best: 0.0024277 (249)   total: 5.95s    remaining: 7.09s\n456:    learn: 0.0003139    test: 0.0025286 best: 0.0024277 (249)   total: 5.96s    remaining: 7.08s\n457:    learn: 0.0003134    test: 0.0025306 best: 0.0024277 (249)   total: 5.97s    remaining: 7.07s\n458:    learn: 0.0003130    test: 0.0025307 best: 0.0024277 (249)   total: 5.99s    remaining: 7.06s\n459:    learn: 0.0003127    test: 0.0025306 best: 0.0024277 (249)   total: 6s   remaining: 7.05s\n460:    learn: 0.0003123    test: 0.0025307 best: 0.0024277 (249)   total: 6.02s    remaining: 7.04s\n461:    learn: 0.0003115    test: 0.0025274 best: 0.0024277 (249)   total: 6.04s    remaining: 7.03s\n462:    learn: 0.0003110    test: 0.0025281 best: 0.0024277 (249)   total: 6.05s    remaining: 7.02s\n463:    learn: 0.0003102    test: 0.0025338 best: 0.0024277 (249)   total: 6.07s    remaining: 7.01s\n464:    learn: 0.0003094    test: 0.0025358 best: 0.0024277 (249)   total: 6.08s    remaining: 6.99s\n465:    learn: 0.0003088    test: 0.0025364 best: 0.0024277 (249)   total: 6.09s    remaining: 6.98s\n466:    learn: 0.0003084    test: 0.0025369 best: 0.0024277 (249)   total: 6.1s remaining: 6.97s\n467:    learn: 0.0003079    test: 0.0025377 best: 0.0024277 (249)   total: 6.12s    remaining: 6.95s\n468:    learn: 0.0003077    test: 0.0025387 best: 0.0024277 (249)   total: 6.13s    remaining: 6.94s\n469:    learn: 0.0003065    test: 0.0025399 best: 0.0024277 (249)   total: 6.14s    remaining: 6.93s\n470:    learn: 0.0003040    test: 0.0025398 best: 0.0024277 (249)   total: 6.16s    remaining: 6.92s\n471:    learn: 0.0003026    test: 0.0025379 best: 0.0024277 (249)   total: 6.17s    remaining: 6.9s\n472:    learn: 0.0003015    test: 0.0025390 best: 0.0024277 (249)   total: 6.19s    remaining: 6.89s\n473:    learn: 0.0003002    test: 0.0025398 best: 0.0024277 (249)   total: 6.2s remaining: 6.88s\n474:    learn: 0.0002984    test: 0.0025426 best: 0.0024277 (249)   total: 6.21s    remaining: 6.87s\n475:    learn: 0.0002972    test: 0.0025416 best: 0.0024277 (249)   total: 6.23s    remaining: 6.86s\n476:    learn: 0.0002965    test: 0.0025414 best: 0.0024277 (249)   total: 6.24s    remaining: 6.84s\n477:    learn: 0.0002956    test: 0.0025452 best: 0.0024277 (249)   total: 6.25s    remaining: 6.83s\n478:    learn: 0.0002945    test: 0.0025432 best: 0.0024277 (249)   total: 6.27s    remaining: 6.82s\n479:    learn: 0.0002924    test: 0.0025469 best: 0.0024277 (249)   total: 6.28s    remaining: 6.8s\n480:    learn: 0.0002918    test: 0.0025482 best: 0.0024277 (249)   total: 6.29s    remaining: 6.79s\n481:    learn: 0.0002916    test: 0.0025481 best: 0.0024277 (249)   total: 6.31s    remaining: 6.78s\n482:    learn: 0.0002911    test: 0.0025482 best: 0.0024277 (249)   total: 6.32s    remaining: 6.76s\n483:    learn: 0.0002902    test: 0.0025498 best: 0.0024277 (249)   total: 6.33s    remaining: 6.75s\n484:    learn: 0.0002872    test: 0.0025501 best: 0.0024277 (249)   total: 6.35s    remaining: 6.74s\n485:    learn: 0.0002859    test: 0.0025506 best: 0.0024277 (249)   total: 6.36s    remaining: 6.72s\n486:    learn: 0.0002838    test: 0.0025516 best: 0.0024277 (249)   total: 6.37s    remaining: 6.71s\n487:    learn: 0.0002793    test: 0.0025516 best: 0.0024277 (249)   total: 6.38s    remaining: 6.7s\n488:    learn: 0.0002787    test: 0.0025514 best: 0.0024277 (249)   total: 6.4s remaining: 6.69s\n489:    learn: 0.0002745    test: 0.0025566 best: 0.0024277 (249)   total: 6.41s    remaining: 6.67s\n490:    learn: 0.0002736    test: 0.0025572 best: 0.0024277 (249)   total: 6.43s    remaining: 6.66s\n491:    learn: 0.0002715    test: 0.0025626 best: 0.0024277 (249)   total: 6.44s    remaining: 6.65s\n492:    learn: 0.0002708    test: 0.0025623 best: 0.0024277 (249)   total: 6.45s    remaining: 6.63s\n493:    learn: 0.0002690    test: 0.0025629 best: 0.0024277 (249)   total: 6.46s    remaining: 6.62s\n494:    learn: 0.0002685    test: 0.0025636 best: 0.0024277 (249)   total: 6.48s    remaining: 6.61s\n495:    learn: 0.0002668    test: 0.0025661 best: 0.0024277 (249)   total: 6.49s    remaining: 6.59s\n496:    learn: 0.0002664    test: 0.0025653 best: 0.0024277 (249)   total: 6.5s remaining: 6.58s\n497:    learn: 0.0002660    test: 0.0025667 best: 0.0024277 (249)   total: 6.52s    remaining: 6.57s\n498:    learn: 0.0002653    test: 0.0025683 best: 0.0024277 (249)   total: 6.53s    remaining: 6.55s\n499:    learn: 0.0002645    test: 0.0025692 best: 0.0024277 (249)   total: 6.54s    remaining: 6.54s\n500:    learn: 0.0002631    test: 0.0025664 best: 0.0024277 (249)   total: 6.55s    remaining: 6.53s\n501:    learn: 0.0002610    test: 0.0025682 best: 0.0024277 (249)   total: 6.57s    remaining: 6.51s\n502:    learn: 0.0002604    test: 0.0025679 best: 0.0024277 (249)   total: 6.58s    remaining: 6.5s\n503:    learn: 0.0002594    test: 0.0025705 best: 0.0024277 (249)   total: 6.59s    remaining: 6.49s\n504:    learn: 0.0002584    test: 0.0025702 best: 0.0024277 (249)   total: 6.61s    remaining: 6.48s\n505:    learn: 0.0002566    test: 0.0025719 best: 0.0024277 (249)   total: 6.62s    remaining: 6.46s\n506:    learn: 0.0002565    test: 0.0025714 best: 0.0024277 (249)   total: 6.63s    remaining: 6.45s\n507:    learn: 0.0002554    test: 0.0025739 best: 0.0024277 (249)   total: 6.65s    remaining: 6.44s\n508:    learn: 0.0002548    test: 0.0025756 best: 0.0024277 (249)   total: 6.66s    remaining: 6.42s\n509:    learn: 0.0002521    test: 0.0025775 best: 0.0024277 (249)   total: 6.67s    remaining: 6.41s\n510:    learn: 0.0002515    test: 0.0025779 best: 0.0024277 (249)   total: 6.68s    remaining: 6.4s\n511:    learn: 0.0002510    test: 0.0025787 best: 0.0024277 (249)   total: 6.7s remaining: 6.38s\n512:    learn: 0.0002505    test: 0.0025789 best: 0.0024277 (249)   total: 6.71s    remaining: 6.37s\n513:    learn: 0.0002497    test: 0.0025774 best: 0.0024277 (249)   total: 6.72s    remaining: 6.36s\n514:    learn: 0.0002495    test: 0.0025781 best: 0.0024277 (249)   total: 6.73s    remaining: 6.34s\n515:    learn: 0.0002490    test: 0.0025771 best: 0.0024277 (249)   total: 6.75s    remaining: 6.33s\n516:    learn: 0.0002487    test: 0.0025779 best: 0.0024277 (249)   total: 6.76s    remaining: 6.31s\n517:    learn: 0.0002485    test: 0.0025794 best: 0.0024277 (249)   total: 6.77s    remaining: 6.3s\n518:    learn: 0.0002475    test: 0.0025838 best: 0.0024277 (249)   total: 6.78s    remaining: 6.29s\n519:    learn: 0.0002470    test: 0.0025830 best: 0.0024277 (249)   total: 6.8s remaining: 6.27s\n520:    learn: 0.0002452    test: 0.0025815 best: 0.0024277 (249)   total: 6.81s    remaining: 6.26s\n521:    learn: 0.0002447    test: 0.0025859 best: 0.0024277 (249)   total: 6.82s    remaining: 6.25s\n522:    learn: 0.0002440    test: 0.0025884 best: 0.0024277 (249)   total: 6.84s    remaining: 6.24s\n523:    learn: 0.0002437    test: 0.0025876 best: 0.0024277 (249)   total: 6.85s    remaining: 6.22s\n524:    learn: 0.0002431    test: 0.0025865 best: 0.0024277 (249)   total: 6.86s    remaining: 6.21s\n525:    learn: 0.0002411    test: 0.0025868 best: 0.0024277 (249)   total: 6.88s    remaining: 6.2s\n526:    learn: 0.0002409    test: 0.0025868 best: 0.0024277 (249)   total: 6.89s    remaining: 6.18s\n527:    learn: 0.0002405    test: 0.0025872 best: 0.0024277 (249)   total: 6.9s remaining: 6.17s\n528:    learn: 0.0002401    test: 0.0025902 best: 0.0024277 (249)   total: 6.92s    remaining: 6.16s\n529:    learn: 0.0002399    test: 0.0025915 best: 0.0024277 (249)   total: 6.93s    remaining: 6.15s\n530:    learn: 0.0002393    test: 0.0025917 best: 0.0024277 (249)   total: 6.96s    remaining: 6.14s\n531:    learn: 0.0002373    test: 0.0025961 best: 0.0024277 (249)   total: 6.97s    remaining: 6.13s\n532:    learn: 0.0002371    test: 0.0025966 best: 0.0024277 (249)   total: 6.98s    remaining: 6.12s\n533:    learn: 0.0002356    test: 0.0025981 best: 0.0024277 (249)   total: 7s   remaining: 6.1s\n534:    learn: 0.0002346    test: 0.0025986 best: 0.0024277 (249)   total: 7.01s    remaining: 6.09s\n535:    learn: 0.0002330    test: 0.0026042 best: 0.0024277 (249)   total: 7.02s    remaining: 6.08s\n536:    learn: 0.0002318    test: 0.0026030 best: 0.0024277 (249)   total: 7.04s    remaining: 6.07s\n537:    learn: 0.0002314    test: 0.0026021 best: 0.0024277 (249)   total: 7.05s    remaining: 6.05s\n538:    learn: 0.0002306    test: 0.0026025 best: 0.0024277 (249)   total: 7.06s    remaining: 6.04s\n539:    learn: 0.0002301    test: 0.0026069 best: 0.0024277 (249)   total: 7.07s    remaining: 6.03s\n540:    learn: 0.0002298    test: 0.0026075 best: 0.0024277 (249)   total: 7.09s    remaining: 6.01s\n541:    learn: 0.0002298    test: 0.0026079 best: 0.0024277 (249)   total: 7.1s remaining: 6s\n542:    learn: 0.0002251    test: 0.0026148 best: 0.0024277 (249)   total: 7.11s    remaining: 5.99s\n543:    learn: 0.0002245    test: 0.0026180 best: 0.0024277 (249)   total: 7.13s    remaining: 5.97s\n544:    learn: 0.0002242    test: 0.0026200 best: 0.0024277 (249)   total: 7.14s    remaining: 5.96s\n545:    learn: 0.0002240    test: 0.0026197 best: 0.0024277 (249)   total: 7.15s    remaining: 5.95s\n546:    learn: 0.0002238    test: 0.0026207 best: 0.0024277 (249)   total: 7.17s    remaining: 5.93s\n547:    learn: 0.0002230    test: 0.0026230 best: 0.0024277 (249)   total: 7.18s    remaining: 5.92s\n548:    learn: 0.0002207    test: 0.0026203 best: 0.0024277 (249)   total: 7.19s    remaining: 5.91s\n549:    learn: 0.0002200    test: 0.0026163 best: 0.0024277 (249)   total: 7.2s remaining: 5.89s\n550:    learn: 0.0002183    test: 0.0026190 best: 0.0024277 (249)   total: 7.21s    remaining: 5.88s\n551:    learn: 0.0002176    test: 0.0026195 best: 0.0024277 (249)   total: 7.23s    remaining: 5.87s\n552:    learn: 0.0002173    test: 0.0026201 best: 0.0024277 (249)   total: 7.24s    remaining: 5.86s\n553:    learn: 0.0002157    test: 0.0026200 best: 0.0024277 (249)   total: 7.26s    remaining: 5.84s\n554:    learn: 0.0002152    test: 0.0026208 best: 0.0024277 (249)   total: 7.27s    remaining: 5.83s\n555:    learn: 0.0002142    test: 0.0026234 best: 0.0024277 (249)   total: 7.29s    remaining: 5.82s\n556:    learn: 0.0002125    test: 0.0026248 best: 0.0024277 (249)   total: 7.3s remaining: 5.8s\n557:    learn: 0.0002106    test: 0.0026259 best: 0.0024277 (249)   total: 7.31s    remaining: 5.79s\n558:    learn: 0.0002103    test: 0.0026299 best: 0.0024277 (249)   total: 7.33s    remaining: 5.78s\n559:    learn: 0.0002101    test: 0.0026297 best: 0.0024277 (249)   total: 7.34s    remaining: 5.76s\n560:    learn: 0.0002099    test: 0.0026305 best: 0.0024277 (249)   total: 7.35s    remaining: 5.75s\n561:    learn: 0.0002095    test: 0.0026326 best: 0.0024277 (249)   total: 7.36s    remaining: 5.74s\n562:    learn: 0.0002092    test: 0.0026344 best: 0.0024277 (249)   total: 7.38s    remaining: 5.72s\n563:    learn: 0.0002091    test: 0.0026345 best: 0.0024277 (249)   total: 7.39s    remaining: 5.71s\n564:    learn: 0.0002084    test: 0.0026351 best: 0.0024277 (249)   total: 7.4s remaining: 5.7s\n565:    learn: 0.0002081    test: 0.0026363 best: 0.0024277 (249)   total: 7.41s    remaining: 5.68s\n566:    learn: 0.0002054    test: 0.0026412 best: 0.0024277 (249)   total: 7.43s    remaining: 5.67s\n567:    learn: 0.0002051    test: 0.0026424 best: 0.0024277 (249)   total: 7.44s    remaining: 5.66s\n568:    learn: 0.0002049    test: 0.0026431 best: 0.0024277 (249)   total: 7.45s    remaining: 5.64s\n569:    learn: 0.0002047    test: 0.0026421 best: 0.0024277 (249)   total: 7.47s    remaining: 5.63s\n570:    learn: 0.0002019    test: 0.0026451 best: 0.0024277 (249)   total: 7.48s    remaining: 5.62s\n571:    learn: 0.0002017    test: 0.0026442 best: 0.0024277 (249)   total: 7.5s remaining: 5.61s\n572:    learn: 0.0002017    test: 0.0026446 best: 0.0024277 (249)   total: 7.51s    remaining: 5.6s\n573:    learn: 0.0002014    test: 0.0026464 best: 0.0024277 (249)   total: 7.52s    remaining: 5.58s\n574:    learn: 0.0002003    test: 0.0026481 best: 0.0024277 (249)   total: 7.54s    remaining: 5.57s\n575:    learn: 0.0002001    test: 0.0026489 best: 0.0024277 (249)   total: 7.55s    remaining: 5.56s\n576:    learn: 0.0001998    test: 0.0026485 best: 0.0024277 (249)   total: 7.56s    remaining: 5.54s\n577:    learn: 0.0001997    test: 0.0026477 best: 0.0024277 (249)   total: 7.57s    remaining: 5.53s\n578:    learn: 0.0001991    test: 0.0026519 best: 0.0024277 (249)   total: 7.59s    remaining: 5.52s\n579:    learn: 0.0001990    test: 0.0026521 best: 0.0024277 (249)   total: 7.6s remaining: 5.5s\n580:    learn: 0.0001989    test: 0.0026524 best: 0.0024277 (249)   total: 7.61s    remaining: 5.49s\n581:    learn: 0.0001986    test: 0.0026543 best: 0.0024277 (249)   total: 7.62s    remaining: 5.47s\n582:    learn: 0.0001984    test: 0.0026540 best: 0.0024277 (249)   total: 7.64s    remaining: 5.46s\n583:    learn: 0.0001978    test: 0.0026566 best: 0.0024277 (249)   total: 7.65s    remaining: 5.45s\n584:    learn: 0.0001977    test: 0.0026569 best: 0.0024277 (249)   total: 7.66s    remaining: 5.44s\n585:    learn: 0.0001967    test: 0.0026599 best: 0.0024277 (249)   total: 7.68s    remaining: 5.42s\n586:    learn: 0.0001962    test: 0.0026612 best: 0.0024277 (249)   total: 7.69s    remaining: 5.41s\n587:    learn: 0.0001959    test: 0.0026623 best: 0.0024277 (249)   total: 7.7s remaining: 5.4s\n588:    learn: 0.0001957    test: 0.0026631 best: 0.0024277 (249)   total: 7.71s    remaining: 5.38s\n589:    learn: 0.0001943    test: 0.0026633 best: 0.0024277 (249)   total: 7.73s    remaining: 5.37s\n590:    learn: 0.0001940    test: 0.0026642 best: 0.0024277 (249)   total: 7.74s    remaining: 5.36s\n591:    learn: 0.0001935    test: 0.0026608 best: 0.0024277 (249)   total: 7.75s    remaining: 5.34s\n592:    learn: 0.0001919    test: 0.0026622 best: 0.0024277 (249)   total: 7.77s    remaining: 5.33s\n593:    learn: 0.0001917    test: 0.0026626 best: 0.0024277 (249)   total: 7.78s    remaining: 5.32s\n594:    learn: 0.0001915    test: 0.0026631 best: 0.0024277 (249)   total: 7.79s    remaining: 5.3s\n595:    learn: 0.0001909    test: 0.0026673 best: 0.0024277 (249)   total: 7.81s    remaining: 5.29s\n596:    learn: 0.0001904    test: 0.0026664 best: 0.0024277 (249)   total: 7.82s    remaining: 5.28s\n597:    learn: 0.0001904    test: 0.0026661 best: 0.0024277 (249)   total: 7.83s    remaining: 5.26s\n598:    learn: 0.0001902    test: 0.0026659 best: 0.0024277 (249)   total: 7.84s    remaining: 5.25s\n599:    learn: 0.0001900    test: 0.0026668 best: 0.0024277 (249)   total: 7.86s    remaining: 5.24s\n600:    learn: 0.0001894    test: 0.0026654 best: 0.0024277 (249)   total: 7.87s    remaining: 5.22s\n601:    learn: 0.0001891    test: 0.0026667 best: 0.0024277 (249)   total: 7.88s    remaining: 5.21s\n602:    learn: 0.0001886    test: 0.0026672 best: 0.0024277 (249)   total: 7.9s remaining: 5.2s\n603:    learn: 0.0001873    test: 0.0026656 best: 0.0024277 (249)   total: 7.91s    remaining: 5.19s\n604:    learn: 0.0001869    test: 0.0026624 best: 0.0024277 (249)   total: 7.92s    remaining: 5.17s\n605:    learn: 0.0001867    test: 0.0026634 best: 0.0024277 (249)   total: 7.94s    remaining: 5.16s\n606:    learn: 0.0001865    test: 0.0026645 best: 0.0024277 (249)   total: 7.95s    remaining: 5.15s\n607:    learn: 0.0001859    test: 0.0026668 best: 0.0024277 (249)   total: 7.96s    remaining: 5.13s\n608:    learn: 0.0001849    test: 0.0026675 best: 0.0024277 (249)   total: 7.97s    remaining: 5.12s\n609:    learn: 0.0001848    test: 0.0026680 best: 0.0024277 (249)   total: 7.99s    remaining: 5.11s\n610:    learn: 0.0001842    test: 0.0026693 best: 0.0024277 (249)   total: 8s   remaining: 5.09s\n611:    learn: 0.0001835    test: 0.0026683 best: 0.0024277 (249)   total: 8.02s    remaining: 5.08s\n612:    learn: 0.0001823    test: 0.0026610 best: 0.0024277 (249)   total: 8.03s    remaining: 5.07s\n613:    learn: 0.0001822    test: 0.0026613 best: 0.0024277 (249)   total: 8.05s    remaining: 5.06s\n614:    learn: 0.0001814    test: 0.0026619 best: 0.0024277 (249)   total: 8.07s    remaining: 5.05s\n615:    learn: 0.0001808    test: 0.0026622 best: 0.0024277 (249)   total: 8.09s    remaining: 5.04s\n616:    learn: 0.0001802    test: 0.0026617 best: 0.0024277 (249)   total: 8.1s remaining: 5.03s\n617:    learn: 0.0001799    test: 0.0026637 best: 0.0024277 (249)   total: 8.11s    remaining: 5.01s\n618:    learn: 0.0001797    test: 0.0026646 best: 0.0024277 (249)   total: 8.13s    remaining: 5s\n619:    learn: 0.0001795    test: 0.0026644 best: 0.0024277 (249)   total: 8.14s    remaining: 4.99s\n620:    learn: 0.0001794    test: 0.0026648 best: 0.0024277 (249)   total: 8.15s    remaining: 4.97s\n621:    learn: 0.0001787    test: 0.0026666 best: 0.0024277 (249)   total: 8.16s    remaining: 4.96s\n622:    learn: 0.0001775    test: 0.0026705 best: 0.0024277 (249)   total: 8.18s    remaining: 4.95s\n623:    learn: 0.0001774    test: 0.0026707 best: 0.0024277 (249)   total: 8.19s    remaining: 4.93s\n624:    learn: 0.0001772    test: 0.0026710 best: 0.0024277 (249)   total: 8.2s remaining: 4.92s\n625:    learn: 0.0001770    test: 0.0026709 best: 0.0024277 (249)   total: 8.21s    remaining: 4.91s\n626:    learn: 0.0001766    test: 0.0026720 best: 0.0024277 (249)   total: 8.22s    remaining: 4.89s\n627:    learn: 0.0001761    test: 0.0026723 best: 0.0024277 (249)   total: 8.24s    remaining: 4.88s\n628:    learn: 0.0001754    test: 0.0026726 best: 0.0024277 (249)   total: 8.25s    remaining: 4.87s\n629:    learn: 0.0001745    test: 0.0026749 best: 0.0024277 (249)   total: 8.26s    remaining: 4.85s\n630:    learn: 0.0001743    test: 0.0026727 best: 0.0024277 (249)   total: 8.28s    remaining: 4.84s\n631:    learn: 0.0001731    test: 0.0026730 best: 0.0024277 (249)   total: 8.29s    remaining: 4.83s\n632:    learn: 0.0001727    test: 0.0026736 best: 0.0024277 (249)   total: 8.31s    remaining: 4.82s\n633:    learn: 0.0001721    test: 0.0026727 best: 0.0024277 (249)   total: 8.32s    remaining: 4.8s\n634:    learn: 0.0001719    test: 0.0026735 best: 0.0024277 (249)   total: 8.33s    remaining: 4.79s\n635:    learn: 0.0001711    test: 0.0026717 best: 0.0024277 (249)   total: 8.35s    remaining: 4.78s\n636:    learn: 0.0001706    test: 0.0026731 best: 0.0024277 (249)   total: 8.36s    remaining: 4.76s\n637:    learn: 0.0001704    test: 0.0026737 best: 0.0024277 (249)   total: 8.37s    remaining: 4.75s\n638:    learn: 0.0001703    test: 0.0026742 best: 0.0024277 (249)   total: 8.38s    remaining: 4.74s\n639:    learn: 0.0001695    test: 0.0026749 best: 0.0024277 (249)   total: 8.4s remaining: 4.72s\n640:    learn: 0.0001685    test: 0.0026774 best: 0.0024277 (249)   total: 8.41s    remaining: 4.71s\n641:    learn: 0.0001681    test: 0.0026777 best: 0.0024277 (249)   total: 8.42s    remaining: 4.7s\n642:    learn: 0.0001674    test: 0.0026779 best: 0.0024277 (249)   total: 8.43s    remaining: 4.68s\n643:    learn: 0.0001669    test: 0.0026797 best: 0.0024277 (249)   total: 8.45s    remaining: 4.67s\n644:    learn: 0.0001666    test: 0.0026804 best: 0.0024277 (249)   total: 8.46s    remaining: 4.66s\n645:    learn: 0.0001666    test: 0.0026809 best: 0.0024277 (249)   total: 8.47s    remaining: 4.64s\n646:    learn: 0.0001650    test: 0.0026842 best: 0.0024277 (249)   total: 8.49s    remaining: 4.63s\n647:    learn: 0.0001643    test: 0.0026860 best: 0.0024277 (249)   total: 8.5s remaining: 4.62s\n648:    learn: 0.0001641    test: 0.0026862 best: 0.0024277 (249)   total: 8.51s    remaining: 4.6s\n649:    learn: 0.0001637    test: 0.0026891 best: 0.0024277 (249)   total: 8.53s    remaining: 4.59s\n650:    learn: 0.0001633    test: 0.0026897 best: 0.0024277 (249)   total: 8.54s    remaining: 4.58s\n651:    learn: 0.0001621    test: 0.0026922 best: 0.0024277 (249)   total: 8.55s    remaining: 4.57s\n652:    learn: 0.0001621    test: 0.0026922 best: 0.0024277 (249)   total: 8.56s    remaining: 4.55s\n653:    learn: 0.0001620    test: 0.0026923 best: 0.0024277 (249)   total: 8.58s    remaining: 4.54s\n654:    learn: 0.0001615    test: 0.0026919 best: 0.0024277 (249)   total: 8.59s    remaining: 4.52s\n655:    learn: 0.0001612    test: 0.0026943 best: 0.0024277 (249)   total: 8.6s remaining: 4.51s\n656:    learn: 0.0001606    test: 0.0026927 best: 0.0024277 (249)   total: 8.61s    remaining: 4.5s\n657:    learn: 0.0001605    test: 0.0026934 best: 0.0024277 (249)   total: 8.63s    remaining: 4.48s\n658:    learn: 0.0001602    test: 0.0026939 best: 0.0024277 (249)   total: 8.64s    remaining: 4.47s\n659:    learn: 0.0001595    test: 0.0026934 best: 0.0024277 (249)   total: 8.65s    remaining: 4.46s\n660:    learn: 0.0001591    test: 0.0026932 best: 0.0024277 (249)   total: 8.66s    remaining: 4.44s\n661:    learn: 0.0001587    test: 0.0026928 best: 0.0024277 (249)   total: 8.68s    remaining: 4.43s\n662:    learn: 0.0001584    test: 0.0026946 best: 0.0024277 (249)   total: 8.69s    remaining: 4.42s\n663:    learn: 0.0001575    test: 0.0026936 best: 0.0024277 (249)   total: 8.7s remaining: 4.4s\n664:    learn: 0.0001571    test: 0.0026944 best: 0.0024277 (249)   total: 8.72s    remaining: 4.39s\n665:    learn: 0.0001570    test: 0.0026958 best: 0.0024277 (249)   total: 8.73s    remaining: 4.38s\n666:    learn: 0.0001566    test: 0.0026997 best: 0.0024277 (249)   total: 8.74s    remaining: 4.37s\n667:    learn: 0.0001564    test: 0.0027026 best: 0.0024277 (249)   total: 8.76s    remaining: 4.35s\n668:    learn: 0.0001563    test: 0.0027029 best: 0.0024277 (249)   total: 8.77s    remaining: 4.34s\n669:    learn: 0.0001554    test: 0.0027030 best: 0.0024277 (249)   total: 8.78s    remaining: 4.33s\n670:    learn: 0.0001553    test: 0.0027031 best: 0.0024277 (249)   total: 8.79s    remaining: 4.31s\n671:    learn: 0.0001549    test: 0.0027028 best: 0.0024277 (249)   total: 8.81s    remaining: 4.3s\n672:    learn: 0.0001547    test: 0.0027039 best: 0.0024277 (249)   total: 8.82s    remaining: 4.29s\n673:    learn: 0.0001543    test: 0.0027044 best: 0.0024277 (249)   total: 8.83s    remaining: 4.27s\n674:    learn: 0.0001529    test: 0.0027067 best: 0.0024277 (249)   total: 8.85s    remaining: 4.26s\n675:    learn: 0.0001527    test: 0.0027069 best: 0.0024277 (249)   total: 8.86s    remaining: 4.25s\n676:    learn: 0.0001521    test: 0.0027065 best: 0.0024277 (249)   total: 8.87s    remaining: 4.23s\n677:    learn: 0.0001519    test: 0.0027080 best: 0.0024277 (249)   total: 8.88s    remaining: 4.22s\n678:    learn: 0.0001516    test: 0.0027081 best: 0.0024277 (249)   total: 8.9s remaining: 4.21s\n679:    learn: 0.0001512    test: 0.0027071 best: 0.0024277 (249)   total: 8.91s    remaining: 4.19s\n680:    learn: 0.0001503    test: 0.0027086 best: 0.0024277 (249)   total: 8.92s    remaining: 4.18s\n681:    learn: 0.0001496    test: 0.0027082 best: 0.0024277 (249)   total: 8.94s    remaining: 4.17s\n682:    learn: 0.0001496    test: 0.0027082 best: 0.0024277 (249)   total: 8.95s    remaining: 4.15s\n683:    learn: 0.0001492    test: 0.0027095 best: 0.0024277 (249)   total: 8.96s    remaining: 4.14s\n684:    learn: 0.0001491    test: 0.0027100 best: 0.0024277 (249)   total: 8.97s    remaining: 4.13s\n685:    learn: 0.0001474    test: 0.0027132 best: 0.0024277 (249)   total: 8.98s    remaining: 4.11s\n686:    learn: 0.0001474    test: 0.0027131 best: 0.0024277 (249)   total: 9s   remaining: 4.1s\n687:    learn: 0.0001472    test: 0.0027145 best: 0.0024277 (249)   total: 9.01s    remaining: 4.08s\n688:    learn: 0.0001467    test: 0.0027167 best: 0.0024277 (249)   total: 9.02s    remaining: 4.07s\n689:    learn: 0.0001462    test: 0.0027148 best: 0.0024277 (249)   total: 9.03s    remaining: 4.06s\n690:    learn: 0.0001460    test: 0.0027158 best: 0.0024277 (249)   total: 9.05s    remaining: 4.04s\n691:    learn: 0.0001459    test: 0.0027162 best: 0.0024277 (249)   total: 9.06s    remaining: 4.03s\n692:    learn: 0.0001457    test: 0.0027176 best: 0.0024277 (249)   total: 9.07s    remaining: 4.02s\n693:    learn: 0.0001456    test: 0.0027187 best: 0.0024277 (249)   total: 9.08s    remaining: 4s\n694:    learn: 0.0001454    test: 0.0027187 best: 0.0024277 (249)   total: 9.1s remaining: 3.99s\n695:    learn: 0.0001450    test: 0.0027188 best: 0.0024277 (249)   total: 9.11s    remaining: 3.98s\n696:    learn: 0.0001446    test: 0.0027214 best: 0.0024277 (249)   total: 9.12s    remaining: 3.96s\n697:    learn: 0.0001445    test: 0.0027221 best: 0.0024277 (249)   total: 9.14s    remaining: 3.95s\n698:    learn: 0.0001440    test: 0.0027236 best: 0.0024277 (249)   total: 9.15s    remaining: 3.94s\n699:    learn: 0.0001440    test: 0.0027239 best: 0.0024277 (249)   total: 9.16s    remaining: 3.93s\n700:    learn: 0.0001438    test: 0.0027249 best: 0.0024277 (249)   total: 9.17s    remaining: 3.91s\n701:    learn: 0.0001435    test: 0.0027249 best: 0.0024277 (249)   total: 9.19s    remaining: 3.9s\n702:    learn: 0.0001432    test: 0.0027239 best: 0.0024277 (249)   total: 9.2s remaining: 3.89s\n703:    learn: 0.0001431    test: 0.0027237 best: 0.0024277 (249)   total: 9.21s    remaining: 3.87s\n704:    learn: 0.0001431    test: 0.0027241 best: 0.0024277 (249)   total: 9.22s    remaining: 3.86s\n705:    learn: 0.0001426    test: 0.0027243 best: 0.0024277 (249)   total: 9.24s    remaining: 3.85s\n706:    learn: 0.0001422    test: 0.0027268 best: 0.0024277 (249)   total: 9.25s    remaining: 3.83s\n707:    learn: 0.0001419    test: 0.0027273 best: 0.0024277 (249)   total: 9.27s    remaining: 3.82s\n708:    learn: 0.0001417    test: 0.0027280 best: 0.0024277 (249)   total: 9.28s    remaining: 3.81s\n709:    learn: 0.0001399    test: 0.0027335 best: 0.0024277 (249)   total: 9.29s    remaining: 3.8s\n710:    learn: 0.0001398    test: 0.0027340 best: 0.0024277 (249)   total: 9.31s    remaining: 3.78s\n711:    learn: 0.0001396    test: 0.0027358 best: 0.0024277 (249)   total: 9.32s    remaining: 3.77s\n712:    learn: 0.0001391    test: 0.0027381 best: 0.0024277 (249)   total: 9.34s    remaining: 3.76s\n713:    learn: 0.0001390    test: 0.0027380 best: 0.0024277 (249)   total: 9.35s    remaining: 3.75s\n714:    learn: 0.0001384    test: 0.0027416 best: 0.0024277 (249)   total: 9.37s    remaining: 3.73s\n715:    learn: 0.0001377    test: 0.0027441 best: 0.0024277 (249)   total: 9.38s    remaining: 3.72s\n716:    learn: 0.0001376    test: 0.0027455 best: 0.0024277 (249)   total: 9.39s    remaining: 3.71s\n717:    learn: 0.0001374    test: 0.0027465 best: 0.0024277 (249)   total: 9.4s remaining: 3.69s\n718:    learn: 0.0001374    test: 0.0027469 best: 0.0024277 (249)   total: 9.42s    remaining: 3.68s\n719:    learn: 0.0001368    test: 0.0027482 best: 0.0024277 (249)   total: 9.43s    remaining: 3.67s\n720:    learn: 0.0001363    test: 0.0027462 best: 0.0024277 (249)   total: 9.44s    remaining: 3.65s\n721:    learn: 0.0001362    test: 0.0027469 best: 0.0024277 (249)   total: 9.45s    remaining: 3.64s\n722:    learn: 0.0001361    test: 0.0027481 best: 0.0024277 (249)   total: 9.47s    remaining: 3.63s\n723:    learn: 0.0001356    test: 0.0027469 best: 0.0024277 (249)   total: 9.48s    remaining: 3.61s\n724:    learn: 0.0001354    test: 0.0027458 best: 0.0024277 (249)   total: 9.49s    remaining: 3.6s\n725:    learn: 0.0001353    test: 0.0027462 best: 0.0024277 (249)   total: 9.5s remaining: 3.59s\n726:    learn: 0.0001351    test: 0.0027477 best: 0.0024277 (249)   total: 9.52s    remaining: 3.57s\n727:    learn: 0.0001343    test: 0.0027412 best: 0.0024277 (249)   total: 9.53s    remaining: 3.56s\n728:    learn: 0.0001339    test: 0.0027422 best: 0.0024277 (249)   total: 9.54s    remaining: 3.55s\n729:    learn: 0.0001338    test: 0.0027417 best: 0.0024277 (249)   total: 9.56s    remaining: 3.53s\n730:    learn: 0.0001336    test: 0.0027428 best: 0.0024277 (249)   total: 9.57s    remaining: 3.52s\n731:    learn: 0.0001333    test: 0.0027452 best: 0.0024277 (249)   total: 9.58s    remaining: 3.51s\n732:    learn: 0.0001326    test: 0.0027508 best: 0.0024277 (249)   total: 9.59s    remaining: 3.49s\n733:    learn: 0.0001326    test: 0.0027506 best: 0.0024277 (249)   total: 9.6s remaining: 3.48s\n734:    learn: 0.0001326    test: 0.0027509 best: 0.0024277 (249)   total: 9.62s    remaining: 3.47s\n735:    learn: 0.0001326    test: 0.0027509 best: 0.0024277 (249)   total: 9.63s    remaining: 3.45s\n736:    learn: 0.0001325    test: 0.0027537 best: 0.0024277 (249)   total: 9.64s    remaining: 3.44s\n737:    learn: 0.0001322    test: 0.0027539 best: 0.0024277 (249)   total: 9.65s    remaining: 3.43s\n738:    learn: 0.0001322    test: 0.0027541 best: 0.0024277 (249)   total: 9.66s    remaining: 3.41s\n739:    learn: 0.0001321    test: 0.0027545 best: 0.0024277 (249)   total: 9.68s    remaining: 3.4s\n740:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.69s    remaining: 3.39s\n741:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.7s remaining: 3.37s\n742:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.71s    remaining: 3.36s\n743:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.72s    remaining: 3.34s\n744:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.73s    remaining: 3.33s\n745:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.74s    remaining: 3.32s\n746:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.75s    remaining: 3.3s\n747:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.77s    remaining: 3.29s\n748:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.78s    remaining: 3.27s\n749:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.79s    remaining: 3.26s\n750:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.8s remaining: 3.25s\n751:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.81s    remaining: 3.23s\n752:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.82s    remaining: 3.22s\n753:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.83s    remaining: 3.21s\n754:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.84s    remaining: 3.19s\n755:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.85s    remaining: 3.18s\n756:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.86s    remaining: 3.16s\n757:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.87s    remaining: 3.15s\n758:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.88s    remaining: 3.14s\n759:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.89s    remaining: 3.12s\n760:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.9s remaining: 3.11s\n761:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.91s    remaining: 3.1s\n762:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.92s    remaining: 3.08s\n763:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.93s    remaining: 3.07s\n764:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.94s    remaining: 3.05s\n765:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.95s    remaining: 3.04s\n766:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.96s    remaining: 3.03s\n767:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.97s    remaining: 3.01s\n768:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 9.99s    remaining: 3s\n769:    learn: 0.0001317    test: 0.0027575 best: 0.0024277 (249)   total: 10s  remaining: 2.98s\n770:    learn: 0.0001316    test: 0.0027551 best: 0.0024277 (249)   total: 10s  remaining: 2.97s\n771:    learn: 0.0001310    test: 0.0027546 best: 0.0024277 (249)   total: 10s  remaining: 2.96s\n772:    learn: 0.0001310    test: 0.0027546 best: 0.0024277 (249)   total: 10s  remaining: 2.94s\n773:    learn: 0.0001310    test: 0.0027542 best: 0.0024277 (249)   total: 10s  remaining: 2.93s\n774:    learn: 0.0001307    test: 0.0027544 best: 0.0024277 (249)   total: 10.1s    remaining: 2.92s\n775:    learn: 0.0001302    test: 0.0027526 best: 0.0024277 (249)   total: 10.1s    remaining: 2.91s\n776:    learn: 0.0001302    test: 0.0027522 best: 0.0024277 (249)   total: 10.1s    remaining: 2.9s\n777:    learn: 0.0001297    test: 0.0027508 best: 0.0024277 (249)   total: 10.1s    remaining: 2.88s\n778:    learn: 0.0001295    test: 0.0027509 best: 0.0024277 (249)   total: 10.1s    remaining: 2.87s\n779:    learn: 0.0001294    test: 0.0027516 best: 0.0024277 (249)   total: 10.1s    remaining: 2.86s\n780:    learn: 0.0001294    test: 0.0027516 best: 0.0024277 (249)   total: 10.1s    remaining: 2.84s\n781:    learn: 0.0001294    test: 0.0027516 best: 0.0024277 (249)   total: 10.2s    remaining: 2.83s\n782:    learn: 0.0001294    test: 0.0027516 best: 0.0024277 (249)   total: 10.2s    remaining: 2.82s\n783:    learn: 0.0001294    test: 0.0027516 best: 0.0024277 (249)   total: 10.2s    remaining: 2.8s\n784:    learn: 0.0001294    test: 0.0027525 best: 0.0024277 (249)   total: 10.2s    remaining: 2.79s\n785:    learn: 0.0001291    test: 0.0027527 best: 0.0024277 (249)   total: 10.2s    remaining: 2.78s\n786:    learn: 0.0001289    test: 0.0027532 best: 0.0024277 (249)   total: 10.2s    remaining: 2.77s\n787:    learn: 0.0001286    test: 0.0027546 best: 0.0024277 (249)   total: 10.2s    remaining: 2.75s\n788:    learn: 0.0001280    test: 0.0027578 best: 0.0024277 (249)   total: 10.2s    remaining: 2.74s\n789:    learn: 0.0001273    test: 0.0027571 best: 0.0024277 (249)   total: 10.3s    remaining: 2.73s\n790:    learn: 0.0001272    test: 0.0027593 best: 0.0024277 (249)   total: 10.3s    remaining: 2.71s\n791:    learn: 0.0001270    test: 0.0027596 best: 0.0024277 (249)   total: 10.3s    remaining: 2.7s\n792:    learn: 0.0001267    test: 0.0027594 best: 0.0024277 (249)   total: 10.3s    remaining: 2.69s\n793:    learn: 0.0001260    test: 0.0027601 best: 0.0024277 (249)   total: 10.3s    remaining: 2.67s\n794:    learn: 0.0001260    test: 0.0027603 best: 0.0024277 (249)   total: 10.3s    remaining: 2.66s\n795:    learn: 0.0001257    test: 0.0027607 best: 0.0024277 (249)   total: 10.3s    remaining: 2.65s\n796:    learn: 0.0001257    test: 0.0027613 best: 0.0024277 (249)   total: 10.3s    remaining: 2.63s\n797:    learn: 0.0001251    test: 0.0027630 best: 0.0024277 (249)   total: 10.4s    remaining: 2.62s\n798:    learn: 0.0001244    test: 0.0027612 best: 0.0024277 (249)   total: 10.4s    remaining: 2.61s\n799:    learn: 0.0001242    test: 0.0027634 best: 0.0024277 (249)   total: 10.4s    remaining: 2.6s\n800:    learn: 0.0001239    test: 0.0027627 best: 0.0024277 (249)   total: 10.4s    remaining: 2.58s\n801:    learn: 0.0001234    test: 0.0027635 best: 0.0024277 (249)   total: 10.4s    remaining: 2.57s\n802:    learn: 0.0001231    test: 0.0027634 best: 0.0024277 (249)   total: 10.4s    remaining: 2.56s\n803:    learn: 0.0001231    test: 0.0027640 best: 0.0024277 (249)   total: 10.4s    remaining: 2.54s\n804:    learn: 0.0001230    test: 0.0027639 best: 0.0024277 (249)   total: 10.5s    remaining: 2.53s\n805:    learn: 0.0001229    test: 0.0027635 best: 0.0024277 (249)   total: 10.5s    remaining: 2.52s\n806:    learn: 0.0001224    test: 0.0027649 best: 0.0024277 (249)   total: 10.5s    remaining: 2.51s\n807:    learn: 0.0001215    test: 0.0027665 best: 0.0024277 (249)   total: 10.5s    remaining: 2.49s\n808:    learn: 0.0001213    test: 0.0027655 best: 0.0024277 (249)   total: 10.5s    remaining: 2.48s\n809:    learn: 0.0001209    test: 0.0027650 best: 0.0024277 (249)   total: 10.5s    remaining: 2.47s\n810:    learn: 0.0001203    test: 0.0027668 best: 0.0024277 (249)   total: 10.5s    remaining: 2.45s\n811:    learn: 0.0001199    test: 0.0027676 best: 0.0024277 (249)   total: 10.5s    remaining: 2.44s\n812:    learn: 0.0001197    test: 0.0027705 best: 0.0024277 (249)   total: 10.6s    remaining: 2.43s\n813:    learn: 0.0001195    test: 0.0027718 best: 0.0024277 (249)   total: 10.6s    remaining: 2.42s\n814:    learn: 0.0001193    test: 0.0027738 best: 0.0024277 (249)   total: 10.6s    remaining: 2.4s\n815:    learn: 0.0001192    test: 0.0027734 best: 0.0024277 (249)   total: 10.6s    remaining: 2.39s\n816:    learn: 0.0001191    test: 0.0027742 best: 0.0024277 (249)   total: 10.6s    remaining: 2.38s\n817:    learn: 0.0001185    test: 0.0027753 best: 0.0024277 (249)   total: 10.6s    remaining: 2.36s\n818:    learn: 0.0001184    test: 0.0027759 best: 0.0024277 (249)   total: 10.6s    remaining: 2.35s\n819:    learn: 0.0001182    test: 0.0027756 best: 0.0024277 (249)   total: 10.6s    remaining: 2.34s\n820:    learn: 0.0001180    test: 0.0027760 best: 0.0024277 (249)   total: 10.7s    remaining: 2.32s\n821:    learn: 0.0001177    test: 0.0027766 best: 0.0024277 (249)   total: 10.7s    remaining: 2.31s\n822:    learn: 0.0001175    test: 0.0027764 best: 0.0024277 (249)   total: 10.7s    remaining: 2.3s\n823:    learn: 0.0001171    test: 0.0027766 best: 0.0024277 (249)   total: 10.7s    remaining: 2.29s\n824:    learn: 0.0001169    test: 0.0027753 best: 0.0024277 (249)   total: 10.7s    remaining: 2.27s\n825:    learn: 0.0001166    test: 0.0027768 best: 0.0024277 (249)   total: 10.7s    remaining: 2.26s\n826:    learn: 0.0001164    test: 0.0027773 best: 0.0024277 (249)   total: 10.7s    remaining: 2.25s\n827:    learn: 0.0001160    test: 0.0027820 best: 0.0024277 (249)   total: 10.8s    remaining: 2.23s\n828:    learn: 0.0001157    test: 0.0027817 best: 0.0024277 (249)   total: 10.8s    remaining: 2.22s\n829:    learn: 0.0001155    test: 0.0027829 best: 0.0024277 (249)   total: 10.8s    remaining: 2.21s\n830:    learn: 0.0001153    test: 0.0027842 best: 0.0024277 (249)   total: 10.8s    remaining: 2.19s\n831:    learn: 0.0001152    test: 0.0027842 best: 0.0024277 (249)   total: 10.8s    remaining: 2.18s\n832:    learn: 0.0001147    test: 0.0027865 best: 0.0024277 (249)   total: 10.8s    remaining: 2.17s\n833:    learn: 0.0001145    test: 0.0027851 best: 0.0024277 (249)   total: 10.8s    remaining: 2.15s\n834:    learn: 0.0001144    test: 0.0027857 best: 0.0024277 (249)   total: 10.8s    remaining: 2.14s\n835:    learn: 0.0001141    test: 0.0027872 best: 0.0024277 (249)   total: 10.9s    remaining: 2.13s\n836:    learn: 0.0001140    test: 0.0027865 best: 0.0024277 (249)   total: 10.9s    remaining: 2.12s\n837:    learn: 0.0001139    test: 0.0027871 best: 0.0024277 (249)   total: 10.9s    remaining: 2.1s\n838:    learn: 0.0001137    test: 0.0027869 best: 0.0024277 (249)   total: 10.9s    remaining: 2.09s\n839:    learn: 0.0001132    test: 0.0027911 best: 0.0024277 (249)   total: 10.9s    remaining: 2.08s\n840:    learn: 0.0001129    test: 0.0027934 best: 0.0024277 (249)   total: 10.9s    remaining: 2.07s\n841:    learn: 0.0001128    test: 0.0027919 best: 0.0024277 (249)   total: 10.9s    remaining: 2.05s\n842:    learn: 0.0001127    test: 0.0027923 best: 0.0024277 (249)   total: 11s  remaining: 2.04s\n843:    learn: 0.0001127    test: 0.0027924 best: 0.0024277 (249)   total: 11s  remaining: 2.03s\n844:    learn: 0.0001124    test: 0.0027950 best: 0.0024277 (249)   total: 11s  remaining: 2.01s\n845:    learn: 0.0001124    test: 0.0027949 best: 0.0024277 (249)   total: 11s  remaining: 2s\n846:    learn: 0.0001119    test: 0.0027940 best: 0.0024277 (249)   total: 11s  remaining: 1.99s\n847:    learn: 0.0001117    test: 0.0027938 best: 0.0024277 (249)   total: 11s  remaining: 1.98s\n848:    learn: 0.0001115    test: 0.0027934 best: 0.0024277 (249)   total: 11s  remaining: 1.96s\n849:    learn: 0.0001114    test: 0.0027939 best: 0.0024277 (249)   total: 11s  remaining: 1.95s\n850:    learn: 0.0001114    test: 0.0027941 best: 0.0024277 (249)   total: 11.1s    remaining: 1.94s\n851:    learn: 0.0001113    test: 0.0027953 best: 0.0024277 (249)   total: 11.1s    remaining: 1.92s\n852:    learn: 0.0001105    test: 0.0027949 best: 0.0024277 (249)   total: 11.1s    remaining: 1.91s\n853:    learn: 0.0001103    test: 0.0027960 best: 0.0024277 (249)   total: 11.1s    remaining: 1.9s\n854:    learn: 0.0001102    test: 0.0027973 best: 0.0024277 (249)   total: 11.1s    remaining: 1.88s\n855:    learn: 0.0001100    test: 0.0027970 best: 0.0024277 (249)   total: 11.1s    remaining: 1.87s\n856:    learn: 0.0001097    test: 0.0027972 best: 0.0024277 (249)   total: 11.1s    remaining: 1.86s\n857:    learn: 0.0001091    test: 0.0027991 best: 0.0024277 (249)   total: 11.1s    remaining: 1.84s\n858:    learn: 0.0001089    test: 0.0028005 best: 0.0024277 (249)   total: 11.2s    remaining: 1.83s\n859:    learn: 0.0001084    test: 0.0028023 best: 0.0024277 (249)   total: 11.2s    remaining: 1.82s\n860:    learn: 0.0001083    test: 0.0028043 best: 0.0024277 (249)   total: 11.2s    remaining: 1.81s\n861:    learn: 0.0001081    test: 0.0028038 best: 0.0024277 (249)   total: 11.2s    remaining: 1.79s\n862:    learn: 0.0001077    test: 0.0028009 best: 0.0024277 (249)   total: 11.2s    remaining: 1.78s\n863:    learn: 0.0001075    test: 0.0028008 best: 0.0024277 (249)   total: 11.2s    remaining: 1.77s\n864:    learn: 0.0001074    test: 0.0027999 best: 0.0024277 (249)   total: 11.2s    remaining: 1.75s\n865:    learn: 0.0001073    test: 0.0027996 best: 0.0024277 (249)   total: 11.3s    remaining: 1.74s\n866:    learn: 0.0001072    test: 0.0027995 best: 0.0024277 (249)   total: 11.3s    remaining: 1.73s\n867:    learn: 0.0001071    test: 0.0027991 best: 0.0024277 (249)   total: 11.3s    remaining: 1.72s\n868:    learn: 0.0001071    test: 0.0027994 best: 0.0024277 (249)   total: 11.3s    remaining: 1.7s\n869:    learn: 0.0001069    test: 0.0028007 best: 0.0024277 (249)   total: 11.3s    remaining: 1.69s\n870:    learn: 0.0001068    test: 0.0028007 best: 0.0024277 (249)   total: 11.3s    remaining: 1.68s\n871:    learn: 0.0001066    test: 0.0028031 best: 0.0024277 (249)   total: 11.3s    remaining: 1.66s\n872:    learn: 0.0001063    test: 0.0028034 best: 0.0024277 (249)   total: 11.4s    remaining: 1.65s\n873:    learn: 0.0001059    test: 0.0028022 best: 0.0024277 (249)   total: 11.4s    remaining: 1.64s\n874:    learn: 0.0001058    test: 0.0028020 best: 0.0024277 (249)   total: 11.4s    remaining: 1.63s\n875:    learn: 0.0001056    test: 0.0028041 best: 0.0024277 (249)   total: 11.4s    remaining: 1.62s\n876:    learn: 0.0001055    test: 0.0028041 best: 0.0024277 (249)   total: 11.4s    remaining: 1.6s\n877:    learn: 0.0001053    test: 0.0028036 best: 0.0024277 (249)   total: 11.4s    remaining: 1.59s\n878:    learn: 0.0001053    test: 0.0028045 best: 0.0024277 (249)   total: 11.5s    remaining: 1.58s\n879:    learn: 0.0001053    test: 0.0028053 best: 0.0024277 (249)   total: 11.5s    remaining: 1.56s\n880:    learn: 0.0001052    test: 0.0028055 best: 0.0024277 (249)   total: 11.5s    remaining: 1.55s\n881:    learn: 0.0001052    test: 0.0028057 best: 0.0024277 (249)   total: 11.5s    remaining: 1.54s\n882:    learn: 0.0001050    test: 0.0028053 best: 0.0024277 (249)   total: 11.5s    remaining: 1.52s\n883:    learn: 0.0001048    test: 0.0028069 best: 0.0024277 (249)   total: 11.5s    remaining: 1.51s\n884:    learn: 0.0001047    test: 0.0028072 best: 0.0024277 (249)   total: 11.6s    remaining: 1.5s\n885:    learn: 0.0001047    test: 0.0028069 best: 0.0024277 (249)   total: 11.6s    remaining: 1.49s\n886:    learn: 0.0001044    test: 0.0028073 best: 0.0024277 (249)   total: 11.6s    remaining: 1.48s\n887:    learn: 0.0001042    test: 0.0028077 best: 0.0024277 (249)   total: 11.6s    remaining: 1.47s\n888:    learn: 0.0001041    test: 0.0028084 best: 0.0024277 (249)   total: 11.6s    remaining: 1.45s\n889:    learn: 0.0001036    test: 0.0028121 best: 0.0024277 (249)   total: 11.6s    remaining: 1.44s\n890:    learn: 0.0001033    test: 0.0028128 best: 0.0024277 (249)   total: 11.7s    remaining: 1.43s\n891:    learn: 0.0001032    test: 0.0028130 best: 0.0024277 (249)   total: 11.7s    remaining: 1.41s\n892:    learn: 0.0001031    test: 0.0028132 best: 0.0024277 (249)   total: 11.7s    remaining: 1.4s\n893:    learn: 0.0001027    test: 0.0028146 best: 0.0024277 (249)   total: 11.7s    remaining: 1.39s\n894:    learn: 0.0001024    test: 0.0028155 best: 0.0024277 (249)   total: 11.7s    remaining: 1.37s\n895:    learn: 0.0001022    test: 0.0028154 best: 0.0024277 (249)   total: 11.7s    remaining: 1.36s\n896:    learn: 0.0001015    test: 0.0028193 best: 0.0024277 (249)   total: 11.7s    remaining: 1.35s\n897:    learn: 0.0001015    test: 0.0028194 best: 0.0024277 (249)   total: 11.8s    remaining: 1.33s\n898:    learn: 0.0001015    test: 0.0028195 best: 0.0024277 (249)   total: 11.8s    remaining: 1.32s\n899:    learn: 0.0001008    test: 0.0028199 best: 0.0024277 (249)   total: 11.8s    remaining: 1.31s\n900:    learn: 0.0001006    test: 0.0028230 best: 0.0024277 (249)   total: 11.8s    remaining: 1.29s\n901:    learn: 0.0001003    test: 0.0028228 best: 0.0024277 (249)   total: 11.8s    remaining: 1.28s\n902:    learn: 0.0001001    test: 0.0028238 best: 0.0024277 (249)   total: 11.8s    remaining: 1.27s\n903:    learn: 0.0001001    test: 0.0028237 best: 0.0024277 (249)   total: 11.8s    remaining: 1.26s\n904:    learn: 0.0000992    test: 0.0028270 best: 0.0024277 (249)   total: 11.8s    remaining: 1.24s\n905:    learn: 0.0000991    test: 0.0028288 best: 0.0024277 (249)   total: 11.9s    remaining: 1.23s\n906:    learn: 0.0000989    test: 0.0028292 best: 0.0024277 (249)   total: 11.9s    remaining: 1.22s\n907:    learn: 0.0000988    test: 0.0028289 best: 0.0024277 (249)   total: 11.9s    remaining: 1.2s\n908:    learn: 0.0000984    test: 0.0028304 best: 0.0024277 (249)   total: 11.9s    remaining: 1.19s\n909:    learn: 0.0000983    test: 0.0028309 best: 0.0024277 (249)   total: 11.9s    remaining: 1.18s\n910:    learn: 0.0000982    test: 0.0028306 best: 0.0024277 (249)   total: 11.9s    remaining: 1.16s\n911:    learn: 0.0000980    test: 0.0028311 best: 0.0024277 (249)   total: 11.9s    remaining: 1.15s\n912:    learn: 0.0000980    test: 0.0028310 best: 0.0024277 (249)   total: 11.9s    remaining: 1.14s\n913:    learn: 0.0000979    test: 0.0028312 best: 0.0024277 (249)   total: 12s  remaining: 1.12s\n914:    learn: 0.0000978    test: 0.0028312 best: 0.0024277 (249)   total: 12s  remaining: 1.11s\n915:    learn: 0.0000976    test: 0.0028311 best: 0.0024277 (249)   total: 12s  remaining: 1.1s\n916:    learn: 0.0000973    test: 0.0028316 best: 0.0024277 (249)   total: 12s  remaining: 1.08s\n917:    learn: 0.0000973    test: 0.0028314 best: 0.0024277 (249)   total: 12s  remaining: 1.07s\n918:    learn: 0.0000972    test: 0.0028316 best: 0.0024277 (249)   total: 12s  remaining: 1.06s\n919:    learn: 0.0000969    test: 0.0028351 best: 0.0024277 (249)   total: 12s  remaining: 1.05s\n920:    learn: 0.0000967    test: 0.0028360 best: 0.0024277 (249)   total: 12s  remaining: 1.03s\n921:    learn: 0.0000962    test: 0.0028377 best: 0.0024277 (249)   total: 12.1s    remaining: 1.02s\n922:    learn: 0.0000962    test: 0.0028377 best: 0.0024277 (249)   total: 12.1s    remaining: 1.01s\n923:    learn: 0.0000961    test: 0.0028386 best: 0.0024277 (249)   total: 12.1s    remaining: 994ms\n924:    learn: 0.0000961    test: 0.0028386 best: 0.0024277 (249)   total: 12.1s    remaining: 981ms\n925:    learn: 0.0000961    test: 0.0028386 best: 0.0024277 (249)   total: 12.1s    remaining: 968ms\n926:    learn: 0.0000961    test: 0.0028386 best: 0.0024277 (249)   total: 12.1s    remaining: 955ms\n927:    learn: 0.0000961    test: 0.0028387 best: 0.0024277 (249)   total: 12.1s    remaining: 942ms\n928:    learn: 0.0000960    test: 0.0028387 best: 0.0024277 (249)   total: 12.2s    remaining: 929ms\n929:    learn: 0.0000959    test: 0.0028387 best: 0.0024277 (249)   total: 12.2s    remaining: 916ms\n930:    learn: 0.0000959    test: 0.0028386 best: 0.0024277 (249)   total: 12.2s    remaining: 903ms\n931:    learn: 0.0000959    test: 0.0028391 best: 0.0024277 (249)   total: 12.2s    remaining: 890ms\n932:    learn: 0.0000953    test: 0.0028421 best: 0.0024277 (249)   total: 12.2s    remaining: 877ms\n933:    learn: 0.0000951    test: 0.0028422 best: 0.0024277 (249)   total: 12.2s    remaining: 864ms\n934:    learn: 0.0000950    test: 0.0028414 best: 0.0024277 (249)   total: 12.2s    remaining: 851ms\n935:    learn: 0.0000950    test: 0.0028412 best: 0.0024277 (249)   total: 12.3s    remaining: 838ms\n936:    learn: 0.0000946    test: 0.0028411 best: 0.0024277 (249)   total: 12.3s    remaining: 825ms\n937:    learn: 0.0000945    test: 0.0028403 best: 0.0024277 (249)   total: 12.3s    remaining: 811ms\n938:    learn: 0.0000942    test: 0.0028403 best: 0.0024277 (249)   total: 12.3s    remaining: 798ms\n939:    learn: 0.0000940    test: 0.0028399 best: 0.0024277 (249)   total: 12.3s    remaining: 785ms\n940:    learn: 0.0000940    test: 0.0028401 best: 0.0024277 (249)   total: 12.3s    remaining: 772ms\n941:    learn: 0.0000939    test: 0.0028399 best: 0.0024277 (249)   total: 12.3s    remaining: 759ms\n942:    learn: 0.0000938    test: 0.0028398 best: 0.0024277 (249)   total: 12.3s    remaining: 746ms\n943:    learn: 0.0000937    test: 0.0028402 best: 0.0024277 (249)   total: 12.4s    remaining: 733ms\n944:    learn: 0.0000935    test: 0.0028394 best: 0.0024277 (249)   total: 12.4s    remaining: 720ms\n945:    learn: 0.0000932    test: 0.0028383 best: 0.0024277 (249)   total: 12.4s    remaining: 707ms\n946:    learn: 0.0000930    test: 0.0028377 best: 0.0024277 (249)   total: 12.4s    remaining: 693ms\n947:    learn: 0.0000929    test: 0.0028376 best: 0.0024277 (249)   total: 12.4s    remaining: 680ms\n948:    learn: 0.0000927    test: 0.0028407 best: 0.0024277 (249)   total: 12.4s    remaining: 667ms\n949:    learn: 0.0000927    test: 0.0028409 best: 0.0024277 (249)   total: 12.4s    remaining: 654ms\n950:    learn: 0.0000925    test: 0.0028412 best: 0.0024277 (249)   total: 12.4s    remaining: 641ms\n951:    learn: 0.0000924    test: 0.0028416 best: 0.0024277 (249)   total: 12.5s    remaining: 628ms\n952:    learn: 0.0000923    test: 0.0028415 best: 0.0024277 (249)   total: 12.5s    remaining: 615ms\n953:    learn: 0.0000922    test: 0.0028415 best: 0.0024277 (249)   total: 12.5s    remaining: 602ms\n954:    learn: 0.0000918    test: 0.0028432 best: 0.0024277 (249)   total: 12.5s    remaining: 589ms\n955:    learn: 0.0000917    test: 0.0028429 best: 0.0024277 (249)   total: 12.5s    remaining: 576ms\n956:    learn: 0.0000917    test: 0.0028429 best: 0.0024277 (249)   total: 12.5s    remaining: 563ms\n957:    learn: 0.0000917    test: 0.0028429 best: 0.0024277 (249)   total: 12.5s    remaining: 549ms\n958:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.5s    remaining: 536ms\n959:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.6s    remaining: 523ms\n960:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.6s    remaining: 510ms\n961:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.6s    remaining: 497ms\n962:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.6s    remaining: 483ms\n963:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.6s    remaining: 470ms\n964:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.6s    remaining: 457ms\n965:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.6s    remaining: 444ms\n966:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.6s    remaining: 431ms\n967:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.6s    remaining: 418ms\n968:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.6s    remaining: 405ms\n969:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.7s    remaining: 391ms\n970:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.7s    remaining: 378ms\n971:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.7s    remaining: 365ms\n972:    learn: 0.0000917    test: 0.0028430 best: 0.0024277 (249)   total: 12.7s    remaining: 352ms\n973:    learn: 0.0000916    test: 0.0028441 best: 0.0024277 (249)   total: 12.7s    remaining: 339ms\n974:    learn: 0.0000916    test: 0.0028441 best: 0.0024277 (249)   total: 12.7s    remaining: 326ms\n975:    learn: 0.0000916    test: 0.0028441 best: 0.0024277 (249)   total: 12.7s    remaining: 313ms\n976:    learn: 0.0000914    test: 0.0028446 best: 0.0024277 (249)   total: 12.7s    remaining: 300ms\n977:    learn: 0.0000914    test: 0.0028444 best: 0.0024277 (249)   total: 12.7s    remaining: 287ms\n978:    learn: 0.0000909    test: 0.0028477 best: 0.0024277 (249)   total: 12.8s    remaining: 274ms\n979:    learn: 0.0000908    test: 0.0028491 best: 0.0024277 (249)   total: 12.8s    remaining: 261ms\n980:    learn: 0.0000908    test: 0.0028491 best: 0.0024277 (249)   total: 12.8s    remaining: 248ms\n981:    learn: 0.0000906    test: 0.0028463 best: 0.0024277 (249)   total: 12.8s    remaining: 235ms\n982:    learn: 0.0000906    test: 0.0028463 best: 0.0024277 (249)   total: 12.8s    remaining: 221ms\n983:    learn: 0.0000904    test: 0.0028459 best: 0.0024277 (249)   total: 12.8s    remaining: 208ms\n984:    learn: 0.0000895    test: 0.0028454 best: 0.0024277 (249)   total: 12.8s    remaining: 195ms\n985:    learn: 0.0000895    test: 0.0028459 best: 0.0024277 (249)   total: 12.8s    remaining: 182ms\n986:    learn: 0.0000892    test: 0.0028465 best: 0.0024277 (249)   total: 12.9s    remaining: 169ms\n987:    learn: 0.0000889    test: 0.0028481 best: 0.0024277 (249)   total: 12.9s    remaining: 156ms\n988:    learn: 0.0000887    test: 0.0028489 best: 0.0024277 (249)   total: 12.9s    remaining: 143ms\n989:    learn: 0.0000887    test: 0.0028492 best: 0.0024277 (249)   total: 12.9s    remaining: 130ms\n990:    learn: 0.0000886    test: 0.0028486 best: 0.0024277 (249)   total: 12.9s    remaining: 117ms\n991:    learn: 0.0000886    test: 0.0028486 best: 0.0024277 (249)   total: 12.9s    remaining: 104ms\n992:    learn: 0.0000886    test: 0.0028492 best: 0.0024277 (249)   total: 12.9s    remaining: 91.2ms\n993:    learn: 0.0000886    test: 0.0028490 best: 0.0024277 (249)   total: 12.9s    remaining: 78.1ms\n994:    learn: 0.0000885    test: 0.0028494 best: 0.0024277 (249)   total: 13s  remaining: 65.1ms\n995:    learn: 0.0000885    test: 0.0028494 best: 0.0024277 (249)   total: 13s  remaining: 52.1ms\n996:    learn: 0.0000882    test: 0.0028504 best: 0.0024277 (249)   total: 13s  remaining: 39.1ms\n997:    learn: 0.0000882    test: 0.0028517 best: 0.0024277 (249)   total: 13s  remaining: 26ms\n998:    learn: 0.0000881    test: 0.0028519 best: 0.0024277 (249)   total: 13s  remaining: 13ms\n999:    learn: 0.0000881    test: 0.0028519 best: 0.0024277 (249)   total: 13s  remaining: 0us\n\nbestTest = 0.00242772666\nbestIteration = 249\n\nShrink model to first 250 iterations.\n\n\n&lt;catboost.core.CatBoostClassifier at 0x1760009a0&gt;\n\n\n\nTree parameters\n\nDepth\nmin_data_in_leaf\ngrow_policy\n\nSampling parameters\n\nSubsample\ncolsample_bylevel\nsampling_frquency\n\nRegularization parameters\n\npenalties_coefficient\nfirst_feature_use_penalties\nleaf_estimation_backtracking\n\nLearning rate\n\nIterations\nlearning_rate\nmodel_shrink_rate\nboost_from_average\n\n\n\nwhat are hyperparams, and why tune\ngridsearch, randomsearch, lhs search -&gt; pro cons of each\noptuna in one go\noptuna in steps\ncompare time should be less, for same or better performance"
  },
  {
    "objectID": "whois.html",
    "href": "whois.html",
    "title": "whois 🤖",
    "section": "",
    "text": "Machine Learning Scientist 🤖\nloves to puzzle 🧩\nAmsterdam area 🇳🇱🇪🇺"
  },
  {
    "objectID": "whois.html#whois",
    "href": "whois.html#whois",
    "title": "whois 🤖",
    "section": "",
    "text": "Machine Learning Scientist 🤖\nloves to puzzle 🧩\nAmsterdam area 🇳🇱🇪🇺"
  }
]