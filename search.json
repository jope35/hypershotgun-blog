[
  {
    "objectID": "whois.html",
    "href": "whois.html",
    "title": "whois ü§ñ",
    "section": "",
    "text": "Machine Learning Scientist ü§ñ\nloves to puzzle üß©\nAmsterdam area üá≥üá±üá™üá∫"
  },
  {
    "objectID": "whois.html#whois",
    "href": "whois.html#whois",
    "title": "whois ü§ñ",
    "section": "",
    "text": "Machine Learning Scientist ü§ñ\nloves to puzzle üß©\nAmsterdam area üá≥üá±üá™üá∫"
  },
  {
    "objectID": "posts/gitgud/gitconfig.html",
    "href": "posts/gitgud/gitconfig.html",
    "title": "Multi-Git-Account-Mastery",
    "section": "",
    "text": "Git, everyone‚Äôs favorite version control system, was released (checks notes) 19 years ago on April 7th, 2005. That is some time ago.\nCurrently, every developer uses Git for one or more projects and/or organizations. In modern days, each and every repo/organization has some form of authentication or other requirements to work with it. I have seen some pretty wacky setups to control how Git is being configured in order to streamline developer productivity and security.\n And now, I will introduce my wacky solution üòú for handeling ssh keys for multiple github accounts.\n\n\nThe structure I use has the following files:\n\n.gitconfig\n.gitconfig_github_personal\n~/.ssh/config\n\n\n\n[user]\nuseConfigOnly = true\n\n[includeIf \"gitdir:~/Developer/github_personal/\"]\npath = ~/.gitconfig_github_personal\n\n\n\n[user]\nname = personal_name\nemail = personal_email@example.com\n\n\n\nHost github.com\n  HostName github.com\n  User personal_name\n  IdentityFile ~/.ssh/personal_key\nSo far, not too whacky, right? Git uses the .gitconfig file to configure Git globally, and then only the actions performed in the folder ~/Developer/github_personal/ will use the .gitconfig_github_personal file. The SSH key is defined in the SSH config and is used solely for GitHub connections.\nNow, if you make a commit, the name and email from the .gitconfig_github_personal are attached to that commit. This is all fun and games, but what if another project requires a Bitbucket or GitLab account? You can create a .gitconfig file for each service and add the right key settings to the SSH file.\nThe ‚Äúfun‚Äù starts when there‚Äôs a second instance of a GitHub account; now, all of a sudden, you have a collision in the way you handle your SSH keys - they have the same hostname. üò©\n\n\n\n\nlets add a additional gitconfig for the work github\n\n.gitconfig\n.gitconfig_github_personal\n.gitconfig_github_work\n.ssh/config\n\n\n\n[user]\n    useConfigOnly = true\n\n[includeIf \"gitdir:~/Developer/github_personal/\"]\n    path = ~/.gitconfig_github_personal\n\n[includeIf \"gitdir:~/Developer/github_work/\"]\n    path = ~/.gitconfig_github_work\nadding one line for the new work directory, this is linked to the new work gitconfig file.\n\n\n\n[user]\n    name=work_name\n    email= work@example.com\n\n[url \"git@github.com-work\"]\n    insteadOf = git@github.com\nthis is where the magic happens, we instruct git to modify the URL it uses to connect to GitHub, and in the SSH config file, we remap this to the correct hostname. The result is that we can clone repositories as normal, and the folder location determines which SSH keys are used. This means that once set up, we can get on with our work instead of fiddling around with SSH keys. Automation wins!\n\n\n\nHost github.com\n  Hostname github.com\n  User personal_name\n  IdentityFile ~/.ssh/personal_key\n\nHost github.com-work\n  Hostname github.com\n  User work_name\n  IdentityFile ~/.ssh/work_key\n\n\n\n\nIf you want to be able to sign commits (and get a green checkmark next to your commits) you can extend the above structure to utilize the SSH key for Git commit signing.\n\n\n[user]\n  useConfigOnly = true\n\n[gpg]\n  format = ssh\n\n[includeIf \"gitdir:~/Developer/github_personal/\"]\n  path = ~/.gitconfig_github_personal\n\n[includeIf \"gitdir:~/Developer/github_work/\"]\n  path = ~/.gitconfig_github_work\nWe start with the global Git config and define the GPG format to be SSH.\n\n\n\n[user]\n  name = work_name\n  email = work@example.com\n  signingkey = &lt;your ssh key&gt;\n\n[commit]\n  gpgsign = true\n\n[url \"git@github.com/work\"]\n  insteadOf = git@github.com\nHere, I have added the signingkey option for the user and set the gpgsign indicator to true.\nNow also add your ssh key as a signing key!\nThe next time you commit, it is signed with your SSH key 100% automatically and pain-free ‚Äì how cool is that!!\n\n\n\nSigned Commit\n\n\nP.S. You can also attend a GPG signing party but that‚Äôs a topic for another time."
  },
  {
    "objectID": "posts/gitgud/gitconfig.html#basic-structure",
    "href": "posts/gitgud/gitconfig.html#basic-structure",
    "title": "Multi-Git-Account-Mastery",
    "section": "",
    "text": "The structure I use has the following files:\n\n.gitconfig\n.gitconfig_github_personal\n~/.ssh/config\n\n\n\n[user]\nuseConfigOnly = true\n\n[includeIf \"gitdir:~/Developer/github_personal/\"]\npath = ~/.gitconfig_github_personal\n\n\n\n[user]\nname = personal_name\nemail = personal_email@example.com\n\n\n\nHost github.com\n  HostName github.com\n  User personal_name\n  IdentityFile ~/.ssh/personal_key\nSo far, not too whacky, right? Git uses the .gitconfig file to configure Git globally, and then only the actions performed in the folder ~/Developer/github_personal/ will use the .gitconfig_github_personal file. The SSH key is defined in the SSH config and is used solely for GitHub connections.\nNow, if you make a commit, the name and email from the .gitconfig_github_personal are attached to that commit. This is all fun and games, but what if another project requires a Bitbucket or GitLab account? You can create a .gitconfig file for each service and add the right key settings to the SSH file.\nThe ‚Äúfun‚Äù starts when there‚Äôs a second instance of a GitHub account; now, all of a sudden, you have a collision in the way you handle your SSH keys - they have the same hostname. üò©"
  },
  {
    "objectID": "posts/gitgud/gitconfig.html#extending-the-structure",
    "href": "posts/gitgud/gitconfig.html#extending-the-structure",
    "title": "Multi-Git-Account-Mastery",
    "section": "",
    "text": "lets add a additional gitconfig for the work github\n\n.gitconfig\n.gitconfig_github_personal\n.gitconfig_github_work\n.ssh/config\n\n\n\n[user]\n    useConfigOnly = true\n\n[includeIf \"gitdir:~/Developer/github_personal/\"]\n    path = ~/.gitconfig_github_personal\n\n[includeIf \"gitdir:~/Developer/github_work/\"]\n    path = ~/.gitconfig_github_work\nadding one line for the new work directory, this is linked to the new work gitconfig file.\n\n\n\n[user]\n    name=work_name\n    email= work@example.com\n\n[url \"git@github.com-work\"]\n    insteadOf = git@github.com\nthis is where the magic happens, we instruct git to modify the URL it uses to connect to GitHub, and in the SSH config file, we remap this to the correct hostname. The result is that we can clone repositories as normal, and the folder location determines which SSH keys are used. This means that once set up, we can get on with our work instead of fiddling around with SSH keys. Automation wins!\n\n\n\nHost github.com\n  Hostname github.com\n  User personal_name\n  IdentityFile ~/.ssh/personal_key\n\nHost github.com-work\n  Hostname github.com\n  User work_name\n  IdentityFile ~/.ssh/work_key"
  },
  {
    "objectID": "posts/gitgud/gitconfig.html#bonus-gpg-signing",
    "href": "posts/gitgud/gitconfig.html#bonus-gpg-signing",
    "title": "Multi-Git-Account-Mastery",
    "section": "",
    "text": "If you want to be able to sign commits (and get a green checkmark next to your commits) you can extend the above structure to utilize the SSH key for Git commit signing.\n\n\n[user]\n  useConfigOnly = true\n\n[gpg]\n  format = ssh\n\n[includeIf \"gitdir:~/Developer/github_personal/\"]\n  path = ~/.gitconfig_github_personal\n\n[includeIf \"gitdir:~/Developer/github_work/\"]\n  path = ~/.gitconfig_github_work\nWe start with the global Git config and define the GPG format to be SSH.\n\n\n\n[user]\n  name = work_name\n  email = work@example.com\n  signingkey = &lt;your ssh key&gt;\n\n[commit]\n  gpgsign = true\n\n[url \"git@github.com/work\"]\n  insteadOf = git@github.com\nHere, I have added the signingkey option for the user and set the gpgsign indicator to true.\nNow also add your ssh key as a signing key!\nThe next time you commit, it is signed with your SSH key 100% automatically and pain-free ‚Äì how cool is that!!\n\n\n\nSigned Commit\n\n\nP.S. You can also attend a GPG signing party but that‚Äôs a topic for another time."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html",
    "href": "posts/boring_forecast/boring_linear_forecast.html",
    "title": "Boring linear forecast",
    "section": "",
    "text": "Linear regression in itself is not performant for longer time-scales\nMost systems have some capabilities for linear regression built-in\nAdding dummy variables for datetime features(i.e.¬†month, weekday etc.) adds predictive power"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#tldr",
    "href": "posts/boring_forecast/boring_linear_forecast.html#tldr",
    "title": "Boring linear forecast",
    "section": "",
    "text": "Linear regression in itself is not performant for longer time-scales\nMost systems have some capabilities for linear regression built-in\nAdding dummy variables for datetime features(i.e.¬†month, weekday etc.) adds predictive power"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#introduction",
    "href": "posts/boring_forecast/boring_linear_forecast.html#introduction",
    "title": "Boring linear forecast",
    "section": "2 Introduction",
    "text": "2 Introduction\nLinear regression is a fundamental statistical model for determining linear relationships between variables, commonly used in Economics, Finance, and Social Sciences. Despite being considered basic, its ability to estimate direction and magnitude makes it valuable. The popularity of linear regression arises from its widespread availability, affordability, and ease of identifying optimal parameters.\nFor time series forecasting, dummy variables can be employed to expand the model‚Äôs capabilities. These dummies offer insights into temporal relationships by capturing seasonality and identifying one-off events such as price reductions or natural disasters. For instance, we can create dummies for specific datetime features like month or weekday/weekend status."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#imports",
    "href": "posts/boring_forecast/boring_linear_forecast.html#imports",
    "title": "Boring linear forecast",
    "section": "3 Imports",
    "text": "3 Imports\nFirst we import all the libraries, the default data science libs and the linear model and metrics from sklearn.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#reading-in-the-data",
    "href": "posts/boring_forecast/boring_linear_forecast.html#reading-in-the-data",
    "title": "Boring linear forecast",
    "section": "4 Reading in the data",
    "text": "4 Reading in the data\nFor this example, we‚Äôll utilize the dataset from the Prophet package. The dataset is described in the Prophet docs as follows:\n\nAs an illustration, let‚Äôs examine a time series of the log daily page views for the Wikipedia page about Peyton Manning. We obtained this data using the Wikipedia trend package in R. Peyton Manning offers an excellent example because it demonstrates some of Prophet‚Äôs features such as multiple seasonality, changing growth rates, and the ability to model special days (like his playoff and Superbowl appearances).\n\nLink to the documentation\n\n\nCode\ndf_in = pd.read_csv(\n    \"https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv\"\n)\ndf_in = df_in.assign(ds=pd.to_datetime(df_in[\"ds\"]))\ndf_in = df_in[(df_in[\"ds\"] &gt; \"2012\")]  # selecting data after 2012\n\n\nTo gain a better understanding of our dataset, let‚Äôs visualize it over time and identify any observable trends or patterns. This visualization provides insights into the patterns and trends in the data, which we can then try to incorporate into our model using seasonal dummies.\n\n\nCode\nplt.plot_date(\n    x=df_in[\"ds\"],\n    y=df_in[\"y\"],\n    label=\"input timeseries\",\n    fmt=\"-\",\n)\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"target variable - $y$\")\nplt.title(\"daily visits ot Peyton Manning wiki on a daily basis (log)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nI‚Äôve selected data spanning from 2012 for analyzing seasonal dummies. The initial observation is that there exists a dip around months 6 and 7. Early August hosts NFL exhibition games, preceding the start of the regular football season.\nAlso, we can observe a pattern over the year, it starts high then dips, and then and high again. This can be seen for the other years as well, so there is some repeating seasonality. Let us continue and train our first models. Starting with a simple ordinary linear regression and then adding dummies to see if they improve the performance of the model."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#train-test-split",
    "href": "posts/boring_forecast/boring_linear_forecast.html#train-test-split",
    "title": "Boring linear forecast",
    "section": "5 Train-test split",
    "text": "5 Train-test split\n\n\nCode\n# train test split\ndf_train = df_in[(df_in[\"ds\"] &gt; \"2012\") & (df_in[\"ds\"] &lt; \"2015\")]\ndf_test = df_in[(df_in[\"ds\"] &gt; \"2015\")]\n\n\nTo evaluate the performance of the model, let‚Äôs split the dataset into two parts: training data (from 2012 to 2015) and testing data (everything after 2015). The model will only be exposed to the training data and expected to generate predictions for the testing data. Once the predictions are obtained, we‚Äôll calculate the performance using these predictions and the true observations.\n\n\nCode\n# visually inspect the train test split\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\"data is splitted, everything before 2015 is train data after 2015 test\")\nplt.show()"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#setting-up-the-regression",
    "href": "posts/boring_forecast/boring_linear_forecast.html#setting-up-the-regression",
    "title": "Boring linear forecast",
    "section": "6 Setting up the regression",
    "text": "6 Setting up the regression\n\n\nCode\nX_train = df_train[\"ds\"].astype(int).to_numpy().reshape(-1, 1)\ny_train = df_train[\"y\"].to_numpy()\n\nX_test = df_test[\"ds\"].astype(int).to_numpy().reshape(-1, 1)\ny_test = df_test[\"y\"].to_numpy()\n\n\nPreprocessing the data to prepare it for fitting the linear model: In this instance, we convert the date columns into a sequence of ever-increasing integers.\n\n\nCode\n# creating, fit, and inference\nlinear = LinearRegression()\nlinear.fit(X=X_train, y=y_train)\ny_pred = linear.predict(X=X_test)\n\n\nThe linear model is swift to fit, taking approximately 4 milliseconds on my machine. Given the modest data size (though it‚Äôs significant for time series), this enables us to fit 1000 models within the span of a single coffee sip‚òïÔ∏è.\nLet‚Äôs visualize the model results by plotting all relevant components: train data, test data, and predictions. Additionally, we will calculate two error metrics ‚Äì Mean Squared Error (mse) and Mean Absolute Error (mae) ‚Äì to quantify the performance of the model.\n\n\nCode\n# calc error metrics\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\n\n# visually inspect the prediction\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred,\n    label=\"prediction\",\n    fmt=\"--\",\n)\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(f\"linear regression applied (MSE= {mse:.3}, MAE={mae:.3})\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe linear model is represented by a plain straight line, as expected from a simple linear regression model. The green line passes through the orange data points roughly, moving in an intuitively correct direction. However, it fails to capture seasonality or other patterns present in the training set. With Mean Squared Error (MSE) and Mean Absolute Error (MAE) values around 0.6, we‚Äôll use this as a baseline for further improvement by incorporating dummies.\n\n\nmean squared error  = 0.617\nmean absolute error = 0.615"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#adding-dummies",
    "href": "posts/boring_forecast/boring_linear_forecast.html#adding-dummies",
    "title": "Boring linear forecast",
    "section": "7 Adding dummies",
    "text": "7 Adding dummies\nLet‚Äôs introduce dummy variables for the months to see whether we can enhance our model‚Äôs performance, both visually and numerically. We‚Äôll create a separate column for each month, where a 1 indicates the occurrence of that month, and a 0 signifies its absence. By doing so, we embed temporal information directly into the model, allowing it to discern the specific impacts of each month and adjust future predictions accordingly. That‚Äôs the theory‚Äînow, let‚Äôs put it to the test!\n\n\nCode\n# creating dummies for the months\ndf_dummies = df_in.assign(\n    month=df_in[\"ds\"].dt.month.astype(\"category\"),\n    ds_int=df_in[\"ds\"].astype(int),\n)\n\nnot_dummy = {\"y\", \"ds\", \"ds_int\"}\nto_dummy = list(set(df_dummies.columns) - not_dummy)\n\ndf_dummies = pd.get_dummies(data=df_dummies, columns=to_dummy)\nall_features = list(set(df_dummies.columns) - {\"y\", \"ds\"})\n\n# slicing the input in train test\ndf_train_dummies = df_dummies[(df_dummies[\"ds\"] &gt; \"2012\") & (df_dummies[\"ds\"] &lt; \"2015\")]\ndf_test_dummies = df_dummies[(df_dummies[\"ds\"] &gt; \"2015\")]\n\nX_train = df_train_dummies.loc[:, all_features]\ny_train = df_train_dummies[[\"y\"]]\n\nX_test = df_test_dummies.loc[:, all_features]\ny_test = df_test_dummies[[\"y\"]]\n\ndf_dummies.drop(columns=[\"ds_int\", \"y\"]).sample(\n    n=6,\n    ignore_index=True,\n    random_state=42,\n)\n\n\n\n\n\n\n\n\n\nds\nmonth_1\nmonth_2\nmonth_3\nmonth_4\nmonth_5\nmonth_6\nmonth_7\nmonth_8\nmonth_9\nmonth_10\nmonth_11\nmonth_12\n\n\n\n\n0\n2014-05-06\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n2012-07-05\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n2015-06-21\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n2012-03-09\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n2012-08-10\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n5\n2013-05-11\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\nWe‚Äôve added month dummies to our data‚Äîmonth_1 for January through month_12 for December. Next, we‚Äôll train our model with these dummies and compare performance against the original model without them.\n\n\nCode\n# create the pipeline and fit pipeline\n# scaler is there so that the coefs can be interpeted later\n# pipeline = make_pipeline(StandardScaler(), LinearRegression())\npipeline = make_pipeline(MinMaxScaler(), LinearRegression())\n\npipeline.fit(X=X_train, y=y_train)\ny_pred_dummies = pipeline.predict(X=X_test)\n\nmse_dummies = mean_squared_error(y_test, y_pred_dummies)\nmae_dummies = mean_absolute_error(y_test, y_pred_dummies)\n\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred,\n    label=\"prediction\",\n    fmt=\"--\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies,\n    label=\"prediction with dummies\",\n    fmt=\"--\",\n)\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\n    f\"linear regression with dummies applied (mse= {mse_dummies:.3}, mae={mae_dummies:.3})\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nVisually, our forecast has improved significantly. It now mirrors the ups and downs of the time series more closely and captures the overall trend more accurately. These enhancements are also evident in the error metrics, with the mean squared error improving by a factor of 1.89 and the mean absolute error by a factor of 1.54. Such substantial gains from simply incorporating ones and zeros are indeed impressive.\n\n\nCode\nprint(f\"mean squared error  = {mse_dummies:.3}\")\nprint(f\"improvement factor mse month dummies -&gt; {mse/mse_dummies:.3}x\")\n\nprint(\"-\" * 79)\n\nprint(f\"mean absolute error = {mae_dummies:.3}\")\nprint(f\"improvement factor mea month dummies -&gt; {mae/mae_dummies:.3}x\")\n\n\nmean squared error  = 0.323\nimprovement factor mse month dummies -&gt; 1.91x\n-------------------------------------------------------------------------------\nmean absolute error = 0.397\nimprovement factor mea month dummies -&gt; 1.55x"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#inspecting-the-seasonality",
    "href": "posts/boring_forecast/boring_linear_forecast.html#inspecting-the-seasonality",
    "title": "Boring linear forecast",
    "section": "8 Inspecting the seasonality",
    "text": "8 Inspecting the seasonality\nNow that we have encoded the information about the seasonality in the model, we can inspect that seasonality on its own. This will give us some insight into the inner workings of the underlying time series model. First, we access the coefficients of the linear model and put them into a separate DataFrame. Then, we need to scale them so that the relative difference is more apparent. Looking at the raw coefficients would not yield any meaningful information, as the scale is not relatable to the original problem.\n\n\nCode\n# pull coefs into a seperate df, to inspect the seasonality\nlin_reg_coefs = (\n    pd.DataFrame(\n        data=pipeline[\"linearregression\"].coef_,\n        columns=X_train.columns,\n    )\n    .T.reset_index()\n    .rename(columns={\"index\": \"month\", 0: \"coefficient\"})\n)\n# exclude the time col\nlin_reg_coefs = lin_reg_coefs[lin_reg_coefs[\"month\"] != \"ds_int\"]\n\n# subtract mean to get the relative difference between the coefs\nlin_reg_coefs[\"coefficient\"] = (\n    lin_reg_coefs[\"coefficient\"] - lin_reg_coefs[\"coefficient\"].mean()\n)\n\n\n\n\nCode\nchart = sns.barplot(\n    data=lin_reg_coefs,\n    x=\"month\",\n    y=\"coefficient\",\n    color=sns.color_palette()[0],\n    order=[f\"month_{i}\" for i in range(1, 13)],\n)\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"\")\nplt.title(\"yearly seasonality\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe have a clear picture of the seasonality throughout the year. There is a dip in the middle of the year, followed by a significant uptick in January. This can be attributed to the Super Bowl, which is played in the first week of February and drives a lot of traffic to the wiki page. The dip in the middle of the year is also evident at the month_6 mark."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#recap",
    "href": "posts/boring_forecast/boring_linear_forecast.html#recap",
    "title": "Boring linear forecast",
    "section": "9 Recap",
    "text": "9 Recap\nIn this blog post, I have demonstrated that the performance of a simple linear regression for time series forecasting can be improved by a factor of 1.54 up to 1.89 by simply adding dummy variables for the months. The nice thing about this approach is that the linear regression is available in most systems with analytical capabilities (yes, even in Excel), and adding the dummy variables is so simple that you can do it in a SQL server. The added benefit is that the model fitting is quick, allowing you to retrain the model monthly, weekly, daily, or even hourly."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#encore",
    "href": "posts/boring_forecast/boring_linear_forecast.html#encore",
    "title": "Boring linear forecast",
    "section": "10 Encore",
    "text": "10 Encore\nWhat if we were to dummies not just for the months, but also for other datetime features and really turn it up to eleven\n\nSure, let‚Äôs create the dummy variables for the given datetime features and then apply the Elastic Net linear model.\nFirst, let‚Äôs create the dummy variables for the datetime features:\n\nMonth: This feature will create 12 dummy variables, one for each month.\nWeek: This feature will create 52 dummy variables, one for each week of the year.\nDay of the Week: This feature will create 7 dummy variables, one for each day of the week.\nIs Weekend: This feature will create 2 dummy variables, one for weekdays and one for weekends.\nQuarter: This feature will create 4 dummy variables, one for each quarter of the year.\n\nBy creating these dummy variables, we will end up with a large number of features, which can lead to overfitting. This is where the Elastic Net linear model comes in handy.\nThe Elastic Net is a regularized linear regression model that combines the L1 (Lasso) and L2 (Ridge) regularization techniques. This helps to handle the large number of features and prevent overfitting. The Elastic Net model will automatically select the most important features and shrink the coefficients of the less important ones towards zero.\nAs an exercise, you can use the sklearn.linear_model.ElasticNetCV class to train the Elastic Net model on your dataset. This class will automatically tune the hyperparameters (alpha and l1_ratio) using cross-validation, which can help to find the optimal balance between the L1 and L2 regularization.\nRemember to split your dataset into training and testing sets, and evaluate the performance of the Elastic Net model on the test set. This will give you a good idea of how well the model can handle the large number of features created by the dummy variables.\n\n\nCode\n# creating dummies for the months\ndf_dummies_all = df_in.assign(\n    month=df_in[\"ds\"].dt.month.astype(\"category\"),\n    week=df_in[\"ds\"].dt.isocalendar().week.astype(\"category\"),\n    dayofweek=df_in[\"ds\"].dt.dayofweek.astype(\"category\"),\n    is_weekend=(df_in[\"ds\"].dt.dayofweek) &gt;= 5,\n    quarter=df_in[\"ds\"].dt.quarter.astype(\"category\"),\n    ds_int=df_in[\"ds\"].astype(int),\n)\n\nnot_dummy = {\"y\", \"ds\", \"ds_int\"}\nto_dummy = list(set(df_dummies_all.columns) - not_dummy)\n\ndf_dummies_all = pd.get_dummies(\n    data=df_dummies_all,\n    columns=to_dummy,\n    drop_first=True,  # reduce the amount of cols with no additional info\n)\nall_features = list(set(df_dummies_all.columns) - {\"y\", \"ds\"})\n\n# slicing the input in train test\ndf_train_dummies_all = df_dummies_all[\n    (df_dummies_all[\"ds\"] &gt; \"2012\") & (df_dummies_all[\"ds\"] &lt; \"2015\")\n]\ndf_test_dummies_all = df_dummies_all[(df_dummies_all[\"ds\"] &gt; \"2015\")]\n\nX_train = df_train_dummies_all.loc[:, all_features]\ny_train = df_train_dummies_all[[\"y\"]]\n\nX_test = df_test_dummies_all.loc[:, all_features]\ny_test = df_test_dummies_all[[\"y\"]]\n\ndf_dummies_all.drop(columns=[\"ds_int\", \"y\"]).sample(\n    n=6,\n    ignore_index=True,\n    random_state=42,\n)\n\n\n\n\n\n\n\n\n\nds\nquarter_2\nquarter_3\nquarter_4\ndayofweek_1\ndayofweek_2\ndayofweek_3\ndayofweek_4\ndayofweek_5\ndayofweek_6\n...\nmonth_3\nmonth_4\nmonth_5\nmonth_6\nmonth_7\nmonth_8\nmonth_9\nmonth_10\nmonth_11\nmonth_12\n\n\n\n\n0\n2014-05-06\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n2012-07-05\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n2015-06-21\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n2012-03-09\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n2012-08-10\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n5\n2013-05-11\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n6 rows √ó 74 columns\n\n\n\n\n\nCode\n# utilzing an elasticnet linear model to compensate for the amount of features\nelastic_params = {\n    \"l1_ratio\": np.linspace(start=0.000001, stop=1, num=100),\n    \"cv\": 7,\n    \"n_alphas\": 1_00,\n    \"n_jobs\": -1,\n}\n\npipeline_all = make_pipeline(MinMaxScaler(), ElasticNetCV(**elastic_params))\n\npipeline_all.fit(X=X_train, y=y_train.to_numpy().ravel())\ny_pred_dummies_all = pipeline_all.predict(X=X_test)\n\nmse_dummies_all = mean_squared_error(y_test, y_pred_dummies_all)\nmae_dummies_all = mean_absolute_error(y_test, y_pred_dummies_all)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n    alpha=0.7,\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies,\n    label=\"prediction with dummies\",\n    fmt=\"--\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies_all,\n    label=\"prediction with all dummies\",\n    fmt=\"--\",\n)\n\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\n    f\"linear regression with dummies applied (mse= {mse_dummies_all:.3}, mae={mae_dummies_all:.3})\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nImmediately, it becomes evident that the model captures more of the fine-grained movement of the time series. This is also reflected in the fact that both error metrics have improved.\n\n\nCode\nprint(f\"mean squared error = {mse_dummies_all:.3}\")\nprint(f\"improvement factor mse month dummies -&gt; {mse/mse_dummies:.3}x\")\nprint(f\"improvement factor mse all dummies   -&gt; {mse/mse_dummies_all:.3}x\")\nprint(\"-\" * 79)\n\nprint(f\"mean absolute error = {mae_dummies_all:.3}\")\nprint(f\"improvement factor mea month dummies -&gt; {mae/mae_dummies:.3}x\")\nprint(f\"improvement factor mea all dummies   -&gt; {mae/mae_dummies_all:.3}x\")\n\n\nmean squared error = 0.262\nimprovement factor mse month dummies -&gt; 1.91x\nimprovement factor mse all dummies   -&gt; 2.35x\n-------------------------------------------------------------------------------\nmean absolute error = 0.356\nimprovement factor mea month dummies -&gt; 1.55x\nimprovement factor mea all dummies   -&gt; 1.73x"
  },
  {
    "objectID": "posts/stepcat/stepcat.html",
    "href": "posts/stepcat/stepcat.html",
    "title": "Some points are more or less random than others",
    "section": "",
    "text": "Code\nimport time\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport seaborn as sns\nfrom catboost import CatBoostClassifier, Pool\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nfrom scipy.stats import qmc\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import ParameterGrid, train_test_split\nfrom tqdm.notebook import tqdm\nCode\nset_matplotlib_formats(\"svg\")\nsns.set_style(\"darkgrid\")\nsns.set_context(\"paper\")\nsns.set_context(context=\"notebook\", font_scale=1.5)\n\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nFIGSIZE = (12, 6)\nN_SAMPLES = 100"
  },
  {
    "objectID": "posts/stepcat/stepcat.html#gridsearch",
    "href": "posts/stepcat/stepcat.html#gridsearch",
    "title": "Some points are more or less random than others",
    "section": "1.1 GridSearch",
    "text": "1.1 GridSearch\nGridSearchCV is a hyperparameter tuning method in Scikit-Learn that utilizes exhaustive search to determine the optimal combination of estimator parameters. It evaluates each set of specified parameter values by trying all possible combinations using cross-validation. The advantages of this approach include exhaustive exploration, providing detailed performance information. However, it is time-consuming and memory-intensive since it tests all combinations. The choice of cross-validation method and the number of folds significantly impact the results."
  },
  {
    "objectID": "posts/stepcat/stepcat.html#randomsearch",
    "href": "posts/stepcat/stepcat.html#randomsearch",
    "title": "Some points are more or less random than others",
    "section": "1.2 RandomSearch",
    "text": "1.2 RandomSearch\nRandomizedSearchCV is a hyperparameter tuning method in Scikit-Learn that utilizes random sampling to find the optimal combination of estimator parameters. In contrast to GridSearchCV, which tests all possible combinations, RandomizedSearchCV samples a fixed number of parameter settings from specified distributions, making it more efficient in exploring the entire search-space. Cross-validation is employed to assess each sampled set, and continuous or categorical distributions can be defined. The chosen distributions, iterations, cross-validation method, and the number of folds all impact the results."
  },
  {
    "objectID": "posts/stepcat/stepcat.html#something-in-between",
    "href": "posts/stepcat/stepcat.html#something-in-between",
    "title": "Some points are more or less random than others",
    "section": "1.3 Something in Between",
    "text": "1.3 Something in Between\nThere are techniques, such as Latin Hypercube Sampling (LHS) or Poisson Disc sampling, that combine grid and random sampling methods.\nLatin Hypercube Sampling is a deterministic, space-filling design technique that selects a representative sample from the parameter space. It does so by ensuring that every hyperrectangle enclosing each unique combination of parameters contains at least one sample. In simpler terms, LHS aims to provide a representative and uniform distribution of samples across the entire range of possible combinations.\nConsider the analogy of a Sudoku puzzle. Each square on the board represents a unique combination of parameters. While you may be tempted to reveal all the numbers in one row or column, this would be unwise as it would limit the overall usefulness of your picks. LHS offers a framework for reducing the number of picks while maximizing their effectiveness. For instance, if you have selected a square from one row, it would favor other rows more, and the same logic applies to the columns and sub-squares.\n\n\n\nSudoku Puzzle\n\n\nIn conclusion, when choosing between hyperparameter tuning methods, it is essential to consider factors like computational resources, time constraints, and the complexity of the problem at hand. GridSearchCV offers exhaustive exploration but comes with higher computational costs, while RandomizedSearchCV provides flexibility and efficiency but may not find the absolute best combination of parameters. Latin Hypercube Sampling offers a balance between these two extremes by providing representative samples while being more computationally efficient than GridSearchCV for large search spaces."
  },
  {
    "objectID": "posts/stepcat/stepcat.html#visual-representation",
    "href": "posts/stepcat/stepcat.html#visual-representation",
    "title": "Some points are more or less random than others",
    "section": "1.4 Visual Representation",
    "text": "1.4 Visual Representation\nTo illustrate the distinction between the three sampling methods, let‚Äôs consider a hypothetical scenario in a two-dimensional space, ranging from 0 to 1 inclusively, and apply the following three sampling strategies: random, Latin Hypercube Sampling (LHS), and grid.\n\n\nCode\nnp.random.seed(99)\n\nuni_sample = np.random.uniform(0, 1, (100, 2))\n\nsampler = qmc.LatinHypercube(d=2, optimization=\"lloyd\")\nlhs_sample = sampler.random(n=100)\n\ngrid_sample = np.array(\n    [[i, j] for i in np.linspace(0, 1, 10) for j in np.linspace(0, 1, 10)]\n)\n\n\n\n\nCode\n_, ax = plt.subplots(1, 3, figsize=(12, 4))\n\n\ncurr_ax = ax[0]\nsns.histplot(\n    x=uni_sample[:, 0], y=uni_sample[:, 1], bins=10, ax=curr_ax, cbar=True, thresh=None\n)\nsns.scatterplot(x=uni_sample[:, 0], y=uni_sample[:, 1], ax=curr_ax, color=\"tab:orange\")\ncurr_ax.set_xticks([])\ncurr_ax.set_yticks([])\ncurr_ax.set_title(\"uniform sampling\")\n\n\ncurr_ax = ax[1]\nsns.histplot(\n    x=lhs_sample[:, 0], y=lhs_sample[:, 1], bins=10, ax=curr_ax, cbar=True, thresh=None\n)\nsns.scatterplot(x=lhs_sample[:, 0], y=lhs_sample[:, 1], ax=curr_ax, color=\"tab:orange\")\ncurr_ax.set_xticks([])\ncurr_ax.set_yticks([])\ncurr_ax.set_title(\"latin hypercube sampling\")\n\n\ncurr_ax = ax[2]\nsns.histplot(\n    x=grid_sample[:, 0],\n    y=grid_sample[:, 1],\n    bins=10,\n    ax=curr_ax,\n    cbar=True,\n    thresh=None,\n)\nsns.scatterplot(\n    x=grid_sample[:, 0], y=grid_sample[:, 1], ax=curr_ax, color=\"tab:orange\"\n)\ncurr_ax.set_xticks([])\ncurr_ax.set_yticks([])\ncurr_ax.set_title(\"grid sampling\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plots above illustrate that the random sampling method selects points randomly from the entire space, which can result in clustering or gaps. In contrast, Latin Hypercube Sampling ensures that each row and column contain a representative sample; however, it still generates a random pattern and disrupts the orthogonality of the grid sampling technique. Finally, the grid sampling method selects points at regular intervals, resulting in a uniform distribution across the space. Nevertheless, it may not be the most efficient method for large search spaces."
  },
  {
    "objectID": "posts/stepcat/stepcat.html#information-on-the-dataset",
    "href": "posts/stepcat/stepcat.html#information-on-the-dataset",
    "title": "Some points are more or less random than others",
    "section": "2.1 Information on the Dataset",
    "text": "2.1 Information on the Dataset\nThe dataset consists of 9 categorical and 7 numerical features, making it suitable for use with CatBoost. This algorithm uses a specialized technique called ‚Äúordered target encoding‚Äù to convert categorical features into numerical values, which can improve model performance and accuracy in gradient boosting models. This approach is particularly effective when managing datasets with numerous categorical features compared to other machine learning algorithms.\n\n\nCode\nX.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 11303 entries, 13121 to 3830\nData columns (total 16 columns):\n #   Column  Non-Null Count  Dtype   \n---  ------  --------------  -----   \n 0   V1      11303 non-null  int64   \n 1   V2      11303 non-null  category\n 2   V3      11303 non-null  category\n 3   V4      11303 non-null  category\n 4   V5      11303 non-null  category\n 5   V6      11303 non-null  int64   \n 6   V7      11303 non-null  category\n 7   V8      11303 non-null  category\n 8   V9      11303 non-null  category\n 9   V10     11303 non-null  int64   \n 10  V11     11303 non-null  category\n 11  V12     11303 non-null  int64   \n 12  V13     11303 non-null  int64   \n 13  V14     11303 non-null  int64   \n 14  V15     11303 non-null  int64   \n 15  V16     11303 non-null  category\ndtypes: category(9), int64(7)\nmemory usage: 807.6 KB\n\n\n\n2.1.1 Creating Data Pools\nCatBoost offers the capability to create data pools. This feature enables CatBoost to optimize the handling of large files internally, providing benefits such as defining categorical features in one place for improved reproducibility and subsequent reuse.\n\n\nCode\ntrain_pool = Pool(\n    data=X_train,\n    label=y_train,\n    cat_features=list(X.select_dtypes(include=\"category\").columns),\n)\n\ntest_pool = Pool(\n    data=X_test,\n    label=y_test,\n    cat_features=list(X.select_dtypes(include=\"category\").columns),\n)"
  },
  {
    "objectID": "posts/stepcat/stepcat.html#applying-three-sampling-techniques-grid-random-and-latin-hypercube",
    "href": "posts/stepcat/stepcat.html#applying-three-sampling-techniques-grid-random-and-latin-hypercube",
    "title": "Some points are more or less random than others",
    "section": "2.2 Applying Three Sampling Techniques: Grid, Random, and Latin Hypercube",
    "text": "2.2 Applying Three Sampling Techniques: Grid, Random, and Latin Hypercube\nIn the following section, we apply and compare the results of three sampling techniques: grid search, random search, and Latin Hypercube Sampling (LHS). Our objective is to demonstrate that LHS explores more of the design space than random search while maintaining sufficient randomness to mitigate the symmetry issues inherent in grid search.\nThe search space is defined by the following parameters:\n\nDepth: 1 to 15\nIterations: 1 to 1024\nSubsample: 0.1 to 1\nBagging Temperature: 1 to 1e6\n\nWe use 100 samples for each technique."
  },
  {
    "objectID": "posts/stepcat/stepcat.html#grid-search",
    "href": "posts/stepcat/stepcat.html#grid-search",
    "title": "Some points are more or less random than others",
    "section": "2.3 Grid Search",
    "text": "2.3 Grid Search\nTo adhere to the limitation of 100 samples, we use \\(100^{1/4} \\approx 3\\), which results in \\(3^{4}=81\\) samples.\n\n\nCode\nn_grid_samples = np.power(N_SAMPLES, 1 / 4)\nn_grid_samples = np.floor(n_grid_samples).astype(int)\n\nparam_grid = {\n    \"depth\": np.linspace(1, 15, n_grid_samples, dtype=int),\n    \"iterations\": np.linspace(1, 1024, n_grid_samples, dtype=int),\n    \"subsample\": np.linspace(0.1, 1, n_grid_samples, endpoint=False),\n    \"bagging_temperature\": np.linspace(1, 1000000, n_grid_samples, dtype=int),\n}\n\nparam_combo = list(ParameterGrid(param_grid=param_grid))\n\ncube_search = []\nstart = time.time()\nfor param in param_combo:\n    cbc = CatBoostClassifier(\n        **param,\n        eval_metric=\"F1\",\n    )\n    cbc.fit(train_pool, eval_set=(test_pool), verbose=False)\n\n    score = cbc.get_best_score().get(\"validation\").get(\"F1\")\n    param[\"score\"] = score\n    param[\"iterations\"] = cbc.get_best_iteration()\n\n    cube_search.append(param)\nend = time.time()\nprint(f\"time grid: {end - start:.2f} sec\")\ngrid_results = pd.DataFrame(cube_search).sort_values(by=\"score\", ascending=False)\n\n\ntime grid: 1538.35 sec\n\n\n\n\nCode\ngrid_results.head()\n\n\n\n\n\n\n\n\n\nbagging_temperature\ndepth\niterations\nsubsample\nscore\n\n\n\n\n70\n1000000\n8\n258\n0.4\n0.569507\n\n\n43\n500000\n8\n258\n0.4\n0.569507\n\n\n16\n1\n8\n258\n0.4\n0.569507\n\n\n40\n500000\n8\n216\n0.4\n0.565611\n\n\n13\n1\n8\n216\n0.4\n0.565611"
  },
  {
    "objectID": "posts/stepcat/stepcat.html#random-search",
    "href": "posts/stepcat/stepcat.html#random-search",
    "title": "Some points are more or less random than others",
    "section": "2.4 Random Search",
    "text": "2.4 Random Search\nFor depth and iterations, the model expects integer values; therefore, randint is used. For subsample and bagging temperature, uniform is utilized since the model accepts float values for these parameters.\n\n\nCode\nrandom_sample = np.hstack(\n    (\n        np.random.randint(1, 15, (N_SAMPLES, 1)),\n        np.random.randint(1, 1024, (N_SAMPLES, 1)),\n        np.random.uniform(0, 1, (N_SAMPLES, 1)),\n        np.random.uniform(1, 1_000_000, (N_SAMPLES, 1)),\n    )\n)\n\nparam_combo = list(\n    pd.DataFrame(\n        data=random_sample,\n        columns=[\"depth\", \"iterations\", \"subsample\", \"bagging_temperature\"],\n    )\n    .to_dict(\"index\")\n    .values()\n)\n\n\ncube_search = []\nstart = time.time()\nfor param in param_combo:\n    cbc = CatBoostClassifier(\n        **param,\n        eval_metric=\"F1\",\n    )\n    cbc.fit(train_pool, eval_set=(test_pool), verbose=False)\n\n    score = cbc.get_best_score().get(\"validation\").get(\"F1\")\n    param[\"score\"] = score\n    param[\"iterations\"] = cbc.get_best_iteration()\n\n    cube_search.append(param)\n\nend = time.time()\nprint(f\"time random: {end - start:.2f} sec\")\nrandom_results = pd.DataFrame(cube_search).sort_values(by=\"score\", ascending=False)\n\n\ntime random: 572.61 sec\n\n\n\n\nCode\nrandom_results.head()\n\n\n\n\n\n\n\n\n\ndepth\niterations\nsubsample\nbagging_temperature\nscore\n\n\n\n\n58\n7.0\n437\n0.281857\n582472.959943\n0.576497\n\n\n11\n7.0\n343\n0.437535\n187632.443518\n0.575893\n\n\n91\n5.0\n713\n0.207018\n544578.689749\n0.568233\n\n\n89\n9.0\n214\n0.658629\n129890.179531\n0.566893\n\n\n65\n8.0\n369\n0.606488\n759745.435830\n0.565022"
  },
  {
    "objectID": "posts/stepcat/stepcat.html#lhs-search",
    "href": "posts/stepcat/stepcat.html#lhs-search",
    "title": "Some points are more or less random than others",
    "section": "2.5 LHS Search",
    "text": "2.5 LHS Search\nWhen applying the LHS (Latin Hypercube Sampling) technique, you need to specify the number of dimensions for the resulting sample and the number of samples you want. These parameters are on a different scale than the search space that you want to explore; therefore, a scaling is applied to adapt it to the desired space. Subsequently, a type conversion is applied.\n\n\nCode\nsampler = qmc.LatinHypercube(d=4, optimization=\"lloyd\")\nsample = sampler.random(n=N_SAMPLES)\n\nl_bounds = [1, 1, 0, 1]\nu_bounds = [15, 1024, 1, 1000000]\nsample_scaled = qmc.scale(sample, l_bounds, u_bounds)\n\n# convert to int\nsample_scaled[:, :2] = sample_scaled[:, :2].astype(int)\n\n\nparam_combo = list(\n    pd.DataFrame(\n        data=sample_scaled,\n        columns=[\"depth\", \"iterations\", \"subsample\", \"bagging_temperature\"],\n    )\n    .to_dict(\"index\")\n    .values()\n)\n\ncube_search = []\nstart = time.time()\nfor param in param_combo:\n    cbc = CatBoostClassifier(\n        **param,\n        eval_metric=\"F1\",\n    )\n    cbc.fit(train_pool, eval_set=(test_pool), verbose=False)\n\n    score = cbc.get_best_score().get(\"validation\").get(\"F1\")\n    param[\"score\"] = score\n    param[\"iterations\"] = cbc.get_best_iteration()\n\n    cube_search.append(param)\n\nend = time.time()\nprint(f\"time lhs: {end - start:.2f} sec\")\nlhs_results = pd.DataFrame(cube_search).sort_values(by=\"score\", ascending=False)\n\n\ntime lhs: 527.88 sec\n\n\n\n\nCode\nlhs_results.head()\n\n\n\n\n\n\n\n\n\ndepth\niterations\nsubsample\nbagging_temperature\nscore\n\n\n\n\n71\n7.0\n858\n0.183817\n282489.633420\n0.577007\n\n\n72\n8.0\n404\n0.949294\n368410.292079\n0.573913\n\n\n94\n9.0\n148\n0.490337\n655506.162010\n0.570787\n\n\n25\n7.0\n655\n0.760262\n385807.469267\n0.568966\n\n\n26\n10.0\n244\n0.710746\n567170.475059\n0.566210"
  },
  {
    "objectID": "posts/stepcat/stepcat.html#analysis-of-results",
    "href": "posts/stepcat/stepcat.html#analysis-of-results",
    "title": "Some points are more or less random than others",
    "section": "2.6 Analysis of results",
    "text": "2.6 Analysis of results\n\n\nCode\ndf_all_sample = pd.concat(\n    [\n        lhs_results.assign(sampling=\"lhs\"),\n        random_results.assign(sampling=\"random\"),\n        grid_results.assign(sampling=\"grid\"),\n    ],\n    axis=0,\n)\n\ndf_all_sample = df_all_sample.sort_values(by=\"score\", ascending=False)\n\ndf_all_sample.head(n=10)\n\n\n\n\n\n\n\n\n\ndepth\niterations\nsubsample\nbagging_temperature\nscore\nsampling\n\n\n\n\n71\n7.0\n858\n0.183817\n282489.633420\n0.577007\nlhs\n\n\n58\n7.0\n437\n0.281857\n582472.959943\n0.576497\nrandom\n\n\n11\n7.0\n343\n0.437535\n187632.443518\n0.575893\nrandom\n\n\n72\n8.0\n404\n0.949294\n368410.292079\n0.573913\nlhs\n\n\n94\n9.0\n148\n0.490337\n655506.162010\n0.570787\nlhs\n\n\n16\n8.0\n258\n0.400000\n1.000000\n0.569507\ngrid\n\n\n43\n8.0\n258\n0.400000\n500000.000000\n0.569507\ngrid\n\n\n70\n8.0\n258\n0.400000\n1000000.000000\n0.569507\ngrid\n\n\n25\n7.0\n655\n0.760262\n385807.469267\n0.568966\nlhs\n\n\n91\n5.0\n713\n0.207018\n544578.689749\n0.568233\nrandom\n\n\n\n\n\n\n\n\n\n\nMethod\nTime (sec)\nBest Score\n\n\n\n\nGrid\n1538\n0.569\n\n\nRandom\n573\n0.576\n\n\nLHS\n528\n0.577\n\n\n\nObserving the time taken by each method, it is evident that grid search is the slowest, followed by random search and then LHS. This outcome is expected, as grid search is the most exhaustive search method, utilizing a large number of search points with high values for iterations (which control the number of trees in the model). In comparison, random search and LHS are faster than grid search due to having fewer points with high-value iterations. The difference is significant, with a reduction factor in runtime of around 3.\nFrom a scoring perspective, the LHS method identified a set of hyperparameters that achieved the highest score. Random search followed closely behind, while grid search performed the worst."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hyper-shotgun",
    "section": "",
    "text": "Pyspark ‚ö°Ô∏è style guide\n\n\nappear in style and sparkly \n\n\nPySpark is a wrapper language that allows users to interface with an Apache Spark backend to quickly process data. Spark can operate on massive datasets across a distributed network of servers, providing major performance and reliability benefits when utilized correctly. It presents challenges, even for experienced Python developers, as the PySpark syntax draws on the JVM heritage of Spark and therefore implements code patterns that may be unfamiliar. Adapted from here and here\n\n\n\n\n\n2025-01-25\n\n\nJoost de Theije + LLM\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Git-Account-Mastery\n\n\nmanaging SSH Keys: A (Wacky) Solution for Multiple GitHub Accounts\n\n\nStreamline Your Git Workflow. Manage multiple GitHub accounts and SSH keys using custom .gitconfig files. Learn how to configure global Git settings, include specific configuration files, and remap hostnames in your SSH config file. Automate your git workflow and say farewell to fiddeling around with SSH and GPG keys.\n\n\n\n\n\n2024-04-23\n\n\nJoost de Theije + LLM\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSome points are more or less random than others\n\n\nA Comparative Analysis of Sampling Strategies\n\n\nThis article delves into various techniques for hyperparameter optimization, comparing the efficiency and effectiveness of Grid Search, Random Search, and Latin Hypercube Sampling in the context of gradient boosting models.\n\n\n\n\n\n2024-03-14\n\n\nJoost de Theije + LLM\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Partial Dependence Plots\n\n\nVisualizing Model Relationships\n\n\nThis article provides an overview of Partial Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots, powerful tools for visualizing the relationships between features and predictions in machine learning models. We‚Äôll explore how these plots can help to interpret model behavior, revealing both average effects and individual data point responses.\n\n\n\n\n\n2024-02-11\n\n\nJoost de Theije + LLM\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nBoring linear forecast\n\n\nImproving Performance by Adding Some Dummies\n\n\nDiscover how the simple addition of dummy variables can transform a linear forecast from mundane to insightful. This article explores the significant impact of encoding seasonality into your models and provides a step-by-step guide on improving forecast accuracy.\n\n\n\n\n\n2023-03-5\n\n\nJoost de Theije + LLM\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01_pyspark_style/pyspark_style.html",
    "href": "posts/01_pyspark_style/pyspark_style.html",
    "title": "Pyspark ‚ö°Ô∏è style guide",
    "section": "",
    "text": "Always start with the following imports, F prefix for all PySpark functions, T prefix for all PySpark types, and W for Windows. Adding the #noqa comment at the end of the line will disable linters (like flake8) from flagging the line as an error. Importing these will enable you to use the types and functions without importing each item separately, resulting in a speedy workflow. This will also eliminate the oopsie when you are applying the sum from the Python library instead of the PySpark library‚òπ.\nfrom pyspark.sql import DataFrame as df, functions as F, types as T, Window as W #noqa\n\nF.sum()\nT.IntegerType()\nW.partitionBy()\n\n\n\ntype hint and docstrings are a pretty good idea, it has numerous benefits such as - improve the readability -&gt; makes it easier to understand the code - enhance the IDE support -&gt; autocompletion, type checking, etc. - better documentation -&gt; document while you are coding store the solution with the problem - static type checking -&gt; tools like mypy can be used to help you catch errors early.\nfrom pyspark.sql import DataFrame\n\n# bad\ndef calculate_daily_scoops(df, flavor_col, date_col):\n    return df.groupBy(date_col, flavor_col).agg(F.sum(\"scoops_sold\").alias(\"total_scoops\"))\n\n# good\ndef calculate_daily_scoops(df: DataFrame, flavor_col: str, date_col: str) -&gt; DataFrame:\n    \"\"\"Calculate total scoops sold per day and flavor.\n\n    Groups the input DataFrame by date and flavor, aggregating the total number\n    of scoops sold for each unique combination.\n\n    Parameters\n    ----------\n    df : DataFrame\n        Input DataFrame containing sales data.\n    flavor_col : str\n        Name of the column containing ice cream flavor information.\n    date_col : str\n        Name of the column containing date information.\n\n    Returns\n    -------\n    DataFrame\n        DataFrame with total scoops sold, grouped by date and flavor.\n        Contains columns for date, flavor, and total scoops.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pyspark.sql.functions as F\n    &gt;&gt;&gt; sales_df = spark.createDataFrame(...)\n    &gt;&gt;&gt; daily_scoops = calculate_daily_scoops(sales_df, 'flavor', 'sale_date')\n    &gt;&gt;&gt; daily_scoops.show()\n    \"\"\"\n    return df.groupBy(date_col, flavor_col).agg(\n        F.sum(\"scoops_sold\").alias(\"total_scoops\")\n    )\n\n\n\nblack, just use black."
  },
  {
    "objectID": "posts/01_pyspark_style/pyspark_style.html#import-pyspark-modules-with-default-aliases",
    "href": "posts/01_pyspark_style/pyspark_style.html#import-pyspark-modules-with-default-aliases",
    "title": "Pyspark ‚ö°Ô∏è style guide",
    "section": "",
    "text": "Always start with the following imports, F prefix for all PySpark functions, T prefix for all PySpark types, and W for Windows. Adding the #noqa comment at the end of the line will disable linters (like flake8) from flagging the line as an error. Importing these will enable you to use the types and functions without importing each item separately, resulting in a speedy workflow. This will also eliminate the oopsie when you are applying the sum from the Python library instead of the PySpark library‚òπ.\nfrom pyspark.sql import DataFrame as df, functions as F, types as T, Window as W #noqa\n\nF.sum()\nT.IntegerType()\nW.partitionBy()"
  },
  {
    "objectID": "posts/01_pyspark_style/pyspark_style.html#type-annotaion-and-docstrings-in-functions",
    "href": "posts/01_pyspark_style/pyspark_style.html#type-annotaion-and-docstrings-in-functions",
    "title": "Pyspark ‚ö°Ô∏è style guide",
    "section": "",
    "text": "type hint and docstrings are a pretty good idea, it has numerous benefits such as - improve the readability -&gt; makes it easier to understand the code - enhance the IDE support -&gt; autocompletion, type checking, etc. - better documentation -&gt; document while you are coding store the solution with the problem - static type checking -&gt; tools like mypy can be used to help you catch errors early.\nfrom pyspark.sql import DataFrame\n\n# bad\ndef calculate_daily_scoops(df, flavor_col, date_col):\n    return df.groupBy(date_col, flavor_col).agg(F.sum(\"scoops_sold\").alias(\"total_scoops\"))\n\n# good\ndef calculate_daily_scoops(df: DataFrame, flavor_col: str, date_col: str) -&gt; DataFrame:\n    \"\"\"Calculate total scoops sold per day and flavor.\n\n    Groups the input DataFrame by date and flavor, aggregating the total number\n    of scoops sold for each unique combination.\n\n    Parameters\n    ----------\n    df : DataFrame\n        Input DataFrame containing sales data.\n    flavor_col : str\n        Name of the column containing ice cream flavor information.\n    date_col : str\n        Name of the column containing date information.\n\n    Returns\n    -------\n    DataFrame\n        DataFrame with total scoops sold, grouped by date and flavor.\n        Contains columns for date, flavor, and total scoops.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pyspark.sql.functions as F\n    &gt;&gt;&gt; sales_df = spark.createDataFrame(...)\n    &gt;&gt;&gt; daily_scoops = calculate_daily_scoops(sales_df, 'flavor', 'sale_date')\n    &gt;&gt;&gt; daily_scoops.show()\n    \"\"\"\n    return df.groupBy(date_col, flavor_col).agg(\n        F.sum(\"scoops_sold\").alias(\"total_scoops\")\n    )"
  },
  {
    "objectID": "posts/01_pyspark_style/pyspark_style.html#formatting",
    "href": "posts/01_pyspark_style/pyspark_style.html#formatting",
    "title": "Pyspark ‚ö°Ô∏è style guide",
    "section": "",
    "text": "black, just use black."
  },
  {
    "objectID": "posts/01_pyspark_style/pyspark_style.html#when-to-deviate",
    "href": "posts/01_pyspark_style/pyspark_style.html#when-to-deviate",
    "title": "Pyspark ‚ö°Ô∏è style guide",
    "section": "2.1 When to deviate",
    "text": "2.1 When to deviate\nIn some contexts, you might need to access columns from multiple dataframes with overlapping names. For example, in matching expressions like df.join(df2, on=(df.key == df2.key), how='left'). In such cases, it‚Äôs acceptable to reference columns directly by their dataframe. Please see Joins for more details."
  },
  {
    "objectID": "posts/01_pyspark_style/pyspark_style.html#modifying-columns-in-select",
    "href": "posts/01_pyspark_style/pyspark_style.html#modifying-columns-in-select",
    "title": "Pyspark ‚ö°Ô∏è style guide",
    "section": "4.1 Modifying columns in select",
    "text": "4.1 Modifying columns in select\nThe select() statement redefines the schema of a dataframe, so it naturally supports the inclusion or exclusion of columns, old and new, as well as the redefinition of pre-existing ones. By centralising all such operations in a single statement, it becomes much easier to identify the final schema, which makes code more readable. It also makes code more concise.\nInstead of calling withColumnRenamed() -&gt; use alias():\n#bad\ndf.select('key', 'comments').withColumnRenamed('comments', 'num_comments')\n\n# good\ndf.select(\"key\", F.col(\"comments\").alias(\"num_comments\"))\nInstead of using withColumn() to redefine type -&gt; cast() in the select:\n# bad\ndf.select('comments').withColumn('comments', F.col('comments').cast('double'))\n\n# good\ndf.select(F.col(\"comments\").cast(\"double\"))"
  },
  {
    "objectID": "posts/01_pyspark_style/pyspark_style.html#inclusive-selection-above-exclusive",
    "href": "posts/01_pyspark_style/pyspark_style.html#inclusive-selection-above-exclusive",
    "title": "Pyspark ‚ö°Ô∏è style guide",
    "section": "4.2 Inclusive selection above exclusive",
    "text": "4.2 Inclusive selection above exclusive\nOnly select columns that are needed, avoid using drop() as over time it might will return a different dataframe than expected, and it will be your job of fixing it.\nFinally, instead of adding new columns via the select statement, using .withColumn() is recommended when adding a single columns. When adding or manipulating tens or hundreds of columns, use a single .select() for performance reasons."
  },
  {
    "objectID": "posts/01_pyspark_style/pyspark_style.html#column-collisions-when-joining",
    "href": "posts/01_pyspark_style/pyspark_style.html#column-collisions-when-joining",
    "title": "Pyspark ‚ö°Ô∏è style guide",
    "section": "8.1 Column collisions when joining",
    "text": "8.1 Column collisions when joining\nAvoid renaming all columns to avoid collisions. Instead, give an alias to the whole dataframe, and use that alias to select which columns you want in the end.\n# bad\ncolumns = ['start_time', 'end_time', 'idle_time', 'total_time']\nfor col in columns:\n    flights = flights.withColumnRenamed(col, 'flights_' + col)\n    parking = parking.withColumnRenamed(col, 'parking_' + col)\n\nflights = flights.join(parking, on='flight_code', how='left')\n\nflights = flights.select(\n    F.col('flights_start_time').alias('flight_start_time'),\n    F.col('flights_end_time').alias('flight_end_time'),\n    F.col('parking_total_time').alias('client_parking_total_time')\n)\n\n# good\nflights = flights.alias(\"flights\")\nparking = parking.alias(\"parking\")\n\nflights = flights.join(other=parking, on=\"flight_code\", how=\"left\")\n\nflights = flights.select(\n    F.col(\"flights.start_time\").alias(\"flight_start_time\"),\n    F.col(\"flights.end_time\").alias(\"flight_end_time\"),\n    F.col(\"parking.total_time\").alias(\"client_parking_total_time\"),\n)\nIn such cases, keep in mind:\n\nIt is a better idea to only select the cols that are needed before joining\nIn case you do need both, it might be best to rename one of them prior to joining, signaling the difference between the two cols, as most likely the underlying data generating process is different.\nYou should always resolve ambiguous columns before outputting a dataset. After the transform is finished you can no longer distinguish between cols."
  },
  {
    "objectID": "posts/01_pyspark_style/pyspark_style.html#dropduplicates-and-.distinct-to-clean-joins",
    "href": "posts/01_pyspark_style/pyspark_style.html#dropduplicates-and-.distinct-to-clean-joins",
    "title": "Pyspark ‚ö°Ô∏è style guide",
    "section": "8.2 .dropDuplicates() and .distinct() to ‚Äúclean‚Äù joins",
    "text": "8.2 .dropDuplicates() and .distinct() to ‚Äúclean‚Äù joins\nDon‚Äôt think about using .dropDuplicates() or .distinct() as a quick fix for data duplication after a join. If unexpected duplicate rows are in your dataframe, there is always an underlying reason for why those duplicate rows appear. Adding .dropDuplicates() only masks this problem and adds unneccesary cpu cycles."
  },
  {
    "objectID": "posts/01_pyspark_style/pyspark_style.html#dealing-with-nulls",
    "href": "posts/01_pyspark_style/pyspark_style.html#dealing-with-nulls",
    "title": "Pyspark ‚ö°Ô∏è style guide",
    "section": "9.1 Dealing with nulls",
    "text": "9.1 Dealing with nulls\nWhile nulls are ignored for aggregate functions (like F.sum() and F.max()), they will impact the result of analytic functions (sucha as F.first()/F.last() and F.rank()).\ndf_nulls = spark.createDataFrame(\n    [(\"a\", None), (\"a\", 2), (\"a\", 1), (\"a\", 3), (\"a\", None)],\n    [\"key\", \"num\"],\n)\n\n\ndf_nulls.select(\"key\", \"num\", F.first(\"num\").over(w4).alias(\"first\")).show()\n# |key| num|first|\n# +---+----+-----+\n# |  a|NULL| NULL|\n# |  a|NULL| NULL|\n# |  a|   1| NULL|\n# |  a|   2| NULL|\n# |  a|   3| NULL|\n# +---+----+-----+\nDepending on the usecase this might be the desired behavior, but it might not always be applicabler. Best to avoid this situation by excplicitly setting the sort order and the nulls behavior.\n# sorting ascending and nulls last\nw5 = (\n    W.partitionBy(\"key\")\n    .orderBy(F.asc_nulls_last(\"num\"))  \n    .rowsBetween(\n        start=W.unboundedPreceding,\n        end=W.unboundedFollowing,\n    )\n)\n\ndf_nulls.select(\"key\", \"num\", F.first(\"num\").over(w5).alias(\"first\")).show()\n# +---+----+-----+\n# |key| num|first|\n# +---+----+-----+\n# |  a|   1|    1|\n# |  a|   2|    1|\n# |  a|   3|    1|\n# |  a|NULL|    1|\n# |  a|NULL|    1|\n# +---+----+-----+"
  },
  {
    "objectID": "posts/01_pyspark_style/pyspark_style.html#empty-partitionby",
    "href": "posts/01_pyspark_style/pyspark_style.html#empty-partitionby",
    "title": "Pyspark ‚ö°Ô∏è style guide",
    "section": "9.2 Empty partitionBy()",
    "text": "9.2 Empty partitionBy()\nSpark window functions can be applied over all rows, using a global frame. This is accomplished by specifying zero columns in the partition by expression (i.e.¬†W.partitionBy()). Code like this should be avoided, however, as it forces Spark to combine all data into a single partition, which can be extremely harmful for performance. Prefer to use aggregations whenever possible:\n# bad\nw = W.partitionBy()\ndf.select(F.sum('num').over(w).alias('sum'))\n# WARN WindowExec: No Partition Defined for Window operation! \n# Moving all data to a single partition, this can cause serious performance degradation\n\n\n# good\ndf.agg(F.sum(\"num\").alias(\"sum\"))"
  },
  {
    "objectID": "posts/DPD/dependency.html",
    "href": "posts/DPD/dependency.html",
    "title": "Introduction to Partial Dependence Plots",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nfrom sklearn.datasets import make_friedman1\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import PartialDependenceDisplay, partial_dependence\nCode\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nCode\n# setting global plotting settings\n# roudning all floats to two digits\npd.options.display.float_format = \"{:.2f}\".format\n\nset_matplotlib_formats(\"svg\")\nsns.set_context(context=\"notebook\", font_scale=1.5)\nsns.set_palette(\"tab10\")\nsns.set_style(\"darkgrid\")\nFIGSIZE = (12, 6)\nRANDOM_STATE = 35"
  },
  {
    "objectID": "posts/DPD/dependency.html#introduction",
    "href": "posts/DPD/dependency.html#introduction",
    "title": "Introduction to Partial Dependence Plots",
    "section": "1 Introduction",
    "text": "1 Introduction\nLet‚Äôs dive into the world of partial dependence plots (PDPs), powerful visual tools that reveal the average impact a specific feature has on the predictions made by a model."
  },
  {
    "objectID": "posts/DPD/dependency.html#why-explore-partial-dependence-plots",
    "href": "posts/DPD/dependency.html#why-explore-partial-dependence-plots",
    "title": "Introduction to Partial Dependence Plots",
    "section": "2 Why Explore Partial Dependence Plots?",
    "text": "2 Why Explore Partial Dependence Plots?\nVisualizing how different features influence model predictions can be crucial for understanding, interpreting, and trusting the decisions made by machine learning models. PDPs help us gain insights into the relationship between the features and the predicted outcomes.\nTo illustrate the concept of PDPs, we will use a synthetic dataset from Scikit-learn, available here.\nThe synthetic dataset is created based on the following formula: \\(y(X) = 10\\sin(\\pi \\cdot X_0 \\cdot X_1) + 20 \\cdot (X_2 - 0.5)^2 + 10 \\cdot X_3 + 5 \\cdot X_4 + \\text{noise} \\cdot N(0, 1)\\).\nFor our demonstration, I will generate a dataset with 7 features. However, it‚Äôs important to note that only 5 of these features actually affect the output‚Äîthis means the other two features do not have any predictive power. The dataset will contain 2000 samples and include a noise level set to 2.\n\n\nCode\nX_reg, y_reg = make_friedman1(\n    n_samples=2_000, n_features=7, noise=2, random_state=RANDOM_STATE\n)\n\n# stick it into a dataframe\ndf_reg = pd.concat(\n    [\n        pd.DataFrame(\n            data=X_reg, columns=[\"x_0\", \"x_1\", \"x_2\", \"x_3\", \"x_4\", \"x_5\", \"x_6\"]\n        ),\n        pd.DataFrame(data=y_reg, columns=[\"target\"]),\n    ],\n    axis=1,\n)\n\n# display descriptive stats\ndf_reg.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nx_0\n2000.00\n0.49\n0.29\n0.00\n0.25\n0.49\n0.74\n1.00\n\n\nx_1\n2000.00\n0.50\n0.29\n0.00\n0.25\n0.51\n0.76\n1.00\n\n\nx_2\n2000.00\n0.50\n0.29\n0.00\n0.25\n0.52\n0.75\n1.00\n\n\nx_3\n2000.00\n0.50\n0.28\n0.00\n0.26\n0.51\n0.75\n1.00\n\n\nx_4\n2000.00\n0.50\n0.29\n0.00\n0.25\n0.50\n0.75\n1.00\n\n\nx_5\n2000.00\n0.50\n0.29\n0.00\n0.24\n0.49\n0.75\n1.00\n\n\nx_6\n2000.00\n0.50\n0.28\n0.00\n0.26\n0.51\n0.75\n1.00\n\n\ntarget\n2000.00\n14.48\n5.26\n-1.56\n10.72\n14.46\n18.15\n30.51\n\n\n\n\n\n\n\nThe dataset‚Äôs input features are uniformly distributed, \\(U(0,1)\\), which typically means their average value hovers around 0.5. In contrast, our target feature stands out with a much higher average value of approximately 14.\nFor our analysis, we‚Äôll be using a random forest model. It‚Äôs my go-to favorite for several reasons: no need for feature scaling, robustness to missing values, and its proficiency in capturing non-linear relationships. Plus, it has the added advantage of relatively quick fitting times.\n\n\nCode\nX = df_reg.drop(columns=\"target\")\ny = df_reg[\"target\"]\n\nreg = RandomForestRegressor(\n    n_estimators=32,\n    max_depth=9,\n    min_samples_split=2,\n    random_state=42,\n)\n_ = reg.fit(X, y)\n\n\nBefore examining the partial dependence of each feature on the target, let‚Äôs first handle some data preparation. A practical approach is to begin with a small random sample of our data. After working through an end-to-end example using this subset, we can consider applying the same method to the larger dataset.\n\n\nCode\ndf_sample = df_reg.sample(3, random_state=1)\ndf_sample\n\n\n\n\n\n\n\n\n\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\ntarget\n\n\n\n\n674\n0.36\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n\n\n1699\n0.32\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n\n\n1282\n0.31\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57"
  },
  {
    "objectID": "posts/DPD/dependency.html#step-by-step-guide-to-calculating-a-partial-dependence-plot-pdp",
    "href": "posts/DPD/dependency.html#step-by-step-guide-to-calculating-a-partial-dependence-plot-pdp",
    "title": "Introduction to Partial Dependence Plots",
    "section": "3 Step-by-Step Guide to Calculating a Partial Dependence Plot (PDP)",
    "text": "3 Step-by-Step Guide to Calculating a Partial Dependence Plot (PDP)\nTo determine the partial dependence (PD) of a feature, such as x_0, on the predicted outcome, we follow a structured process:\n\nIdentify the Range of the Feature: Ascertain the range within which the feature x_0 varies. For most datasets, this would be the minimum and maximum values that x_0 can take.\nDetermine the Grid Size: Decide on a grid size, which essentially is the number of points or the sampling resolution across the feature‚Äôs range.\nExpand the Dataset Using the Grid: Implement the grid across the dataset. This step significantly increases the size of the dataset because for each original data point, we now have multiple points covering the entire grid.\nGenerate Predictions: Use the model to predict outcomes on this augmented dataset. These predictions are solely for inference, which generally has a lower computational cost.\nCompute the Average: Take the mean of the predictions corresponding to each value on the grid. This average is what gives us the partial dependence value at each point.\n\nThrough this method, we can illustrate how changes in x_0 influence the model‚Äôs predictions, regardless of the values of other features. This visual representation helps in understanding the behavior and decision-making process of complex models.\n\n3.1 Identify the Range of the Feature\n\n\nCode\ndf_reg[\"x_0\"].agg([\"min\", \"max\"])\n\n\nmin   0.00\nmax   1.00\nName: x_0, dtype: float64\n\n\nWhen calculating the partial dependence of x_0, which ranges from zero to one, you have the option to consider the entire range of values x_0 can take or focus on a specific region, such as the 90th percentile. The latter approach eliminates potential outliers that may skew the PD calculation.\n\n\n3.2 Determine the Grid Size\nFor this particular analysis, we‚Äôre opting for a modest grid size of 7. This means we‚Äôll create a sequence of 7 points that are evenly spaced across the feature x_0 range of 0 to 1.\n\n\n3.3 Expand the Dataset Using the Grid\nUsing the np.linspace() function in Python, we can easily generate our grid. This function creates an array of evenly spaced values over a specified interval. Once we have our grid, we can augment our dataset through a cross join operation. For those familiar with SQL, it‚Äôs common knowledge that cross joins can rapidly increase the size of your data. With 3 original data points and a grid size of 7, we increase our data count to ($ 3 = 21$ ). In this expanded dataset, the new column x_0_sample represents the values for x_0 across which we want to compute the partial dependence.\n\n\nCode\ndf_pdp = df_sample.drop(columns=\"x_0\")\ndf_sample_grid = pd.Series(np.linspace(0, 1, 7), name=\"x_0_sample\").round(2)\ndf_pdp = df_pdp.join(other=df_sample_grid, how=\"cross\")\ndf_pdp\n\n\n\n\n\n\n\n\n\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\ntarget\nx_0_sample\n\n\n\n\n0\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.00\n\n\n1\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.17\n\n\n2\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.33\n\n\n3\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.50\n\n\n4\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.67\n\n\n5\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.83\n\n\n6\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n1.00\n\n\n7\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.00\n\n\n8\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.17\n\n\n9\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.33\n\n\n10\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.50\n\n\n11\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.67\n\n\n12\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.83\n\n\n13\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n1.00\n\n\n14\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.00\n\n\n15\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.17\n\n\n16\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.33\n\n\n17\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.50\n\n\n18\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.67\n\n\n19\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.83\n\n\n20\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n1.00\n\n\n\n\n\n\n\n\n\n3.4 Generate Predictions\nWith our expanded dataset, now consisting of 21 observations, we move forward by utilizing our model to generate predictions for each observation. In essence, we ask the original model to provide predictions for every new data point that we‚Äôve added.\n\n\nCode\ndf_pdp = df_pdp.assign(\n    y_pred=reg.predict(\n        df_pdp.loc[\n            :, [\"x_0_sample\", \"x_1\", \"x_2\", \"x_3\", \"x_4\", \"x_5\", \"x_6\"]\n        ].to_numpy()\n    )\n)\ndf_pdp\n\n\n\n\n\n\n\n\n\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\ntarget\nx_0_sample\ny_pred\n\n\n\n\n0\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.00\n5.23\n\n\n1\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.17\n9.61\n\n\n2\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.33\n12.19\n\n\n3\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.50\n12.58\n\n\n4\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.67\n12.72\n\n\n5\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.83\n12.66\n\n\n6\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n1.00\n12.79\n\n\n7\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.00\n5.76\n\n\n8\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.17\n9.31\n\n\n9\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.33\n9.89\n\n\n10\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.50\n11.27\n\n\n11\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.67\n11.67\n\n\n12\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.83\n11.60\n\n\n13\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n1.00\n11.86\n\n\n14\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.00\n6.65\n\n\n15\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.17\n6.97\n\n\n16\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.33\n7.73\n\n\n17\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.50\n9.19\n\n\n18\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.67\n9.91\n\n\n19\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.83\n10.83\n\n\n20\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n1.00\n10.82\n\n\n\n\n\n\n\n\n\n3.5 Compute the Average\nAfter obtaining predictions, we aggregate results by selecting x_0_sample and y_pred, grouping by x_0_sample, and applying a mean aggregation function to reveal the average impact of changes in x_0 on model predictions.\n\n\nCode\ndf_pdp_plot = (\n    df_pdp[[\"x_0_sample\", \"y_pred\"]].groupby(\"x_0_sample\", as_index=False).mean()\n)\ndf_pdp_plot\n\n\n\n\n\n\n\n\n\nx_0_sample\ny_pred\n\n\n\n\n0\n0.00\n5.88\n\n\n1\n0.17\n8.63\n\n\n2\n0.33\n9.94\n\n\n3\n0.50\n11.01\n\n\n4\n0.67\n11.43\n\n\n5\n0.83\n11.70\n\n\n6\n1.00\n11.82\n\n\n\n\n\n\n\nThe dataframe shows the average impact of feature x_0 on the target variable, with an observable increase in the target variable as x_0 values rise, as depicted below.\n\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=FIGSIZE)\nax.plot(df_pdp_plot[\"x_0_sample\"], df_pdp_plot[\"y_pred\"])\nax.set_xlabel(\"$x_0$\")\nax.set_ylabel(\"target\")\nax.set_title(\"partial dependence of $x_0$ on the target variable\")\nplt.show()"
  },
  {
    "objectID": "posts/DPD/dependency.html#interpreting-the-partial-dependence-plot",
    "href": "posts/DPD/dependency.html#interpreting-the-partial-dependence-plot",
    "title": "Introduction to Partial Dependence Plots",
    "section": "4 Interpreting the Partial Dependence Plot",
    "text": "4 Interpreting the Partial Dependence Plot\nThe plot shows that the model responds positively to increases in x_0. However, this analysis is based on only three observations. To gain a more precise understanding of the relationship between x_0 and the target variable, consider utilizing the full dataset of 2000 samples along with a finer grid of 128 points. This expansion results in \\(2000 * 128 = 256,000\\) data points, illustrating the significant data growth when examining partial dependence for just one feature.\nBy using more data and a refined grid, we can generate a more accurate representation of the x_0 variable‚Äôs relationship with the target variable."
  },
  {
    "objectID": "posts/DPD/dependency.html#implementation-in-code",
    "href": "posts/DPD/dependency.html#implementation-in-code",
    "title": "Introduction to Partial Dependence Plots",
    "section": "5 Implementation in Code",
    "text": "5 Implementation in Code\nBelow is the code snippet that will generate this comprehensive dataset. Note the comments indicating where alterations are made to accommodate the full dataset and the finer grid size.\n\n\nCode\ndf_pdp = df_reg.drop(columns=\"x_0\")  # &lt;- the full dataset\ndf_sample_grid = pd.Series(\n    np.linspace(0, 1, 128),  # &lt;- sampling is 128\n    name=\"x_0_sample\",\n)\ndf_pdp = df_pdp.join(other=df_sample_grid, how=\"cross\")\n\ndf_pdp = df_pdp.assign(\n    y_pred=reg.predict(\n        df_pdp.loc[\n            :, [\"x_0_sample\", \"x_1\", \"x_2\", \"x_3\", \"x_4\", \"x_5\", \"x_6\"]\n        ].to_numpy()\n    )\n)\n\ndf_pdp_plot_full = (\n    df_pdp[[\"x_0_sample\", \"y_pred\"]].groupby(\"x_0_sample\", as_index=False).mean()\n)\n\n_, ax = plt.subplots(1, 1, figsize=FIGSIZE)\nax.plot(df_pdp_plot_full[\"x_0_sample\"], df_pdp_plot_full[\"y_pred\"])\nax.set_xlabel(\"$x_0$\")\nax.set_ylabel(\"target\")\nax.set_title(\"partial dependence of $x_0$ on the target variable\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAfter utilizing the full dataset of 256,000 samples and applying a more refined grid of 127 points, the resulting plot confirms our earlier findings that as the value of x_0 increases, so does the target variable, and this relationship appears to be almost monotonic.\nWith the increased data resolution, we now have a more granular view confirming and clarifying the positive correlation between x_0 and the target variable. This level of detail is invaluable for model interpretation and can guide decisions on feature importance and model adjustments. It underscores the capability of partial dependence plots to provide deeper insights into the model‚Äôs behavior and help pinpoint how specific features drive the predictions.\n\n5.1 Utilizing Existing Libraries for Partial Dependence Plots\nWhen it comes to practical applications in data science, it‚Äôs often more efficient to leverage existing tools and libraries rather than creating custom code from scratch. This is particularly true for partial dependence plots (PDPs), where coding our own routine can be complex and time-consuming.\nScikit-learn, a widely-used library in the machine learning community, offers a convenient and robust implementation of partial dependence with its partial_dependence function. This can save a substantial amount of effort and helps ensure that our calculations are accurate and aligned with best practices.\nAdditionally, for those who are already utilizing the SHAP library to compute Shapley values‚Äîwhich provide insight into the contribution of each feature to the prediction‚Äîthere‚Äôs good news. SHAP also includes functionality for generating partial dependence plots with its shap.plots.partial_dependence feature. This is extremely beneficial as it integrates seamlessly with other SHAP-based interpretability techniques, offering a consistent and comprehensive suite of tools for model explanation.\nThese existing implementations not only streamline the process but also ensure that you‚Äôre taking advantage of the latest methodologies and optimizations in the field. Whether you‚Äôre conducting exploratory data analysis or presenting findings to stakeholders, these tools can greatly enhance the efficiency and clarity of your model interpretation efforts.\n\n\n5.2 one-way partial dependence\nThe beauty of using scikit-learn‚Äôs built-in functions lies in their simplicity and power. Generating partial dependence plots for your entire dataset can indeed be as straightforward as a single line of code. Scikit-learn takes care of the heavy lifting involved in computing the partial dependence and plotting it. This function will automatically handle the grid creation, predictions, and averaging needed to generate the PDP. The resulting plot is a powerful visual tool that can instantly convey how changes in the specified feature(s) affect the model‚Äôs output, facilitating a better understanding of the model‚Äôs behavior.\nThis ease of use, combined with the robustness of scikit-learn, makes it a go-to choice for practitioners looking to interpret their machine learning models without getting bogged down in the details of implementation. üòä\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=(12, 16))\n\nPartialDependenceDisplay.from_estimator(\n    estimator=reg, X=X, features=range(0, 7), grid_resolution=128, kind=\"average\", ax=ax\n)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5.3 Insights from the Partial Dependence Plot Analysis\nThe partial dependence plots provide a wealth of insights into how different features impact the model‚Äôs predictions. Based on the plot, we can make several observations:\n\nFeatures x_6 and x_5: These features appear to have no meaningful impact on the model‚Äôs output. Variations in x_6 or x_5 do not significantly alter the predicted target value. This aligns with our earlier understanding that these features represent random noise, thus confirming their lack of predictive power.\nFeature x_2: The partial dependence plot for x_2 resembles the shape of a bathtub, indicating that the extremes of its value range lead to higher predictions for the target variable, while a mean value of 0.5 correlates with the lowest target outcome. This suggests a non-linear relationship where both low and high values of x_2 increase the target value, with a trough in the middle.\nFeatures x_0 and x_1: These features exhibit a steady monotonic increase in their impact on the target variable, which begins to plateau at higher input values. This plateau suggests that beyond a certain point, further increases in x_0 or x_1 have a diminished effect on the prediction.\nRemaining Features x_3 and x_4: The plots for x_3 and x_4 indicate that higher values of these features generally lead to higher predicted outcomes for the target variable. Notably, there‚Äôs a significant jump in the target prediction at around 0.4 for x_3 and 0.6 for x_4, highlighting specific values where there might be a threshold effect or a strong non-linear influence.\n\nThese insights from the partial dependence plots can guide further data analysis and model refinement. By understanding how each feature affects the target, we can prioritize features for engineering, adjust the model to better capture non-linear relationships, and potentially simplify the model by removing non-contributory noise features.\n\n\n5.4 Exploring Feature Interactions with Two-Way Partial Dependence Plots\nPartial dependence plots aren‚Äôt limited to individual features; they can also be extended to explore the interactions between two features and their joint effect on the target variable. This two-dimensional analysis, known as a two-way partial dependence plot, provides a deeper understanding of how feature pairs contribute to the model‚Äôs predictions. Here‚Äôs how you would approach creating a two-way PDP:\n\nSelect Two Features: Choose the two features you want to analyze for interaction.\nCreate a Sample Grid: Construct a grid that is the Cartesian product of the individual grids for the two selected features. This means if you have 7 points for x_0 and 7 for x_1, your sample grid will have 49 points representing all combinations of x_0 and x_1.\nExpand the Dataset: Just as with the one-way PDP, you now apply this grid to the dataset. Each original datapoint is replaced by the set of new points from the grid, dramatically increasing the dataset size.\nGenerate Predictions: Use the model to predict the target variable for this comprehensive grid, taking into account every combination of the two features.\nCompute the Average: By averaging the predictions across the grid for each combination, you can visualize the effect of varying both features simultaneously.\n\nThe result is a contour plot or a three-dimensional surface plot that shows how changes in the two features together impact the predicted outcome. This gives insight into whether the features act independently on the target or if there are interaction effects‚Äîsituations where the influence of one feature on the target depends on the value of the other feature.\nTwo-way partial dependence plots can help identify synergies or redundancies between features, informing decisions about feature engineering or model selection to better capture complex relationships within the data.\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n\nPartialDependenceDisplay.from_estimator(\n    reg, X, features=[(1, 2)], grid_resolution=128, ax=ax\n)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/DPD/dependency.html#bonus-individual-conditional-expectation",
    "href": "posts/DPD/dependency.html#bonus-individual-conditional-expectation",
    "title": "Introduction to Partial Dependence Plots",
    "section": "6 Bonus: Individual Conditional Expectation üßä",
    "text": "6 Bonus: Individual Conditional Expectation üßä\nIndividual Conditional Expectation (ICE) plots diverge from the average effect shown in PDPs by tracing the response of each individual observation to changes in a feature. Here‚Äôs the distilled essence of ICE plots:\n\n6.1 Focused on the Individual\nEach line in an ICE plot represents the prediction path for a single observation as the feature of interest varies. This allows us to inspect how specific instances react to feature changes individually, rather than in aggregate.\n\n\n6.2 Visual Diagnosis of Variability\nICE plots can reveal if different observations respond inconsistently to a feature change, showing increases for some and decreases for others. This can pinpoint performance issues not visible in PDPs.\n\n\n6.3 Enhanced Model Understanding\nBy identifying how subsets of data are influenced by feature changes, ICE plots offer detailed insights into model behavior, helping to refine predictions and address potential issues at the individual level.\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=(12, 9))\n\nidx = np.random.randint(len(X), size=int(len(X) * 0.1), dtype=int)\n\n\nPartialDependenceDisplay.from_estimator(\n    reg, X.loc[idx, :], features=[3], kind=\"both\", grid_resolution=128, ax=ax\n)\nax.set_title(\"Individual Condition Expectation\")\nplt.show()"
  },
  {
    "objectID": "writing_tools/corrections.html",
    "href": "writing_tools/corrections.html",
    "title": "hyper-shotgun",
    "section": "",
    "text": "Code\nimport json\n\nimport ollama\nfrom tqdm.notebook import tqdm\n\n\n\n\nCode\nwith open(\"../posts/cluster/cluster_reflect.ipynb\") as file:\n    notebook = json.load(file)\n\n\n\n\nCode\ninstruction = \"Your job: \\\n- Improve grammar and language \\\n- fix errors \\\n- cut the clutter \\\n- but keep tone and voice \\\n- don't change markdown syntax, \\\n- rephrase for easier reading \\\n- limit the use of emoji \\\n- never cut jokes \\\n- only return the corrected text, no summaries of changes or list of improvements\"\n\n\n\n\nCode\nfor cell in tqdm(notebook[\"cells\"]):\n    # print(cell[\"cell_type\"])\n    if cell[\"cell_type\"] == \"markdown\":\n        print(\"-\" * 6)\n        print(cell[\"source\"])\n\n        response = ollama.generate(\n            model=\"phi\",\n            system=instruction,\n            prompt=\" \".join(cell[\"source\"]),\n            keep_alive=\"5m\",\n        )\n\n        cell[\"source\"].append(\"--- \\n \" + response[\"response\"])\n\n\n\n\nCode\nwith open(\"../posts/cluster/cluster_reflect.ipynb\", mode=\"w\") as output_file:\n    json.dump(notebook, output_file)"
  }
]