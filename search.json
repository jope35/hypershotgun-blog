[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hyper-shotgun",
    "section": "",
    "text": "pooking and looking üëÄ\n\n\ntogether\n\n\n \n\n\n\n\n\n2024-03-14\n\n\nJoost de Theije + LLM\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nCluster\n\n\ntogether\n\n\n \n\n\n\n\n\n2024-01-17\n\n\nJoost de Theije + LLM\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nBoring linear forecast\n\n\nimproving performance by adding some dummies\n\n\n \n\n\n\n\n\n2023-03-5\n\n\nJoost de Theije\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html",
    "href": "posts/boring_forecast/boring_linear_forecast.html",
    "title": "Boring linear forecast",
    "section": "",
    "text": "Linear regression in itself is not performant for longer time-scales\nMost systems have some capabilities for linear regression built-in\nAdding dummy variables for datetime features(i.e.¬†month, weekday etc.) adds predictive power"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#tldr",
    "href": "posts/boring_forecast/boring_linear_forecast.html#tldr",
    "title": "Boring linear forecast",
    "section": "",
    "text": "Linear regression in itself is not performant for longer time-scales\nMost systems have some capabilities for linear regression built-in\nAdding dummy variables for datetime features(i.e.¬†month, weekday etc.) adds predictive power"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#introduction",
    "href": "posts/boring_forecast/boring_linear_forecast.html#introduction",
    "title": "Boring linear forecast",
    "section": "2 Introduction",
    "text": "2 Introduction\nLinear regression is a statistical model that can be used to determine the linear relationship between variables, most of the time this is seen as a beginners model that is not particularly useful, and most of the time it is discarded and replaced by a shiny neural net or a fancy gradient-boosted model.\nthe nice feature of linear regression is that the direction and magnitude of the relationship can be estimated with the help of linear regression. It is used in many fields including but not limited to Economics, Finance, Social science, etc. The popularity of this model is model is likely due to the fact that most systems have built-in functionality that enables the training of linear models, they are also very cheap/fast to train, and one can also determine that the model has optimal parameters. all these features make linear regression an excellent model to start with.\nTo extend the capabilities of the linear model for time series forecasting, dummy variables can be utilized. These dummies can provide additional information about the relationship over time, can help the model to identify seasonality over time, and also gauge the effect of one-off events, examples are price reductions or natural disasters.\nFor instance, we can create a dummy to identify certain datetime features such as what the month is or whether a particular day is a weekday or a weekend."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#imports",
    "href": "posts/boring_forecast/boring_linear_forecast.html#imports",
    "title": "Boring linear forecast",
    "section": "3 Imports",
    "text": "3 Imports\nFirst we import all the libraries, the default data science libs and the linear model and metrics from sklearn.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#reading-in-the-data",
    "href": "posts/boring_forecast/boring_linear_forecast.html#reading-in-the-data",
    "title": "Boring linear forecast",
    "section": "4 Reading in the data",
    "text": "4 Reading in the data\nFor this example, we will be using a dataset form the prophet package. I have selected this one because, in the prophet documentation, this dataset is used in the section ‚ÄúSeasonality, Holiday Effects, And Regressors‚Äù so it seems fitting to use it, to demonstrate the usefulness of seasonal dummies. The prophet docs describes the dataset in the following way:\n\nAs an example, let‚Äôs look at a time series of the log daily page views for the Wikipedia page for Peyton Manning. We scraped this data using the Wikipedia trend package in R. Peyton Manning provides a nice example because it illustrates some of Prophet‚Äôs features, like multiple seasonality, changing growth rates, and the ability to model special days (such as Manning‚Äôs playoff and Superbowl appearances).\n\n\n\nCode\ndf_in = pd.read_csv(\n    \"https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv\"\n)\ndf_in = df_in.assign(ds=pd.to_datetime(df_in[\"ds\"]))\ndf_in = df_in[(df_in[\"ds\"] &gt; \"2012\")]  # selecting data after 2012\n\n\nIt is always nice to see what we are working with so let us plot the data over time to see what we visually can extract from the plot. we are looking for patterns over time and other behavior and/or pattern that we can exploit.\n\n\nCode\nplt.plot_date(\n    x=df_in[\"ds\"],\n    y=df_in[\"y\"],\n    label=\"input timeseries\",\n    fmt=\"-\",\n)\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"target variable - $y$\")\nplt.title(\"daily visits ot Peyton Manning wiki on a daily basis (log)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nI have selected data from 2012 onwards, this should be enough to test drive the seasonal dummies. The first thing that I notice is that there is a dip around months 06 and 07. In early August teams play so exhibition games before the start of the actual season.\nAlso, we can observe a pattern over the year, it starts high then dips, and then and high again. This can be seen for the other years as well, so there is some repeating seasonality. Let us continue and train our first models. Starting with a simple ordinary linear regression and then adding dummies to see if they improve the performance of the model."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#train-test-split",
    "href": "posts/boring_forecast/boring_linear_forecast.html#train-test-split",
    "title": "Boring linear forecast",
    "section": "5 Train-test split",
    "text": "5 Train-test split\n\n\nCode\n# train test split\ndf_train = df_in[(df_in[\"ds\"] &gt; \"2012\") & (df_in[\"ds\"] &lt; \"2015\")]\ndf_test = df_in[(df_in[\"ds\"] &gt; \"2015\")]\n\n\nTo gauge the performance of the model the data is split in two parts, the train data from 2012 up to 2015 and the test data everything after 2015. The model will only see the train data and is asked to create a prediction for the test data, after which we will calculate the perfomance with the predictions and the true observations.\n\n\nCode\n# visually inspect the train test split\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\"data is splitted, everything before 2015 is train data after 2015 test\")\nplt.show()"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#setting-up-the-regression",
    "href": "posts/boring_forecast/boring_linear_forecast.html#setting-up-the-regression",
    "title": "Boring linear forecast",
    "section": "6 Setting up the regression",
    "text": "6 Setting up the regression\n\n\nCode\nX_train = df_train[\"ds\"].astype(int).to_numpy().reshape(-1, 1)\ny_train = df_train[\"y\"].to_numpy()\n\nX_test = df_test[\"ds\"].astype(int).to_numpy().reshape(-1, 1)\ny_test = df_test[\"y\"].to_numpy()\n\n\nShaping and converting the data so that we can fit the linear model. In this case, we are converting the date columns into an ever-increasing integer.\n\n\nCode\n# creating, fit, and inference\nlinear = LinearRegression()\nlinear.fit(X=X_train, y=y_train)\ny_pred = linear.predict(X=X_test)\n\n\nFitting the linear model is very fast, around 4 milliseconds on my machine, and yes, I know the data volume is low(actually for time series it is pretty decent), but this allows me to fit 1000 models in 4 seconds. So, we can make predictions on 1000 different time series, all in the time it takes to take a sip of coffee‚òïÔ∏è.\nAnyways lets us visually inspect the results of the model, by plotting all the components (train, test, predictions) that we have gathered. Also, let us calculate two error metrics mse and mae that way we can quantify the performance of the model.\n\n\nCode\n# calc error metrics\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\n\n# visually inspect the prediction\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred,\n    label=\"prediction\",\n    fmt=\"--\",\n)\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(f\"linear regression applied (MSE= {mse:.3}, MAE={mae:.3})\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe model is just a boring straight line, but what did you expect with a linear model ?! The green line is passing through the orange blob more or less, and intuitively it kind of moves in the right direction. However, it doesn‚Äôt capture the seasonality or other patterns of the train set. The two error metrics mse and mae are both around 0.6. this in itself does not say much, but we can use this as the ‚Äòbenchmark‚Äô and try to improve on this with the addition of the dummies.\n\n\nmean squared error  = 0.617\nmean absolute error = 0.615"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#adding-dummies",
    "href": "posts/boring_forecast/boring_linear_forecast.html#adding-dummies",
    "title": "Boring linear forecast",
    "section": "7 Adding dummies",
    "text": "7 Adding dummies\nLet us add some dummies for the months and see if we can improve the performance of the model visually and on the metrics. For each month we create a column that can either have the value of 0 or 1. if the value is 1 then that row corresponds to that particular month, in this way we are actively encoding this knowledge into the model. in this way the model can learn the magnitude and direction of each month and apply that to future predictions. at least that is the idea, let us test that!\n\n\nCode\n# creating dummies for the months\ndf_dummies = df_in.assign(\n    month=df_in[\"ds\"].dt.month.astype(\"category\"),\n    ds_int=df_in[\"ds\"].astype(int),\n)\n\nnot_dummy = {\"y\", \"ds\", \"ds_int\"}\nto_dummy = list(set(df_dummies.columns) - not_dummy)\n\ndf_dummies = pd.get_dummies(data=df_dummies, columns=to_dummy)\nall_features = list(set(df_dummies.columns) - {\"y\", \"ds\"})\n\n# slicing the input in train test\ndf_train_dummies = df_dummies[(df_dummies[\"ds\"] &gt; \"2012\") & (df_dummies[\"ds\"] &lt; \"2015\")]\ndf_test_dummies = df_dummies[(df_dummies[\"ds\"] &gt; \"2015\")]\n\nX_train = df_train_dummies.loc[:, all_features]\ny_train = df_train_dummies[[\"y\"]]\n\nX_test = df_test_dummies.loc[:, all_features]\ny_test = df_test_dummies[[\"y\"]]\n\ndf_dummies.drop(columns=[\"ds_int\", \"y\"]).sample(\n    n=6,\n    ignore_index=True,\n    random_state=42,\n)\n\n\n\n\n\n\n\n\n\n\nds\nmonth_1\nmonth_2\nmonth_3\nmonth_4\nmonth_5\nmonth_6\nmonth_7\nmonth_8\nmonth_9\nmonth_10\nmonth_11\nmonth_12\n\n\n\n\n0\n2014-05-06\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n2012-07-05\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n2015-06-21\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n2012-03-09\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n2012-08-10\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n5\n2013-05-11\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\nIn the code block above we are adding the dummies for the month to the data. In this case month_1 is jan and month_12 is dec.\nNext up we are training the model with the added dummies and plotting the results next to the model without the dummies.\n\n\nCode\n# create the pipeline and fit pipeline\n# scaler is there so that the coefs can be interpeted later\n# pipeline = make_pipeline(StandardScaler(), LinearRegression())\npipeline = make_pipeline(MinMaxScaler(), LinearRegression())\n\npipeline.fit(X=X_train, y=y_train)\ny_pred_dummies = pipeline.predict(X=X_test)\n\nmse_dummies = mean_squared_error(y_test, y_pred_dummies)\nmae_dummies = mean_absolute_error(y_test, y_pred_dummies)\n\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred,\n    label=\"prediction\",\n    fmt=\"--\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies,\n    label=\"prediction with dummies\",\n    fmt=\"--\",\n)\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\n    f\"linear regression with dummies applied (mse= {mse_dummies:.3}, mae={mae_dummies:.3})\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nVisually the forecast already looks a lot better. It follows the peaks and valleys of the time-series, and it captures the overall trend better. this is also reflected when we look at the error metrics and the improvement factor that we have achieved. the mean squared error improved with a factor of 1.89 and the mean absolute error improved with a factor of 1.54. That is impressive for just adding a bunch of ones and zeros.\n\n\nCode\nprint(f\"mean squared error  = {mse_dummies:.3}\")\nprint(f\"improvement factor mse month dummies -&gt; {mse/mse_dummies:.3}x\")\n\nprint(\"-\" * 79)\n\nprint(f\"mean absolute error = {mae_dummies:.3}\")\nprint(f\"improvement factor mea month dummies -&gt; {mae/mae_dummies:.3}x\")\n\n\nmean squared error  = 0.325\nimprovement factor mse month dummies -&gt; 1.9x\n-------------------------------------------------------------------------------\nmean absolute error = 0.4\nimprovement factor mea month dummies -&gt; 1.54x"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#inspecting-the-seasonality",
    "href": "posts/boring_forecast/boring_linear_forecast.html#inspecting-the-seasonality",
    "title": "Boring linear forecast",
    "section": "8 Inspecting the seasonality",
    "text": "8 Inspecting the seasonality\nNow that we have encoded the information about the seasonality in the model, this also allows us to inspect that seasonality by itself. this would give us some insight into the inner workings of the underlying time series model. first we access the coefficients of the linear model and put them into a separate dataframe. then we need to scale them so that the relative difference is more apparent. looking at the raw coefficients would not yield any information as the scale is not relatable to the original problem.\n\n\nCode\n# pull coefs into a seperate df, to inspect the seasonality\nlin_reg_coefs = (\n    pd.DataFrame(\n        data=pipeline[\"linearregression\"].coef_,\n        columns=X_train.columns,\n    )\n    .T.reset_index()\n    .rename(columns={\"index\": \"month\", 0: \"coefficient\"})\n)\n# exclude the time col\nlin_reg_coefs = lin_reg_coefs[lin_reg_coefs[\"month\"] != \"ds_int\"]\n\n# subtract mean to get the relative difference between the coefs\nlin_reg_coefs[\"coefficient\"] = (\n    lin_reg_coefs[\"coefficient\"] - lin_reg_coefs[\"coefficient\"].mean()\n)\n\n\n\n\nCode\nchart = sns.barplot(\n    data=lin_reg_coefs,\n    x=\"month\",\n    y=\"coefficient\",\n    color=sns.color_palette()[0],\n    order=[f\"month_{i}\" for i in range(1, 13)],\n)\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"\")\nplt.title(\"yearly seasonality\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNow we have a beautiful overview of the seasonality over the year. we can clearly see a dip in the middle of the year and a significant uptick in the month of January. in the first week of February, the super bowl is played which is a major factor in the traffic to the wiki page. Also, we noticed that there was a dip in the middle of the year which we can also clearly see at the month_6 mark."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#recap",
    "href": "posts/boring_forecast/boring_linear_forecast.html#recap",
    "title": "Boring linear forecast",
    "section": "9 Recap",
    "text": "9 Recap\nIn this blog post I have demonstrated that the performance of a simple linear regression for time series forecasting can be improved by a factor of 1.54 up to 1.89 by simply adding dummy variables for the months. the nice thing about this is that the linear regression is available in most systems that have some kind of analytical capability (yes even in excel) and adding the dummies is so simple that you can even do it in a SQL server, the added benefit of this all is that the fitting of the model is quick, therefore you can retrain the model monthly üòâ, weekly, daily, hourly."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#encore",
    "href": "posts/boring_forecast/boring_linear_forecast.html#encore",
    "title": "Boring linear forecast",
    "section": "10 Encore",
    "text": "10 Encore\nWhat if we were to dummies not just for the months, but also for other datetime features and really turn it up to eleven\n\nLet us create dummies for the following datetime features:\n\nmonth\nweek\ndayofweek\nis_weekend\nquarter\n\nwhen converting this to dummies it will result in a tremendous number of extra features and therefore we will apply a Elasticnet linear model. Usually, this type of model can handle lots of features better than an ordinary linear regression because of the regularization, this will be left as an exercise for the reader.\n\n\nCode\n# creating dummies for the months\ndf_dummies_all = df_in.assign(\n    month=df_in[\"ds\"].dt.month.astype(\"category\"),\n    week=df_in[\"ds\"].dt.isocalendar().week.astype(\"category\"),\n    dayofweek=df_in[\"ds\"].dt.dayofweek.astype(\"category\"),\n    is_weekend=(df_in[\"ds\"].dt.dayofweek) &gt;= 5,\n    quarter=df_in[\"ds\"].dt.quarter.astype(\"category\"),\n    ds_int=df_in[\"ds\"].astype(int),\n)\n\nnot_dummy = {\"y\", \"ds\", \"ds_int\"}\nto_dummy = list(set(df_dummies_all.columns) - not_dummy)\n\ndf_dummies_all = pd.get_dummies(\n    data=df_dummies_all,\n    columns=to_dummy,\n    drop_first=True,  # reduce the amount of cols with no additional info\n)\nall_features = list(set(df_dummies_all.columns) - {\"y\", \"ds\"})\n\n# slicing the input in train test\ndf_train_dummies_all = df_dummies_all[\n    (df_dummies_all[\"ds\"] &gt; \"2012\") & (df_dummies_all[\"ds\"] &lt; \"2015\")\n]\ndf_test_dummies_all = df_dummies_all[(df_dummies_all[\"ds\"] &gt; \"2015\")]\n\nX_train = df_train_dummies_all.loc[:, all_features]\ny_train = df_train_dummies_all[[\"y\"]]\n\nX_test = df_test_dummies_all.loc[:, all_features]\ny_test = df_test_dummies_all[[\"y\"]]\n\ndf_dummies_all.drop(columns=[\"ds_int\", \"y\"]).sample(\n    n=6,\n    ignore_index=True,\n    random_state=42,\n)\n\n\n\n\n\n\n\n\n\n\nds\nweek_2\nweek_3\nweek_4\nweek_5\nweek_6\nweek_7\nweek_8\nweek_9\nweek_10\n...\nmonth_9\nmonth_10\nmonth_11\nmonth_12\ndayofweek_1\ndayofweek_2\ndayofweek_3\ndayofweek_4\ndayofweek_5\ndayofweek_6\n\n\n\n\n0\n2014-05-06\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n2012-07-05\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n2\n2015-06-21\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\n2012-03-09\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n2012-08-10\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n5\n2013-05-11\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n6 rows √ó 74 columns\n\n\n\n\n\n\nCode\n# utilzing an elasticnet linear model to compensate for the amount of features\nelastic_params = {\n    \"l1_ratio\": np.linspace(start=0.000001, stop=1, num=100),\n    \"cv\": 7,\n    \"n_alphas\": 1_00,\n    \"n_jobs\": -1,\n}\n\npipeline_all = make_pipeline(MinMaxScaler(), ElasticNetCV(**elastic_params))\n\npipeline_all.fit(X=X_train, y=y_train.to_numpy().ravel())\ny_pred_dummies_all = pipeline_all.predict(X=X_test)\n\nmse_dummies_all = mean_squared_error(y_test, y_pred_dummies_all)\nmae_dummies_all = mean_absolute_error(y_test, y_pred_dummies_all)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n    alpha=0.7,\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies,\n    label=\"prediction with dummies\",\n    fmt=\"--\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies_all,\n    label=\"prediction with all dummies\",\n    fmt=\"--\",\n)\n\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\n    f\"linear regression with dummies applied (mse= {mse_dummies_all:.3}, mae={mae_dummies_all:.3})\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nImmediately it becomes obvious that the model captures more of the fine-grained movement of the time series. this is also reflected in the fact that both error metrics have improved.\n\n\nCode\nprint(f\"mean squared error = {mse_dummies_all:.3}\")\nprint(f\"improvement factor mse month dummies -&gt; {mse/mse_dummies:.3}x\")\nprint(f\"improvement factor mse all dummies   -&gt; {mse/mse_dummies_all:.3}x\")\nprint(\"-\" * 79)\n\nprint(f\"mean absolute error = {mae_dummies_all:.3}\")\nprint(f\"improvement factor mea month dummies -&gt; {mae/mae_dummies:.3}x\")\nprint(f\"improvement factor mea all dummies   -&gt; {mae/mae_dummies_all:.3}x\")\n\n\nmean squared error = 0.262\nimprovement factor mse month dummies -&gt; 1.9x\nimprovement factor mse all dummies   -&gt; 2.35x\n-------------------------------------------------------------------------------\nmean absolute error = 0.356\nimprovement factor mea month dummies -&gt; 1.54x\nimprovement factor mea all dummies   -&gt; 1.73x"
  },
  {
    "objectID": "posts/DPD/dependency.html",
    "href": "posts/DPD/dependency.html",
    "title": "pooking and looking üëÄ",
    "section": "",
    "text": "generate data\ntrain rf\ncreate pdp, and determine monotonic relations\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nfrom sklearn.datasets import make_friedman1\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import PartialDependenceDisplay, partial_dependence\nCode\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nCode\n# setting global plotting settings\npd.options.display.float_format = \"{:.2f}\".format\n\n# set_matplotlib_formats(\"svg\")\nsns.set_context(context=\"notebook\", font_scale=1.5)\nsns.set_palette(\"tab10\")\nsns.set_style(\"darkgrid\")\nFIGSIZE = (12, 6)\nRANDOM_STATE = 35"
  },
  {
    "objectID": "posts/DPD/dependency.html#intro",
    "href": "posts/DPD/dependency.html#intro",
    "title": "pooking and looking üëÄ",
    "section": "1 Intro",
    "text": "1 Intro\nToday, we‚Äôre examining partial dependence plots, a tool for visualizing the average influence of a feature on a model‚Äôs predictions.\n#TODO why would we want to see a partial dependece plot\nTo demonstrate this, we‚Äôll utilize a dataset from Scikit-learn, which can be found here.\nThe formula generating the synthetic dataset is as follows: \\(y(X) = 10\\sin(\\pi \\cdot X_0 \\cdot X_1) + 20 \\cdot (X_2 - 0.5)^2 + 10 \\cdot X_3 + 5 \\cdot X_4 + \\text{noise} \\cdot N(0, 1)\\).\nI will generate a dataset comprising 7 features, but only 5 will actually influence the output‚Äîmeaning the remaining two have no predictive value. the generate dataset contains 2000 samples and has a noise factor of 2.\n\n\nCode\nX_reg, y_reg = make_friedman1(\n    n_samples=2_000, n_features=7, noise=2, random_state=RANDOM_STATE\n)\n\n# stick it into a dataframe\ndf_reg = pd.concat(\n    [\n        pd.DataFrame(\n            data=X_reg, columns=[\"x_0\", \"x_1\", \"x_2\", \"x_3\", \"x_4\", \"x_5\", \"x_6\"]\n        ),\n        pd.DataFrame(data=y_reg, columns=[\"target\"]),\n    ],\n    axis=1,\n)\n\n# display descriptive stats\ndisplay(df_reg.describe().T)\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nx_0\n2000.00\n0.49\n0.29\n0.00\n0.25\n0.49\n0.74\n1.00\n\n\nx_1\n2000.00\n0.50\n0.29\n0.00\n0.25\n0.51\n0.76\n1.00\n\n\nx_2\n2000.00\n0.50\n0.29\n0.00\n0.25\n0.52\n0.75\n1.00\n\n\nx_3\n2000.00\n0.50\n0.28\n0.00\n0.26\n0.51\n0.75\n1.00\n\n\nx_4\n2000.00\n0.50\n0.29\n0.00\n0.25\n0.50\n0.75\n1.00\n\n\nx_5\n2000.00\n0.50\n0.29\n0.00\n0.24\n0.49\n0.75\n1.00\n\n\nx_6\n2000.00\n0.50\n0.28\n0.00\n0.26\n0.51\n0.75\n1.00\n\n\ntarget\n2000.00\n14.48\n5.26\n-1.56\n10.72\n14.46\n18.15\n30.51\n\n\n\n\n\n\n\n\nall input features are uniform distributed \\(U(0,1)\\) -&gt; the mean is ~0.5 the target feature has a higher mean of ~14\nThe data is fed into a random-forest model, it is my favorite go to model\n\n\nCode\nX = df_reg.drop(columns=\"target\")\ny = df_reg[\"target\"]\n\nreg = RandomForestRegressor(\n    n_estimators=32,\n    max_depth=9,\n    min_samples_split=2,\n    random_state=42,\n)\n_ = reg.fit(X, y)\n\n\nto determine the partial dependence (PD) of each feature on the target we need to do some dataset wrangling. lets start with a (very very small) subset of our data taken at random.\n\n\nCode\ndf_sample = df_reg.sample(3, random_state=1)\ndf_sample\n\n\n\n\n\n\n\n\n\n\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\ntarget\n\n\n\n\n674\n0.36\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n\n\n1699\n0.32\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n\n\n1282\n0.31\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57"
  },
  {
    "objectID": "posts/DPD/dependency.html#how-to-calculate-the-pdp",
    "href": "posts/DPD/dependency.html#how-to-calculate-the-pdp",
    "title": "pooking and looking üëÄ",
    "section": "2 how to calculate the PDP",
    "text": "2 how to calculate the PDP\nIn order to calculate the PD of lets say x_0 we would need to follow these steps: 1. determine the range of x_0 2. set a grid size, i.e the sampling ratio over the range 3. apply the grid over the data, this will increase the data size by a lot 4. predict with the model using the newly created dataset 5. average out the individual predictions to get a singular result\n\nrange of x_0 is between 0 and 1\nlets use a grid size of 7, that means that we create a list of 7 equaly space values between 0 and 1\nfor each sample in our dataset we apply the grid of 7 values which means that if we have 3 datapoints to begin we end up with 3*7=21 datapoints. that is an increase in data and for bigger data sets and finer grids this increases very fast\nnow we ask the model to make predictions, this is inference and not training so the computational cost is reasonable\nperform a group by on the sampled grid and apply a average aggregation function\nthe result is the partial dependence of x_0 and the target variable\n\n\n2.1 determine the range of x_0\n\n\nCode\ndf_reg[\"x_0\"].agg([\"min\", \"max\"])\n\n\nmin   0.00\nmax   1.00\nName: x_0, dtype: float64\n\n\nthe range of x_0 is between zero and one\n\n\n2.2 set a grid size, i.e the sampling ratio over the range\nin this case a rather small grid size of 7 is chosen\n\n\n2.3 apply the grid over the data\nby utilizing the np.linspace() function we can easily create the grid, and create the required dataset with a cross join. i told you that this could get out of hand pretty fast.\n\n\nCode\ndf_pdp = df_sample.drop(columns=\"x_0\")\ndf_sample_grid = pd.Series(np.linspace(0, 1, 7), name=\"x_0_sample\").round(2)\ndf_pdp = df_pdp.join(other=df_sample_grid, how=\"cross\")\ndf_pdp\n\n\n\n\n\n\n\n\n\n\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\ntarget\nx_0_sample\n\n\n\n\n0\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.00\n\n\n1\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.17\n\n\n2\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.33\n\n\n3\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.50\n\n\n4\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.67\n\n\n5\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.83\n\n\n6\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n1.00\n\n\n7\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.00\n\n\n8\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.17\n\n\n9\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.33\n\n\n10\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.50\n\n\n11\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.67\n\n\n12\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.83\n\n\n13\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n1.00\n\n\n14\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.00\n\n\n15\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.17\n\n\n16\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.33\n\n\n17\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.50\n\n\n18\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.67\n\n\n19\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.83\n\n\n20\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n1.00\n\n\n\n\n\n\n\n\n\n\n2.4 predict with the model\n\n\nCode\ndf_pdp = df_pdp.assign(\n    y_pred=reg.predict(\n        df_pdp.loc[\n            :, [\"x_0_sample\", \"x_1\", \"x_2\", \"x_3\", \"x_4\", \"x_5\", \"x_6\"]\n        ].to_numpy()\n    )\n)\ndf_pdp\n\n\n\n\n\n\n\n\n\n\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\ntarget\nx_0_sample\ny_pred\n\n\n\n\n0\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.00\n5.23\n\n\n1\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.17\n9.61\n\n\n2\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.33\n12.19\n\n\n3\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.50\n12.58\n\n\n4\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.67\n12.72\n\n\n5\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n0.83\n12.66\n\n\n6\n0.70\n0.83\n0.03\n0.26\n0.53\n0.61\n12.79\n1.00\n12.79\n\n\n7\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.00\n5.76\n\n\n8\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.17\n9.31\n\n\n9\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.33\n9.89\n\n\n10\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.50\n11.27\n\n\n11\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.67\n11.67\n\n\n12\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n0.83\n11.60\n\n\n13\n0.70\n0.38\n0.02\n0.38\n0.40\n0.14\n8.29\n1.00\n11.86\n\n\n14\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.00\n6.65\n\n\n15\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.17\n6.97\n\n\n16\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.33\n7.73\n\n\n17\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.50\n9.19\n\n\n18\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.67\n9.91\n\n\n19\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n0.83\n10.83\n\n\n20\n0.21\n0.66\n0.40\n0.27\n0.11\n0.84\n7.57\n1.00\n10.82\n\n\n\n\n\n\n\n\n\n\n2.5 average out the individual predictions\nthis can be done by taking the columns x_0_sample , y_pred and grouping by x_0_sample and taking the average value\n\n\nCode\ndf_pdp_plot = (\n    df_pdp[[\"x_0_sample\", \"y_pred\"]].groupby(\"x_0_sample\", as_index=False).mean()\n)\ndf_pdp_plot\n\n\n\n\n\n\n\n\n\n\nx_0_sample\ny_pred\n\n\n\n\n0\n0.00\n5.88\n\n\n1\n0.17\n8.63\n\n\n2\n0.33\n9.94\n\n\n3\n0.50\n11.01\n\n\n4\n0.67\n11.43\n\n\n5\n0.83\n11.70\n\n\n6\n1.00\n11.82\n\n\n\n\n\n\n\n\nthis is the dataframe that shows the average dependence of the target variable with respect to the feature x_0 in this case. increasing the value of x_0 also yields an increase of the target variable. as can be seen in the plot below. however keep in mind that this is done with a subset of the data and with a coarse grid. if the amount of include samples and the size of the grid is increased you can seen more detail in the dependence.\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=FIGSIZE)\nax.plot(df_pdp_plot[\"x_0_sample\"], df_pdp_plot[\"y_pred\"])\nax.set_xlabel(\"$x_0$\")\nax.set_ylabel(\"target\")\nax.set_title(\"partial dependence of $x_0$ on the target variable\")\nplt.show()\n\n\n\n\n\n\n\n\n\nthe plot above shows that the response of the model is postive with respect to an increase in the value of feature x_0 however this is done with just three datapoints,If we use the full dataset of 2000 samples and a sampling grid of 127 that would result in a final dataset of \\(2000 * 127 = 254.000\\) samples. yikes that explodes fast and this is just for one feature. however we do have the ability to get a more accurate picture of the partial dependece of the x_0 variable.\n\n\nCode\ndf_pdp = df_reg.drop(columns=\"x_0\")  # &lt;- the full dataset\ndf_sample_grid = pd.Series(\n    np.linspace(0, 1, 127),  # &lt;- sampling is 127\n    name=\"x_0_sample\",\n)\ndf_pdp = df_pdp.join(other=df_sample_grid, how=\"cross\")\n\ndf_pdp = df_pdp.assign(\n    y_pred=reg.predict(\n        df_pdp.loc[\n            :, [\"x_0_sample\", \"x_1\", \"x_2\", \"x_3\", \"x_4\", \"x_5\", \"x_6\"]\n        ].to_numpy()\n    )\n)\n\ndf_pdp_plot_full = (\n    df_pdp[[\"x_0_sample\", \"y_pred\"]].groupby(\"x_0_sample\", as_index=False).mean()\n)\n\n_, ax = plt.subplots(1, 1, figsize=FIGSIZE)\nax.plot(df_pdp_plot_full[\"x_0_sample\"], df_pdp_plot_full[\"y_pred\"])\nax.set_xlabel(\"$x_0$\")\nax.set_ylabel(\"target\")\nax.set_title(\"partial dependence of $x_0$ on the target variable\")\nplt.show()\n\n\n\n\n\n\n\n\n\nthe same picture as before emerges, an increaase in x_0 also yields an increase in the target variable to an almost monotonic level."
  },
  {
    "objectID": "posts/DPD/dependency.html#scikitlearn-implementation",
    "href": "posts/DPD/dependency.html#scikitlearn-implementation",
    "title": "pooking and looking üëÄ",
    "section": "3 scikitlearn implementation",
    "text": "3 scikitlearn implementation\nin practise we would not code our own PDP routinge but first look if there already is a lib that does the job and off course it is scikit-learn that has an implementation of a partial dependence\n\n3.1 one-way partial dependence\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=(12, 16))\n\nPartialDependenceDisplay.from_estimator(reg, X, features=range(0, 7), ax=ax)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.2 two-way partial dependence\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n\nPartialDependenceDisplay.from_estimator(reg, X, features=[(1, 2)], ax=ax)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/DPD/dependency.html#bonus-individual-condition-expectation",
    "href": "posts/DPD/dependency.html#bonus-individual-condition-expectation",
    "title": "pooking and looking üëÄ",
    "section": "4 Bonus: individual condition expectation",
    "text": "4 Bonus: individual condition expectation\ninstead of performing a aggregation and plotting the result, also the Individual observations can be used. in the plot each blue line is an original observation were the value of x_0 has taken over by the sampling grid. this allows you to inspect the individual dependecy. in this case we have a well beheaved dataset were all points are in agreement however this plot might give one some insights if performance is lacking for a couple of datapoints.\n\n\nCode\n_, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n\nPartialDependenceDisplay.from_estimator(reg, X, features=[0], kind=\"both\", ax=ax)\nax.set_title(\"Individual Condition Expectation\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npdp_results = partial_dependence(\n    estimator=reg,\n    X=X,\n    features=[\"x_3\"],\n    grid_resolution=33,\n    kind=\"both\",\n)\n\nplt.plot(\n    pdp_results[\"individual\"][0].T,\n    linestyle=\"-\",\n    linewidth=0.25,\n    alpha=0.33,\n    color=\"tab:blue\",\n)\nplt.plot(\n    np.mean(pdp_results[\"individual\"][0].T, axis=1), linestyle=\"--\", color=\"tab:orange\"\n);\n\n\n\n\n\n\n\n\n\n\n\nCode\n### gif\n\n\n\n\nCode\nimport gif\n\n\nModuleNotFoundError: No module named 'gif'\n\n\n\n\nCode\nindividual_lines = pdp_results[\"individual\"][0].T\n\n\n_, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n\n\nax.plot(\n    pdp_results[\"grid_values\"][0],\n    individual_lines[:, 0],\n    linestyle=\"-\",\n    linewidth=0.5,\n    alpha=0.5,\n    color=\"tab:blue\",\n)\n\n\nxlim_begin, ylim_begin = ax.get_xlim(), ax.get_ylim()\n\n\nax.plot(\n    pdp_results[\"grid_values\"][0],\n    individual_lines[:, :],\n    linestyle=\"-\",\n    linewidth=0.5,\n    alpha=0.5,\n    color=\"tab:blue\",\n)\n\nxlim_end, ylim_end = ax.get_xlim(), ax.get_ylim()\n\nzoom_factor = 0\n# get the global axes limits\noveral_xlim = (\n    min(xlim_begin[0], xlim_end[0]) * (1 - zoom_factor),\n    max(xlim_begin[1], xlim_end[1]) * (1 + zoom_factor),\n)\noveral_ylim = (\n    min(ylim_begin[0], ylim_end[0]) * (1 - zoom_factor),\n    max(ylim_begin[1], ylim_end[1]) * (1 + zoom_factor),\n)\n\n\n\n\nCode\n# for i in range(individual_lines.shape[1])\n\n\n@gif.frame\ndef one_frame(i: int, overal_xlim=None, overal_ylim=None) -&gt; None:\n    _, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n\n    ax.plot(\n        pdp_results[\"grid_values\"][0],\n        individual_lines[:, :i],\n        linestyle=\"-\",\n        linewidth=0.75,\n        alpha=0.35,\n        color=\"tab:blue\",\n    )\n\n    if i &gt; 1:\n        ax.plot(\n            pdp_results[\"grid_values\"][0],\n            np.mean(individual_lines[:, :i], axis=1),\n            linestyle=\"--\",\n            linewidth=3,\n            color=\"tab:orange\",\n        )\n\n    # set the overal axes\n    ax.set_xlim(overal_xlim), ax.set_ylim(overal_ylim)\n\n    # remove the ticks and lables from the axes\n    xticks = ax.get_xticks()\n    ax.set_xticks(xticks, labels=[])\n    ax.set_xlabel(\"\")\n\n    yticks = ax.get_yticks()\n    ax.set_yticks(yticks, labels=[])\n    ax.set_ylabel(\"\")\n\n    plt.tight_layout()\n\n\n\n\nCode\n# genearate all base frames\ngif_frames = [\n    one_frame(i, overal_xlim, overal_ylim) for i in range(individual_lines.shape[1])\n]\n\n\n\n\nCode\n# add bounce and freeze point\ngif_frames.extend([gif_frames[-1] for _ in range(60)])\ngif_frames.extend(gif_frames[::-1])\n\n\n\n\nCode\ngif.save(gif_frames, \"artifacts/ice_lines.gif\", duration=1)"
  },
  {
    "objectID": "posts/cluster/cluster.html",
    "href": "posts/cluster/cluster.html",
    "title": "Cluster",
    "section": "",
    "text": "Unsupervised Learning -&gt; Clustering algorithms are used for unsupervised learning, ideal for exploratory data analysis.\nGrouping Data -&gt; These algorithms group similar data into clusters based on specific criteria.\nVariety of Applications -&gt; They‚Äôre used in diverse fields like customer segmentation, anomaly detection, and more.\nDifferent Techniques -&gt; Various types exist, like K-means and DBSCAN, each with unique strengths and suited for specific data types.\nChoice of Parameters -&gt; The selection and tuning of parameters, like the number of clusters, significantly influence the results.\n\nIn the field of machine learning,clustering algorithms play a role in uncovering hidden patterns present in the data. they group together datapoints based on the simalirty of features without the need for labeled data, these groups are refered to as clusters. There are multiple algorithms that can be used to perform cluster analysis.\n\ncentroid based (K-means)\nconnectivity-based aka hierarchical clustering (Agglomerative lcustering)\ndistribution based (Gaussian-mixture modelling)\ndensity based (DBSCAN)\n\nClustering algorithms have found their place in a diverse range of real-world applications, from customer segmentation in marketing strategies to image segmentation in computer vision, and anomaly detection in cybersecurity for insightful data-driven decision making.\nTo demonstrate the workflow, I will use a K-means clustering algorithm to group together similar shoppers at a shopping mall."
  },
  {
    "objectID": "posts/cluster/cluster.html#tldr",
    "href": "posts/cluster/cluster.html#tldr",
    "title": "Cluster",
    "section": "",
    "text": "Unsupervised Learning -&gt; Clustering algorithms are used for unsupervised learning, ideal for exploratory data analysis.\nGrouping Data -&gt; These algorithms group similar data into clusters based on specific criteria.\nVariety of Applications -&gt; They‚Äôre used in diverse fields like customer segmentation, anomaly detection, and more.\nDifferent Techniques -&gt; Various types exist, like K-means and DBSCAN, each with unique strengths and suited for specific data types.\nChoice of Parameters -&gt; The selection and tuning of parameters, like the number of clusters, significantly influence the results.\n\nIn the field of machine learning,clustering algorithms play a role in uncovering hidden patterns present in the data. they group together datapoints based on the simalirty of features without the need for labeled data, these groups are refered to as clusters. There are multiple algorithms that can be used to perform cluster analysis.\n\ncentroid based (K-means)\nconnectivity-based aka hierarchical clustering (Agglomerative lcustering)\ndistribution based (Gaussian-mixture modelling)\ndensity based (DBSCAN)\n\nClustering algorithms have found their place in a diverse range of real-world applications, from customer segmentation in marketing strategies to image segmentation in computer vision, and anomaly detection in cybersecurity for insightful data-driven decision making.\nTo demonstrate the workflow, I will use a K-means clustering algorithm to group together similar shoppers at a shopping mall."
  },
  {
    "objectID": "posts/cluster/cluster.html#business-problem-introduction",
    "href": "posts/cluster/cluster.html#business-problem-introduction",
    "title": "Cluster",
    "section": "2 Business Problem Introduction",
    "text": "2 Business Problem Introduction\nAs the owners of a thriving supermarket mall, you seek a deeper understanding of your customer base. Data points including demographics, spending habits, and loyalty program membership details are at your disposal. Your goal is to identify distinct groups within your customers to tailor marketing initiatives, thereby maximizing the efficiency of your promotional efforts.\nTo achieve this, you opt for clustering algorithms to group similar customers based on characteristics and behaviors. By isolating specific segments of your customer base, you aim to tailor marketing strategies to respective subgroups, increasing the likelihood of a successful outcome and resulting in maximizing the bang for your marketing buck.\n\n\n\na lovely shopping mall\n\n\nPhoto by Sung Jin Cho on Unsplash"
  },
  {
    "objectID": "posts/cluster/cluster.html#data-preprocessing",
    "href": "posts/cluster/cluster.html#data-preprocessing",
    "title": "Cluster",
    "section": "3 Data Preprocessing",
    "text": "3 Data Preprocessing\nIn this initial stage, the goal is to prepare the data for analysis. This involves cleaning the data by removing or filling in missing values, which could be done through various strategies like dropping the missing rows, filling them with mean/median/mode, or using a prediction model. It‚Äôs also crucial to handle outliers and potentially normalize features if they‚Äôre on different scales. This stage might also involve dealing with categorical variables using encoding techniques. Effective preprocessing is crucial for reliable results in the subsequent stages.\nthe owner of the mall has provide the following dataset\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nCustomerID\nUnique ID assigned to the customer\n\n\nGender\nGender of the customer\n\n\nAge\nAge of the customer\n\n\nAnnual Income (k$)\nAnnual income of the customer\n\n\nSpending Score (1-100)\nScore assigned by the mall based on customer behavior and spending nature\n\n\n\n\n\nCode\n# imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport umap\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import power_transform\nfrom yellowbrick.cluster import kelbow_visualizer, silhouette_visualizer\n\n# setting global plotting settings\n# set_matplotlib_formats(\"svg\")\nsns.set_palette(\"tab10\")\nsns.set_style(\"darkgrid\")\nFIGSIZE = (12, 6)\n\n\n\n\nCode\n# Load the customer dataset to analyze shopping patterns\ndf_mall = pd.read_csv(\"artifacts/Mall_Customers.csv\")\n\n# rename columns to be lowercase, for easy typing\ndf_mall = df_mall.rename(\n    columns={\n        \"CustomerID \": \"id\",\n        \"Gender \": \"gender\",\n        \"Age \": \"age\",\n        \"Annual Income (k$) \": \"income\",\n        \"Spending Score (1-100)\": \"spending\",\n    }\n)\ndf_mall[\"gender\"] = df_mall[\"gender\"].str.lower()\ndf_mall[\"gender\"] = df_mall[\"gender\"].str.strip()\n\n\n#\nprint(f\"amount of NULL \\n{df_mall.isna().sum()} \\n\")\n\n# look at a random sample to validate the contents\ndf_mall.sample(6)\n\n\nSo, upon taking a look at the dataset, it seems like we‚Äôve got a mix of numeric and non-numeric columns. Specifically, the gender column is the only non-numeric feature - it‚Äôs all categorical data, with customers labeled as either ‚ÄúMale‚Äù or ‚ÄúFemale‚Äù. All the other columns - id, age, income, and spending - are numeric data types.\nThe id column looks like it‚Äôs just a unique identifier for each customer, so we can exclude that one from our feature set for clustering. Makes sense, right?\nNow, the other features need to be in the same ballpark (i.e., normalized) to work effectively with clustering algorithms. Otherwise, the algorithm will group together instances based on the features with the highest numbers, rather than looking at all the features. That‚Äôd be like trying to compare apples and oranges!\nSo, we need to process the gender column by encoding those categories as numbers. One common way to do this is to map ‚ÄúMale‚Äù and ‚ÄúFemale‚Äù to 1 and 0, respectively. Once we‚Äôve done that, gender will be represented numerically like the other features.\nIn a nutshell, we need to encode the gender categorical data, exclude the customer id column, and normalize the remaining columns before we can apply clustering algorithms. Luckily for us, there are no NULL values in the data, so we don‚Äôt have to worry about dealing with those. Easy peasy!\n\n\nCode\n# convert gender to a numerical value via one-hot-encoding\n# clustering models usally need numerical values\ndf_mall = df_mall.assign(gender=df_mall[\"gender\"].map({\"male\": 1, \"female\": 0}))\n\n# list with features for easy reference\nfeatures = [\"age\", \"income\", \"spending\", \"gender\"]\ndf_feature = df_mall[features]\n\n\n# look at a random sample to validate the contents\ndf_feature.sample(5)\n\n\nI used one-hot encoding to convert the string values of the gender column into numerical values, using the pandas map method in combination with a simple mapping of ‚Äúmale‚Äù to 1 and ‚Äúfemale‚Äù to 0. This resulted in a new ‚Äúgender‚Äù column with 0/1 encoding. The id column is dropped from the feature dataframe."
  },
  {
    "objectID": "posts/cluster/cluster.html#exploratory-data-analysis",
    "href": "posts/cluster/cluster.html#exploratory-data-analysis",
    "title": "Cluster",
    "section": "4 Exploratory Data Analysis",
    "text": "4 Exploratory Data Analysis\nIn this section, we visualize the data and hope to gain some insights into meaningful patterns that can inform our clustering analysis during the next phase.\n\n\nCode\nprint(df_feature.describe().T)\n\n\nThe dataset contains information 200 customers. The average (mean) age is 38.85 years. Ages range from 18 to 70, with 50% of customers aged 36 years or below.\nThe average annual income is $60,560, ranging from $15,000 to $137,000. 50% of customers earn $61,500 or less.\nFor the spending score (1-100), the average is 50.2. Half the customers have a spending score of 50 or below. The minimum is 1 and maximum 99, showing a wide range in spending habits.\nOverall, we see variation among customers in age, income levels, and purchasing patterns. Clustering algorithms can help segment customers into groups based on these attributes to develop targeted marketing approaches. let us first look at the distributions of the features.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=FIGSIZE)\n\ncurr_ax = ax[0]\nsns.countplot(data=df_feature, x=\"gender\", stat=\"count\", hue=\"gender\", ax=curr_ax)\ncurr_ax.legend([\"female\", \"male\"])\n# little hacky way of getting the numbers to show in the plot\ncurr_ax.bar_label(curr_ax.containers[0], fontsize=10)\ncurr_ax.bar_label(curr_ax.containers[1], fontsize=10)\n\ncurr_ax = ax[1]\nsns.kdeplot(x=\"age\", data=df_feature, common_norm=False, hue=\"gender\", ax=curr_ax)\ncurr_ax.legend([\"male\", \"female\"])\n\nplt.show()\n\n\nthe barplot on the right shows the distribution between the gender. Our dataset contains more females(112) than males(88).\nnow if we focus on the age distributions as shown in the kernel density estimation plots. The most striking observation here is that for both genders, there‚Äôs a noticeable peak around 30 years of age. But, if you look closely, there seems to be a more significant number of men falling into the older age groups (55+)\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=FIGSIZE)\n\ncurr_ax = ax[0]\nsns.regplot(\n    data=df_feature[df_feature[\"gender\"] == 0],\n    x=\"age\",\n    y=\"income\",\n    color=sns.color_palette()[0],\n    order=1,\n    lowess=True,\n    truncate=True,\n    ax=curr_ax,\n)\n\n\ncurr_ax = ax[0]\nsns.regplot(\n    data=df_feature[df_feature[\"gender\"] == 1],\n    x=\"age\",\n    y=\"income\",\n    color=sns.color_palette()[1],\n    order=1,\n    lowess=True,\n    truncate=True,\n    ax=curr_ax,\n)\ncurr_ax.legend([\"gender=0\", \"lowess regression\", \"gender=1\", \"lowess regression\"])\ncurr_ax.set_title(\"age vs. income\")\n\n\ncurr_ax = ax[1]\nsns.regplot(\n    data=df_feature[df_feature[\"gender\"] == 0],\n    x=\"age\",\n    y=\"spending\",\n    color=sns.color_palette()[0],\n    order=1,\n    lowess=True,\n    truncate=True,\n    ax=curr_ax,\n)\n\n\ncurr_ax = ax[1]\nsns.regplot(\n    data=df_feature[df_feature[\"gender\"] == 1],\n    x=\"age\",\n    y=\"spending\",\n    color=sns.color_palette()[1],\n    order=1,\n    lowess=True,\n    truncate=True,\n    ax=curr_ax,\n)\ncurr_ax.legend([\"gender=0\", \"lowess regression\", \"gender=1\", \"lowess regression\"])\ncurr_ax.set_title(\"age vs. spending\")\nplt.show()\n\n\nExploring the relation between age, income, and spending is an intriguing aspect of our dataset. using a scatterplot with a lowess regression.\nstarting with the left scatterplot we can see a postive correaltion between age and income up until 35 years old, with the income tapering off got older individuals. This finding is in line with our intuition ‚Äì as people age, they typically accumulate more income due to career advancement and increased earning potential. Interestingly, this trend appears to be consistent for both genders.\nHowever, the spending pattern is a different story! Up until 30 years of age, we see a slight increase in spending. But around that age, there‚Äôs a noticeable decrease in spending that lasts until approximately 50 years old. After that point, there‚Äôs a slight uptick in spending again, but it doesn‚Äôt quite reach the previous level. These findings suggest that people tend to spend less as they get older.\nThe difference between genders regarding age and spending appears to be relatively small compared to the overall age effect. This means that both males and females exhibit similar trends in spending throughout their lives, with some differences in the exact shapes of their spending curves.\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=FIGSIZE)\n\ncurr_ax = ax[0]\nsns.scatterplot(\n    data=df_feature,\n    x=\"income\",\n    y=\"spending\",\n    hue=\"age\",\n    palette=sns.color_palette(\"viridis\", as_cmap=True),\n    ax=curr_ax,\n)\ncurr_ax.set_title(\"age\")\n\n\ncurr_ax = ax[1]\nsns.scatterplot(\n    data=df_feature,\n    x=\"income\",\n    y=\"spending\",\n    hue=\"gender\",\n    ax=curr_ax,\n)\ncurr_ax.set_title(\"gender\")\ncurr_ax.legend([\"female\", \"male\"])\n\nplt.suptitle(\"the relation between income and spending\")\nplt.show()\n\n\nIn the left scatterplot you can clearly see that younger people do seem to have higher levels of spending, but as we move towards older age groups, the spending levels decrease for most individuals. There is no clear relation between income and spending. The cluster of people with spending and income in the 40-60 and 40-65 ranges could indicate that these individuals are in a specific life stage, such as starting families or paying off mortgages. Alternatively, it might suggest that there‚Äôs an external factor influencing both income and spending for this group.\nRegarding the right scatterplot, it seems that there‚Äôs no apparent relationship between income/spending and gender, as the colors representing males and females appear to be evenly distributed throughout the plot. This suggests that income and spending levels are relatively similar for both genders in our dataset.\n\n\nCode\n# create a ratio between income and spending\ndf_ratio = df_feature.assign(si_ratio=df_feature[\"income\"] / df_feature[\"spending\"])\n\nfig, ax = plt.subplots(1, 2, figsize=FIGSIZE)\ncurr_ax = ax[0]\n\nsns.scatterplot(\n    data=df_ratio,\n    x=\"si_ratio\",\n    y=\"age\",\n    hue=\"gender\",\n    alpha=0.7,\n    ax=curr_ax,\n)\ncurr_ax.legend([\"male\", \"female\"])\ncurr_ax.set_title(\"income / spending - unclipped\")\n\n# clip to 95% quantile of the si ratio,\n# this will zoom into the interesting part of the plot\ndf_ratio = df_ratio.assign(\n    si_ratio_clip=df_ratio[\"si_ratio\"].clip(\n        upper=np.quantile(df_ratio[\"si_ratio\"], 0.95)\n    )\n)\n\ncurr_ax = ax[1]\nsns.scatterplot(\n    data=df_ratio,\n    x=\"si_ratio_clip\",\n    y=\"age\",\n    hue=\"gender\",\n    alpha=0.7,\n    ax=curr_ax,\n)\ncurr_ax.set_xlim(right=np.ceil(np.quantile(df_ratio[\"si_ratio\"], 0.95)))\ncurr_ax.legend([\"male\", \"female\"])\ncurr_ax.set_title(\n    f\"income / spending - clipped @ {np.quantile(df_ratio['si_ratio'], 0.95):.2f}\"\n)\n\nplt.show()\n\n\nBy calculating the ratio of income to spending (i.e., income divided by spending), we can gain a better understanding of an individual‚Äôs spending habits in relation to their income. The raw numbers alone don‚Äôt provide a complete picture, but examining the relationship between these ratios can give us valuable insights.\nin the left plot, there are some outliers with rations around 80!!! that means that their income is much higher than the spending score that was assigned to them, possible cause could be a data entry error. To better visualize the trends in our data while still showing the majority of the data points, we can compress the rest of the data by clipping the values at the 0.95 quantile. This approach will help us to focus on the more typical spending behaviors.\nThe right plot shows a clearer separation between age groups, with younger people generally having ratios below 1 and older people above 1 . Lower ratios indicate that individuals are spending relatively more than their income levels compared to their peers. Conversely, higher ratios suggest that they‚Äôre spending less than their income levels.\nThese findings could be due to various factors such as different life stages (e.g., younger people might have more debt or be starting families), savings goals, or lifestyle choices.\n\n\nCode\n# apply yeo-johnson transform to si_ratio\n# this will make the data more like a normal distributuon\ndf_ratio = df_ratio.assign(\n    si_ratio_transform=power_transform(\n        df_ratio[\"si_ratio\"].to_numpy().reshape(-1, 1), method=\"yeo-johnson\"\n    )\n)\n\nfig, ax = plt.subplots(1, 2, figsize=FIGSIZE)\n\ncurr_ax = ax[0]\nsns.histplot(data=df_ratio[\"si_ratio\"], stat=\"percent\", ax=curr_ax)\ncurr_ax.set_title(\"Original\")\n\ncurr_ax = ax[1]\nsns.histplot(\n    data=df_ratio[\"si_ratio_transform\"],\n    stat=\"percent\",\n    ax=curr_ax,\n)\ncurr_ax.set_title(\"applied Yeo-Johnson transform\")\nplt.show()\n\n\nwhen using distance-based clustering algorithms like K-Means, it‚Äôs essential to ensure that all features are on a comparable scale to avoid the algorithm being influenced disproportionately by one feature with larger values. The Yeo-Johnson power transformation is a popular method for normalizing data distribution, especially when dealing with skewed data like in our case. This transformation can help bring our income/spending ratio data closer to a more normal distribution, making it easier for clustering algorithms to identify meaningful patterns across all features. In the right plot, you can see how the yeo-johnson power transformation has helped to make the feature behave much more similarly to a normally distributed one, which will be beneficial when applying distance-based clustering algorithms. By ensuring that all features are on a comparable scale, we can effectively capture the relationships between income, spending, and potentially other demographic variables to discover meaningful patterns in our dataset."
  },
  {
    "objectID": "posts/cluster/cluster.html#feature-engineering",
    "href": "posts/cluster/cluster.html#feature-engineering",
    "title": "Cluster",
    "section": "5 Feature Engineering",
    "text": "5 Feature Engineering\nIn the EDA phase, we‚Äôve observed a connection between age and spending/income. However, what constitutes high spending for a 20-year-old isn‚Äôt the same as for a 60-year-old. To account for this, I‚Äôve grouped ages into bins and calculated the difference between each bin‚Äôs mean value and individual observations. The result is a number that shows whether someone spends more or less than average for their age bracket. Similarly, I applied this methodology to gender as well.\n\n\nCode\n# bin the age variable into 7 bins\n# fmt: off\nbinned = pd.cut(\n    df_feature[\"age\"],\n    bins=[0,20,30,40,50,60,70,80],\n    labels=[1,2,3,4,5,6,7],\n    )\n# fmt: on\ndf_feature = df_feature.assign(age_bin=binned)\n\n# turn off the formatter, to increase readability\n# fmt: off\n# create a new column with the difference between income and the mean income of the gender group\ndf_feature = df_feature.assign(\n    # create a new column with the difference between income and the mean income of the gender group\n    income_vs_gender_mean=df_feature['income'] - df_feature.groupby(\"gender\",)[[\"income\",]].transform(\"mean\").iloc[:, 0],\n    spending_vs_gender_mean=df_feature[\"spending\"] - df_feature.groupby(\"gender\")[[\"spending\",]].transform(\"mean\").iloc[:, 0],\n\n    # create a new column with the difference between income and the mean income of the age group\n    income_vs_age_mean=df_feature[\"income\"] - df_feature.groupby(\"age_bin\",observed=False)[[\"income\",]].transform(\"mean\").iloc[:, 0],\n    spending_vs_age_mean=df_feature[\"spending\"] - df_feature.groupby(\"age_bin\",observed=False)[[\"spending\",]].transform(\"mean\").iloc[:, 0],\n)\n# fmt: on\n\ndf_feature.sample(10)\n\n\nTo capture if a person is spending behaviour is deviating from what we might expect based on their incomer or age. to quantify this, i have opted to calculate the ratio between income (or age) and spending for each observation. this allows us to determine the young spenders or the old savers. at this stage no clipping or normalziation of the ratios is applied. that means that extreme values will be reflected in the data as they are.\n\n\nCode\n# calculate the ratio between\n# spending and income -&gt; how much of the income do you spend\n# spending and age -&gt; if you are older do you spend more or less\ndf_feature = df_feature.assign(\n    si_ratio=df_feature[\"income\"] / df_feature[\"spending\"],\n    sa_ratio=df_feature[\"age\"] / df_feature[\"spending\"],\n)\ndf_feature.sample(10)\n\n\nnow that we have all our features it is time to apply a power transform to them, By applying the Yeo-Johnson normalization method to each of your input features, you‚Äôre transforming them in such a way that they approach normality, making the subsequent clustering process more reliable and robust. This transformation also ensures that all features contribute equally to the clustering results, as no single feature with high values will skew or distort the final outcomes, all will contribute in a fair manner.\n\n\nCode\n# casting values to integer in order for scaling later on\ndf_feature = df_feature.assign(\n    age_bin=df_feature[\"age_bin\"].astype(int),\n)\n\ndf_feature[df_feature.select_dtypes(include=\"number\").columns] = power_transform(\n    X=df_feature[df_feature.select_dtypes(include=\"number\").columns],\n    method=\"yeo-johnson\",\n)\ndf_feature.describe().T\n\n\nThe values in the mean column are nearly zero, with a standard deviation of 1. Given these descriptive statistics and the data transformation we performed, we can be confident that the original data approximates a normal distribution."
  },
  {
    "objectID": "posts/cluster/cluster.html#clustering",
    "href": "posts/cluster/cluster.html#clustering",
    "title": "Cluster",
    "section": "6 Clustering",
    "text": "6 Clustering\nUse a suitable clustering algorithm (like K-means or hierarchical clustering) to divide customers into distinct groups.\n\n\nCode\nfig, ax = plt.subplots(figsize=FIGSIZE)\n\n_ = kelbow_visualizer(\n    KMeans(\n        n_init=10,\n    ),\n    X=df_feature,\n    timings=False,\n    metric=\"distortion\",\n    ax=ax,\n)  # distortion: mean sum of squared distances to centers\n\n\n\n\nCode\n# fit kmeans for various number of clusters\nkmeans_clusters = [\n    KMeans(n_clusters=i, n_init=\"auto\", max_iter=900) for i in range(2, 11)\n]\n\n\n\n\nCode\nfig, axes = plt.subplots(3, 3, figsize=(18, 16), layout=\"constrained\")\n\nfor i, ax in enumerate(axes.flatten()):\n    silhouette_visualizer(\n        kmeans_clusters[i],\n        X=df_feature,\n        ax=ax,\n        is_fitted=False,\n        show=False,\n        colors=sns.color_palette(\"tab10\"),\n    )\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\ndecomp = PCA(n_components=2)\ndecomp_components = decomp.fit_transform(df_feature)\n\n\nfig, ax = plt.subplots(figsize=FIGSIZE)\nsns.scatterplot(\n    x=decomp_components[:, 0],\n    y=decomp_components[:, 1],\n    hue=kmeans_clusters[4].predict(df_feature),\n    palette=sns.color_palette(\"tab10\", 6),\n    ax=ax,\n)\nax.set_xticklabels([\"\"])\nax.set_yticklabels([\"\"])\nplt.suptitle(\"clusters visualised with PCA\")\nplt.show()\n\n\n\n\nCode\ndecomp_umap = umap.UMAP(\n    n_components=2, min_dist=0.5, n_neighbors=12, n_jobs=1, random_state=91\n)\ndecomp_components_umap = decomp_umap.fit_transform(df_feature)\n\nfig, ax = plt.subplots(figsize=FIGSIZE)\n\nsns.scatterplot(\n    x=decomp_components_umap[:, 0],\n    y=decomp_components_umap[:, 1],\n    hue=kmeans_clusters[4].predict(df_feature),\n    palette=sns.color_palette(\"tab10\", 6),\n    ax=ax,\n)\nax.set_xticklabels([\"\"])\nax.set_yticklabels([\"\"])\nplt.suptitle(\"clusters visualised with UMAP\")\n\nplt.show()"
  },
  {
    "objectID": "posts/cluster/cluster.html#analysis-and-evaluation",
    "href": "posts/cluster/cluster.html#analysis-and-evaluation",
    "title": "Cluster",
    "section": "7 Analysis and Evaluation",
    "text": "7 Analysis and Evaluation\nAnalyze each customer group‚Äôs traits, like average age or buying habits. Use metrics like Silhouette Score or Dunn Index to assess clustering quality, evaluating cluster cohesion and separation. A successful clustering result scores well on these metrics and provides actionable business insights.\n\n\nCode\ndf_cluster = df_feature.assign(cluster=kmeans_clusters[4].predict(df_feature))\ndf_mall_cluster = df_mall.assign(cluster=kmeans_clusters[4].predict(df_feature)).drop(\n    columns=[\"id\"]\n)\n\n\n\n\nCode\n#\ndf_all = pd.concat([df_mall.drop(columns=[\"id\"]), df_cluster], axis=1)\n\n\n\n\nCode\ngrouper = df_mall_cluster.groupby(\"cluster\", as_index=False)\ndf_cluster_agg = grouper.mean().round(2)\ndf_cluster_agg = df_cluster_agg.assign(\n    count=grouper.count()[\"age\"], age=(df_cluster_agg[\"age\"].astype(\"int\"))\n)\n\n\n\n\nCode\ndf_cluster_agg\n\n\n\n\nCode\nsns.boxplot(\n    df_mall_cluster,\n    y=\"cluster\",\n    x=\"age\",\n    orient=\"h\",\n    hue=\"cluster\",\n    palette=sns.color_palette(\"tab10\", 6),\n)\n\nplt.title(\"Age of each customer segment\")\nplt.show()\n\n\n\n\nCode\nsns.countplot(\n    df_cluster,\n    x=\"cluster\",\n    hue=\"gender\",\n    stat=\"proportion\",\n    palette=sns.color_palette(\"tab10\", 2),\n)\nplt.legend(\n    [\n        \"female\",\n        \"male\",\n    ]\n)\nplt.title(\"gender ratio between the customer segments\")\nplt.show()\n\n\n\n\nCode\na = 0.2\nmean_gender = df_mall[\"gender\"].mean()\nl_mean_gender, u_mean_gender = mean_gender * (1 - a), mean_gender * (1 + a)\n\n# define a mostly gender column\n# determine which cluster falls outside of the bounds\ndf_gender = df_cluster_agg[[\"cluster\", \"gender\"]].assign(type_gender=\"neutral\")\ndf_gender.loc[df_gender[\"gender\"].gt(u_mean_gender), \"type_gender\"] = \"mostly men\"\ndf_gender.loc[df_gender[\"gender\"].lt(l_mean_gender), \"type_gender\"] = \"mostly female\"\ndf_gender\n\n\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n\ncurr_ax = ax\n_ = sns.scatterplot(\n    data=df_cluster_agg,\n    x=\"income\",\n    y=\"spending\",\n    hue=\"cluster\",\n    palette=sns.color_palette(\"tab10\", 6),\n    s=150,\n    ax=curr_ax,\n)\n\nxlim0, xlim1 = ax.get_xlim()\nylim0, ylim1 = ax.get_ylim()\n\nplt.vlines(\n    df_mall_cluster[\"income\"].median(), ylim0, ylim1, color=\"grey\", linestyles=\"--\"\n)\nplt.hlines(\n    df_mall_cluster[\"spending\"].median(),\n    xlim0,\n    xlim1,\n    color=\"grey\",\n    linestyles=\"--\",\n)\nplt.suptitle(\"Income vs. Spending\")\nplt.show()"
  },
  {
    "objectID": "posts/cluster/cluster.html#insights-and-business-applications",
    "href": "posts/cluster/cluster.html#insights-and-business-applications",
    "title": "Cluster",
    "section": "8 Insights and Business Applications",
    "text": "8 Insights and Business Applications\nExplain how the results could be used to tailor marketing strategies towards each segment for improved customer engagement and retention."
  },
  {
    "objectID": "posts/cluster/cluster.html#data-preprocessingin-this-initial-stage-the-goal-is-to-prepare-the-data-for-analysis.-this-involves-cleaning-the-data-by-removing-or-filling-in-missing-values-which-could-be-done-through-various-strategies-like-dropping-the-missing-rows-filling-them-with-meanmedianmode-or-using-a-prediction-model.-its-also-crucial-to-handle-outliers-and-potentially-normalize-features-if-theyre-on-different-scales.-this-stage-might-also-involve-dealing-with-categorical-variables-using-encoding-techniques.-effective-preprocessing-is-crucial-for-reliable-results-in-the-subsequent-stages.the-owner-of-the-mall-has-provide-the-following-dataset-field-description-----customerid-unique-id-assigned-to-the-customer-gender-gender-of-the-customer-age-age-of-the-customer-annual-income-k-annual-income-of-the-customer-spending-score-1-100-score-assigned-by-the-mall-based-on-customer-behavior-and-spending-nature",
    "href": "posts/cluster/cluster.html#data-preprocessingin-this-initial-stage-the-goal-is-to-prepare-the-data-for-analysis.-this-involves-cleaning-the-data-by-removing-or-filling-in-missing-values-which-could-be-done-through-various-strategies-like-dropping-the-missing-rows-filling-them-with-meanmedianmode-or-using-a-prediction-model.-its-also-crucial-to-handle-outliers-and-potentially-normalize-features-if-theyre-on-different-scales.-this-stage-might-also-involve-dealing-with-categorical-variables-using-encoding-techniques.-effective-preprocessing-is-crucial-for-reliable-results-in-the-subsequent-stages.the-owner-of-the-mall-has-provide-the-following-dataset-field-description-----customerid-unique-id-assigned-to-the-customer-gender-gender-of-the-customer-age-age-of-the-customer-annual-income-k-annual-income-of-the-customer-spending-score-1-100-score-assigned-by-the-mall-based-on-customer-behavior-and-spending-nature",
    "title": "Cluster",
    "section": "9 Data Preprocessing‚Äú,‚Äù‚Äú,‚ÄùIn this initial stage, the goal is to prepare the data for analysis. This involves cleaning the data by removing or filling in missing values, which could be done through various strategies like dropping the missing rows, filling them with mean/median/mode, or using a prediction model. It‚Äôs also crucial to handle outliers and potentially normalize features if they‚Äôre on different scales. This stage might also involve dealing with categorical variables using encoding techniques. Effective preprocessing is crucial for reliable results in the subsequent stages.‚Äú,‚Äù‚Äú,‚Äù‚Äú,‚Äùthe owner of the mall has provide the following dataset‚Äú,‚Äù‚Äú,‚Äù| Field | Description |‚Äú,‚Äù| ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- | ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- |‚Äú,‚Äù| CustomerID | Unique ID assigned to the customer |‚Äú,‚Äù| Gender | Gender of the customer |‚Äú,‚Äù| Age | Age of the customer |‚Äú,‚Äù| Annual Income (k$) | Annual income of the customer |‚Äú,‚Äù| Spending Score (1-100) | Score assigned by the mall based on customer behavior and spending nature |",
    "text": "9 Data Preprocessing‚Äú,‚Äù‚Äú,‚ÄùIn this initial stage, the goal is to prepare the data for analysis. This involves cleaning the data by removing or filling in missing values, which could be done through various strategies like dropping the missing rows, filling them with mean/median/mode, or using a prediction model. It‚Äôs also crucial to handle outliers and potentially normalize features if they‚Äôre on different scales. This stage might also involve dealing with categorical variables using encoding techniques. Effective preprocessing is crucial for reliable results in the subsequent stages.‚Äú,‚Äù‚Äú,‚Äù‚Äú,‚Äùthe owner of the mall has provide the following dataset‚Äú,‚Äù‚Äú,‚Äù| Field | Description |‚Äú,‚Äù| ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- | ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- |‚Äú,‚Äù| CustomerID | Unique ID assigned to the customer |‚Äú,‚Äù| Gender | Gender of the customer |‚Äú,‚Äù| Age | Age of the customer |‚Äú,‚Äù| Annual Income (k$) | Annual income of the customer |‚Äú,‚Äù| Spending Score (1-100) | Score assigned by the mall based on customer behavior and spending nature |\n\n\nCode\n# add the cluster labels to the dataframe\ndf_mall_cluster_centroid = df_mall_cluster.merge(\n    df_cluster_agg.drop(columns=[\"count\"]), on=\"cluster\", suffixes=(\"\", \"_cluster\")\n)\n\n\n\n\nCode\ndef create_blend(df_in: pd.DataFrame, col1: str, steps: int):\n    \"\"\"\n    Applies a function that generates a linear sequence between the value of\n    a specified column and the corresponding cluster value in each row of the dataframe.\n\n    Parameters\n    ----------\n    df_in : pd.DataFrame\n        Input dataframe with at least two columns: one specified by `col1` and another with `col1` suffix '_cluster'.\n    col1 : str\n        The name of the column in the dataframe from which to start the linear sequence.\n    steps : int\n        The number of steps in the linear sequence.\n\n    Returns\n    -------\n    pd.DataFrame\n        A dataframe with each cell containing a linear sequence between the corresponding cell in `col1` and `col1_cluster`.\n\n    \"\"\"\n    return df_in.apply(\n        lambda row: np.linspace(row[col1], row[f\"{col1}_cluster\"], steps), axis=1\n    )\n\n\n# add the blend columns to the three variables of interest\ndf_mall_cluster_centroid = df_mall_cluster_centroid.assign(\n    spending_blend=create_blend(df_mall_cluster_centroid, \"spending\", 50),\n    age_blend=create_blend(df_mall_cluster_centroid, \"age\", 50),\n    income_blend=create_blend(df_mall_cluster_centroid, \"income\", 50),\n)\ndf_mall_cluster_centroid\n\n\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n\nsns.scatterplot(\n    x=df_mall_cluster_centroid[\"spending_blend\"].apply(lambda row: row[0]),\n    y=df_mall_cluster_centroid[\"age_blend\"].apply(lambda row: row[0]),\n    hue=df_mall_cluster_centroid[\"cluster\"],\n    palette=sns.color_palette(\"tab10\", 6),\n    s=200,\n    alpha=0.7,\n    ax=ax,\n)\n\nxlim_begin, ylim_begin = ax.get_xlim(), ax.get_ylim()\n\n###\n\nfig, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n\n\nsns.scatterplot(\n    x=df_mall_cluster_centroid[\"spending_blend\"].apply(lambda row: row[-1]),\n    y=df_mall_cluster_centroid[\"age_blend\"].apply(lambda row: row[-1]),\n    hue=df_mall_cluster_centroid[\"cluster\"],\n    palette=sns.color_palette(\"tab10\", 6),\n    s=200,\n    alpha=0.7,\n    ax=ax,\n)\n\nxlim_end, ylim_end = ax.get_xlim(), ax.get_ylim()\n\nzoom_factor = 0.1\n# get the global axes limits\noveral_xlim = (\n    min(xlim_begin[0], xlim_end[0]) * (1 - zoom_factor),\n    max(xlim_begin[1], xlim_end[1]) * (1 + zoom_factor),\n)\noveral_ylim = (\n    min(ylim_begin[0], ylim_end[0]) * (1 - zoom_factor),\n    max(ylim_begin[1], ylim_end[1]) * (1 + zoom_factor),\n)\n\n\n\n\nCode\nfreeze_frames = 10\nnum_of_steps = len(df_mall_cluster_centroid[\"spending_blend\"][0])\n# create the animation\ngif_frames = [plot_step(i, overal_xlim, overal_ylim) for i in range(num_of_steps)]\n\n# freeze on the bounce point\ngif_frames.extend([gif_frames[-1] for _ in range(freeze_frames // 6)])\n\n# add the the original series in reverse\ngif_frames.extend(gif_frames[::-1])\n\n\ngif.save(gif_frames, \"artifacts/clumper.gif\", duration=1)"
  },
  {
    "objectID": "whois.html",
    "href": "whois.html",
    "title": "whois ü§ñ",
    "section": "",
    "text": "Machine Learning Scientist ü§ñ\nloves to puzzle üß©\nAmsterdam area üá≥üá±üá™üá∫"
  },
  {
    "objectID": "whois.html#whois",
    "href": "whois.html#whois",
    "title": "whois ü§ñ",
    "section": "",
    "text": "Machine Learning Scientist ü§ñ\nloves to puzzle üß©\nAmsterdam area üá≥üá±üá™üá∫"
  }
]