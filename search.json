[
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html",
    "href": "posts/boring_forecast/boring_linear_forecast.html",
    "title": "Boring linear forecast",
    "section": "",
    "text": "Linear regression in itself is not performant for longer time-scales\nMost systems have some capabilities for linear regression built-in\nAdding dummy variables for datetime features(i.e.¬†month, weekday etc.) adds predictive power"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#tldr",
    "href": "posts/boring_forecast/boring_linear_forecast.html#tldr",
    "title": "Boring linear forecast",
    "section": "",
    "text": "Linear regression in itself is not performant for longer time-scales\nMost systems have some capabilities for linear regression built-in\nAdding dummy variables for datetime features(i.e.¬†month, weekday etc.) adds predictive power"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#introduction",
    "href": "posts/boring_forecast/boring_linear_forecast.html#introduction",
    "title": "Boring linear forecast",
    "section": "2 Introduction",
    "text": "2 Introduction\nLinear regression is a statistical model that can be used to determine the linear relationship between variables, most of the time this is seen as a beginners model that is not particularly useful, and most of the time it is discarded and replaced by a shiny neural net or a fancy gradient-boosted model.\nthe nice feature of linear regression is that the direction and magnitude of the relationship can be estimated with the help of linear regression. It is used in many fields including but not limited to Economics, Finance, Social science, etc. The popularity of this model is model is likely due to the fact that most systems have built-in functionality that enables the training of linear models, they are also very cheap/fast to train, and one can also determine that the model has optimal parameters. all these features make linear regression an excellent model to start with.\nTo extend the capabilities of the linear model for time series forecasting, dummy variables can be utilized. These dummies can provide additional information about the relationship over time, can help the model to identify seasonality over time, and also gauge the effect of one-off events, examples are price reductions or natural disasters.\nFor instance, we can create a dummy to identify certain datetime features such as what the month is or whether a particular day is a weekday or a weekend."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#imports",
    "href": "posts/boring_forecast/boring_linear_forecast.html#imports",
    "title": "Boring linear forecast",
    "section": "3 Imports",
    "text": "3 Imports\nFirst we import all the libraries, the default data science libs and the linear model and metrics from sklearn.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import ElasticNetCV, LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#reading-in-the-data",
    "href": "posts/boring_forecast/boring_linear_forecast.html#reading-in-the-data",
    "title": "Boring linear forecast",
    "section": "4 Reading in the data",
    "text": "4 Reading in the data\nFor this example, we will be using a dataset form the prophet package. I have selected this one because, in the prophet documentation, this dataset is used in the section ‚ÄúSeasonality, Holiday Effects, And Regressors‚Äù so it seems fitting to use it, to demonstrate the usefulness of seasonal dummies. The prophet docs describes the dataset in the following way:\n\nAs an example, let‚Äôs look at a time series of the log daily page views for the Wikipedia page for Peyton Manning. We scraped this data using the Wikipedia trend package in R. Peyton Manning provides a nice example because it illustrates some of Prophet‚Äôs features, like multiple seasonality, changing growth rates, and the ability to model special days (such as Manning‚Äôs playoff and Superbowl appearances).\n\n\n\nCode\ndf_in = pd.read_csv(\n    \"https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv\"\n)\ndf_in = df_in.assign(ds=pd.to_datetime(df_in[\"ds\"]))\ndf_in = df_in[(df_in[\"ds\"] &gt; \"2012\")]  # selecting data after 2012\n\n\nIt is always nice to see what we are working with so let us plot the data over time to see what we visually can extract from the plot. we are looking for patterns over time and other behavior and/or pattern that we can exploit.\n\n\nCode\nplt.plot_date(\n    x=df_in[\"ds\"],\n    y=df_in[\"y\"],\n    label=\"input timeseries\",\n    fmt=\"-\",\n)\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"target variable - $y$\")\nplt.title(\"daily visits ot Peyton Manning wiki on a daily basis (log)\")\nplt.show()\n\n\n\n\n\nI have selected data from 2012 onwards, this should be enough to test drive the seasonal dummies. The first thing that I notice is that there is a dip around months 06 and 07. In early August teams play so exhibition games before the start of the actual season.\nAlso, we can observe a pattern over the year, it starts high then dips, and then and high again. This can be seen for the other years as well, so there is some repeating seasonality. Let us continue and train our first models. Starting with a simple ordinary linear regression and then adding dummies to see if they improve the performance of the model."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#train-test-split",
    "href": "posts/boring_forecast/boring_linear_forecast.html#train-test-split",
    "title": "Boring linear forecast",
    "section": "5 Train-test split",
    "text": "5 Train-test split\n\n\nCode\n# train test split\ndf_train = df_in[(df_in[\"ds\"] &gt; \"2012\") & (df_in[\"ds\"] &lt; \"2015\")]\ndf_test = df_in[(df_in[\"ds\"] &gt; \"2015\")]\n\n\nTo gauge the performance of the model the data is split in two parts, the train data from 2012 up to 2015 and the test data everything after 2015. The model will only see the train data and is asked to create a prediction for the test data, after which we will calculate the perfomance with the predictions and the true observations.\n\n\nCode\n# visually inspect the train test split\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\"data is splitted, everything before 2015 is train data after 2015 test\")\nplt.show()"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#setting-up-the-regression",
    "href": "posts/boring_forecast/boring_linear_forecast.html#setting-up-the-regression",
    "title": "Boring linear forecast",
    "section": "6 Setting up the regression",
    "text": "6 Setting up the regression\n\n\nCode\nX_train = df_train[\"ds\"].astype(int).values.reshape(-1, 1)\ny_train = df_train[\"y\"].values\n\nX_test = df_test[\"ds\"].astype(int).values.reshape(-1, 1)\ny_test = df_test[\"y\"].values\n\n\nShaping and converting the data so that we can fit the linear model. In this case, we are converting the date columns into an ever-increasing integer.\n\n\nCode\n# creating, fit, and inference\nlinear = LinearRegression()\nlinear.fit(X=X_train, y=y_train)\ny_pred = linear.predict(X=X_test)\n\n\nFitting the linear model is very fast, around 4 milliseconds on my machine, and yes, I know the data volume is low(actually for time series it is pretty decent), but this allows me to fit 1000 models in 4 seconds. So, we can make predictions on 1000 different time series, all in the time it takes to take a sip of coffee‚òïÔ∏è.\nAnyways lets us visually inspect the results of the model, by plotting all the components (train, test, predictions) that we have gathered. Also, let us calculate two error metrics mse and mae that way we can quantify the performance of the model.\n\n\nCode\n# calc error metrics\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\n\n# visually inspect the prediction\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred,\n    label=\"prediction\",\n    fmt=\"--\",\n)\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(f\"linear regression applied (MSE= {mse:.3}, MAE={mae:.3})\")\nplt.show()\n\n\n\n\n\nThe model is just a boring straight line, but what did you expect with a linear model ?! The green line is passing through the orange blob more or less, and intuitively it kind of moves in the right direction. However, it doesn‚Äôt capture the seasonality or other patterns of the train set. The two error metrics mse and mae are both around 0.6. this in itself does not say much, but we can use this as the ‚Äòbenchmark‚Äô and try to improve on this with the addition of the dummies.\n\n\nmean squared error  = 0.617\nmean absolute error = 0.615"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#adding-dummies",
    "href": "posts/boring_forecast/boring_linear_forecast.html#adding-dummies",
    "title": "Boring linear forecast",
    "section": "7 Adding dummies",
    "text": "7 Adding dummies\nLet us add some dummies for the months and see if we can improve the performance of the model visually and on the metrics. For each month we create a column that can either have the value of 0 or 1. if the value is 1 then that row corresponds to that particular month, in this way we are actively encoding this knowledge into the model. in this way the model can learn the magnitude and direction of each month and apply that to future predictions. at least that is the idea, let us test that!\n\n\nCode\n# creating dummies for the months\ndf_dummies = df_in.assign(\n    month=df_in[\"ds\"].dt.month.astype(\"category\"),\n    ds_int=df_in[\"ds\"].astype(int),\n)\n\nnot_dummy = {\"y\", \"ds\", \"ds_int\"}\nto_dummy = list(set(df_dummies.columns) - not_dummy)\n\ndf_dummies = pd.get_dummies(data=df_dummies, columns=to_dummy)\nall_features = list(set(df_dummies.columns) - {\"y\", \"ds\"})\n\n# slicing the input in train test\ndf_train_dummies = df_dummies[(df_dummies[\"ds\"] &gt; \"2012\") & (df_dummies[\"ds\"] &lt; \"2015\")]\ndf_test_dummies = df_dummies[(df_dummies[\"ds\"] &gt; \"2015\")]\n\nX_train = df_train_dummies.loc[:, all_features]\ny_train = df_train_dummies[[\"y\"]]\n\nX_test = df_test_dummies.loc[:, all_features]\ny_test = df_test_dummies[[\"y\"]]\n\ndf_dummies.drop(columns=[\"ds_int\", \"y\"]).sample(\n    n=6,\n    ignore_index=True,\n    random_state=42,\n)\n\n\n\n\n\n\n\n\n\nds\nmonth_1\nmonth_2\nmonth_3\nmonth_4\nmonth_5\nmonth_6\nmonth_7\nmonth_8\nmonth_9\nmonth_10\nmonth_11\nmonth_12\n\n\n\n\n0\n2014-05-06\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n2012-07-05\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n2\n2015-06-21\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n2012-03-09\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n2012-08-10\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n5\n2013-05-11\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nIn the code block above we are adding the dummies for the month to the data. In this case month_1 is jan and month_12 is dec.\nNext up we are training the model with the added dummies and plotting the results next to the model without the dummies.\n\n\nCode\n# create the pipeline and fit pipeline\n# scaler is there so that the coefs can be interpeted later\n# pipeline = make_pipeline(StandardScaler(), LinearRegression())\npipeline = make_pipeline(MinMaxScaler(), LinearRegression())\n\npipeline.fit(X=X_train, y=y_train)\ny_pred_dummies = pipeline.predict(X=X_test)\n\nmse_dummies = mean_squared_error(y_test, y_pred_dummies)\nmae_dummies = mean_absolute_error(y_test, y_pred_dummies)\n\nplt.plot_date(\n    x=df_train[\"ds\"],\n    y=df_train[\"y\"],\n    label=\"train\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred,\n    label=\"prediction\",\n    fmt=\"--\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies,\n    label=\"prediction with dummies\",\n    fmt=\"--\",\n)\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\n    f\"linear regression with dummies applied (mse= {mse_dummies:.3}, mae={mae_dummies:.3})\"\n)\nplt.show()\n\n\n\n\n\nVisually the forecast already looks a lot better. It follows the peaks and valleys of the time-series, and it captures the overall trend better. this is also reflected when we look at the error metrics and the improvement factor that we have achieved. the mean squared error improved with a factor of 1.89 and the mean absolute error improved with a factor of 1.54. That is impressive for just adding a bunch of ones and zeros.\n\n\nCode\nprint(f\"mean squared error  = {mse_dummies:.3}\")\nprint(f\"improvement factor mse month dummies -&gt; {mse/mse_dummies:.3}x\")\n\nprint(\"-\" * 79)\n\nprint(f\"mean absolute error = {mae_dummies:.3}\")\nprint(f\"improvement factor mea month dummies -&gt; {mae/mae_dummies:.3}x\")\n\n\nmean squared error  = 0.323\nimprovement factor mse month dummies -&gt; 1.91x\n-------------------------------------------------------------------------------\nmean absolute error = 0.398\nimprovement factor mea month dummies -&gt; 1.55x"
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#inspecting-the-seasonality",
    "href": "posts/boring_forecast/boring_linear_forecast.html#inspecting-the-seasonality",
    "title": "Boring linear forecast",
    "section": "8 Inspecting the seasonality",
    "text": "8 Inspecting the seasonality\nNow that we have encoded the information about the seasonality in the model, this also allows us to inspect that seasonality by itself. this would give us some insight into the inner workings of the underlying time series model. first we access the coefficients of the linear model and put them into a separate dataframe. then we need to scale them so that the relative difference is more apparent. looking at the raw coefficients would not yield any information as the scale is not relatable to the original problem.\n\n\nCode\n# pull coefs into a seperate df, to inspect the seasonality\nlin_reg_coefs = (\n    pd.DataFrame(\n        data=pipeline[\"linearregression\"].coef_,\n        columns=X_train.columns,\n    )\n    .T.reset_index()\n    .rename(columns={\"index\": \"month\", 0: \"coefficient\"})\n)\n# exclude the time col\nlin_reg_coefs = lin_reg_coefs[lin_reg_coefs[\"month\"] != \"ds_int\"]\n\n# subtract mean to get the relative difference between the coefs\nlin_reg_coefs[\"coefficient\"] = (\n    lin_reg_coefs[\"coefficient\"] - lin_reg_coefs[\"coefficient\"].mean()\n)\n\n\n\n\nCode\nchart = sns.barplot(\n    data=lin_reg_coefs,\n    x=\"month\",\n    y=\"coefficient\",\n    color=sns.color_palette()[0],\n    order=[f\"month_{i}\" for i in range(1, 13)],\n)\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"\")\nplt.title(\"yearly seasonality\")\nplt.show()\n\n\n\n\n\nNow we have a beautiful overview of the seasonality over the year. we can clearly see a dip in the middle of the year and a significant uptick in the month of January. in the first week of February, the super bowl is played which is a major factor in the traffic to the wiki page. Also, we noticed that there was a dip in the middle of the year which we can also clearly see at the month_6 mark."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#recap",
    "href": "posts/boring_forecast/boring_linear_forecast.html#recap",
    "title": "Boring linear forecast",
    "section": "9 Recap",
    "text": "9 Recap\nIn this blog post I have demonstrated that the performance of a simple linear regression for time series forecasting can be improved by a factor of 1.54 up to 1.89 by simply adding dummy variables for the months. the nice thing about this is that the linear regression is available in most systems that have some kind of analytical capability (yes even in excel) and adding the dummies is so simple that you can even do it in a SQL server, the added benefit of this all is that the fitting of the model is quick, therefore you can retrain the model monthly üòâ, weekly, daily, hourly."
  },
  {
    "objectID": "posts/boring_forecast/boring_linear_forecast.html#encore",
    "href": "posts/boring_forecast/boring_linear_forecast.html#encore",
    "title": "Boring linear forecast",
    "section": "10 Encore",
    "text": "10 Encore\nWhat if we were to dummies not just for the months, but also for other datetime features and really turn it up to eleven\n\nLet us create dummies for the following datetime features:\n\nmonth\nweek\ndayofweek\nis_weekend\nquarter\n\nwhen converting this to dummies it will result in a tremendous number of extra features and therefore we will apply a Elasticnet linear model. Usually, this type of model can handle lots of features better than an ordinary linear regression because of the regularization, this will be left as an exercise for the reader.\n\n\nCode\n# creating dummies for the months\ndf_dummies_all = df_in.assign(\n    month=df_in[\"ds\"].dt.month.astype(\"category\"),\n    week=df_in[\"ds\"].dt.isocalendar().week.astype(\"category\"),\n    dayofweek=df_in[\"ds\"].dt.dayofweek.astype(\"category\"),\n    is_weekend=(df_in[\"ds\"].dt.dayofweek) &gt;= 5,\n    quarter=df_in[\"ds\"].dt.quarter.astype(\"category\"),\n    ds_int=df_in[\"ds\"].astype(int),\n)\n\nnot_dummy = {\"y\", \"ds\", \"ds_int\"}\nto_dummy = list(set(df_dummies_all.columns) - not_dummy)\n\ndf_dummies_all = pd.get_dummies(\n    data=df_dummies_all,\n    columns=to_dummy,\n    drop_first=True,  # reduce the amount of cols with no additional info\n)\nall_features = list(set(df_dummies_all.columns) - {\"y\", \"ds\"})\n\n# slicing the input in train test\ndf_train_dummies_all = df_dummies_all[\n    (df_dummies_all[\"ds\"] &gt; \"2012\") & (df_dummies_all[\"ds\"] &lt; \"2015\")\n]\ndf_test_dummies_all = df_dummies_all[(df_dummies_all[\"ds\"] &gt; \"2015\")]\n\nX_train = df_train_dummies_all.loc[:, all_features]\ny_train = df_train_dummies_all[[\"y\"]]\n\nX_test = df_test_dummies_all.loc[:, all_features]\ny_test = df_test_dummies_all[[\"y\"]]\n\ndf_dummies_all.drop(columns=[\"ds_int\", \"y\"]).sample(\n    n=6,\n    ignore_index=True,\n    random_state=42,\n)\n\n\n\n\n\n\n\n\n\nds\ndayofweek_1\ndayofweek_2\ndayofweek_3\ndayofweek_4\ndayofweek_5\ndayofweek_6\nweek_2\nweek_3\nweek_4\n...\nmonth_6\nmonth_7\nmonth_8\nmonth_9\nmonth_10\nmonth_11\nmonth_12\nquarter_2\nquarter_3\nquarter_4\n\n\n\n\n0\n2014-05-06\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n1\n2012-07-05\n0\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n2015-06-21\n0\n0\n0\n0\n0\n1\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n2012-03-09\n0\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n2012-08-10\n0\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n5\n2013-05-11\n0\n0\n0\n0\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n6 rows √ó 74 columns\n\n\n\n\n\nCode\n# utilzing an elasticnet linear model to compensate for the amount of features\nelastic_params = {\n    \"l1_ratio\": np.linspace(start=0.000001, stop=1, num=100),\n    \"cv\": 7,\n    \"n_alphas\": 1_00,\n    \"n_jobs\": -1,\n}\n\npipeline_all = make_pipeline(MinMaxScaler(), ElasticNetCV(**elastic_params))\n\npipeline_all.fit(X=X_train, y=y_train.values.ravel())\ny_pred_dummies_all = pipeline_all.predict(X=X_test)\n\nmse_dummies_all = mean_squared_error(y_test, y_pred_dummies_all)\nmae_dummies_all = mean_absolute_error(y_test, y_pred_dummies_all)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=df_test[\"y\"],\n    label=\"test\",\n    fmt=\"-\",\n    alpha=0.7,\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies,\n    label=\"prediction with dummies\",\n    fmt=\"--\",\n)\n\nplt.plot_date(\n    x=df_test[\"ds\"],\n    y=y_pred_dummies_all,\n    label=\"prediction with all dummies\",\n    fmt=\"--\",\n)\n\n\nplt.legend(loc=\"upper right\")\nplt.tick_params(axis=\"x\", rotation=45)\nplt.ylabel(\"$y$\")\nplt.title(\n    f\"linear regression with dummies applied (mse= {mse_dummies_all:.3}, mae={mae_dummies_all:.3})\"\n)\nplt.show()\n\n\n\n\n\nImmediately it becomes obvious that the model captures more of the fine-grained movement of the time series. this is also reflected in the fact that both error metrics have improved.\n\n\nCode\nprint(f\"mean squared error = {mse_dummies_all:.3}\")\nprint(f\"improvement factor mse month dummies -&gt; {mse/mse_dummies:.3}x\")\nprint(f\"improvement factor mse all dummies   -&gt; {mse/mse_dummies_all:.3}x\")\nprint(\"-\" * 79)\n\nprint(f\"mean absolute error = {mae_dummies_all:.3}\")\nprint(f\"improvement factor mea month dummies -&gt; {mae/mae_dummies:.3}x\")\nprint(f\"improvement factor mea all dummies   -&gt; {mae/mae_dummies_all:.3}x\")\n\n\nmean squared error = 0.262\nimprovement factor mse month dummies -&gt; 1.91x\nimprovement factor mse all dummies   -&gt; 2.35x\n-------------------------------------------------------------------------------\nmean absolute error = 0.356\nimprovement factor mea month dummies -&gt; 1.55x\nimprovement factor mea all dummies   -&gt; 1.73x"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hyper-shotgun",
    "section": "",
    "text": "Cluster\n\n\ntogether\n\n\n\n\n\n\n\n\n\n2024-01-17\n\n\nJoost de Theije + LLM\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nBoring linear forecast\n\n\nimproving performance by adding some dummies\n\n\n\n\n\n\n\n\n\n2023-03-5\n\n\nJoost de Theije\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "whois.html",
    "href": "whois.html",
    "title": "whois ü§ñ",
    "section": "",
    "text": "Machine Learning Scientist ü§ñ\nloves to puzzle üß©\nAmsterdam area üá≥üá±üá™üá∫"
  },
  {
    "objectID": "whois.html#whois",
    "href": "whois.html#whois",
    "title": "whois ü§ñ",
    "section": "",
    "text": "Machine Learning Scientist ü§ñ\nloves to puzzle üß©\nAmsterdam area üá≥üá±üá™üá∫"
  },
  {
    "objectID": "posts/cluster/cluster.html",
    "href": "posts/cluster/cluster.html",
    "title": "Cluster",
    "section": "",
    "text": "Unsupervised Learning -&gt; Clustering algorithms are used for unsupervised learning, ideal for exploratory data analysis.\nGrouping Data -&gt; These algorithms group similar data into clusters based on specific criteria.\nVariety of Applications -&gt; They‚Äôre used in diverse fields like customer segmentation, anomaly detection, and more.\nDifferent Techniques -&gt; Various types exist, like K-means and DBSCAN, each with unique strengths and suited for specific data types.\nChoice of Parameters -&gt; The selection and tuning of parameters, like the number of clusters, significantly influence the results.\n\nIn the vast field of machine learning, clustering algorithms hold a pivotal role. Used as a form of unsupervised learning, they uncover hidden patterns and structures within unlabeled data, making them instrumental for exploratory data analysis. The essence of clustering algorithms lies in their ability to sift through unstructured data and bring together similar items, forming distinct groups or ‚Äòclusters‚Äô based on defined criteria.\nThese algorithms have found their place in a myriad of real-world applications. From customer segmentation in marketing strategies, to image segmentation in computer vision, anomaly detection in cybersecurity, and dimensionality reduction in high-dimensional data‚Äîclustering algorithms are at the heart of insightful data-driven decision making.\nThe world of clustering algorithms is diverse, housing various types such as K-means, hierarchical, DBSCAN, and Gaussian Mixture Models, each coming with its unique strengths, limitations, and suitability for certain types of data.\nA crucial aspect of working with these algorithms is the specification of parameters. While some algorithms like K-means require the user to define the number of clusters in advance, others, like DBSCAN, determine the number of clusters based on the data itself. The choice and tuning of these parameters can significantly steer the algorithm‚Äôs effectiveness, making this a critical skill for any data practitioner.\nIn this blog post, we delve deeper into the intriguing world of clustering algorithms, demystifying their workings, exploring their varieties, and discussing how to choose and fine-tune parameters for optimal results. Stay tuned as we embark on this insightful journey."
  },
  {
    "objectID": "posts/cluster/cluster.html#tldr",
    "href": "posts/cluster/cluster.html#tldr",
    "title": "Cluster",
    "section": "",
    "text": "Unsupervised Learning -&gt; Clustering algorithms are used for unsupervised learning, ideal for exploratory data analysis.\nGrouping Data -&gt; These algorithms group similar data into clusters based on specific criteria.\nVariety of Applications -&gt; They‚Äôre used in diverse fields like customer segmentation, anomaly detection, and more.\nDifferent Techniques -&gt; Various types exist, like K-means and DBSCAN, each with unique strengths and suited for specific data types.\nChoice of Parameters -&gt; The selection and tuning of parameters, like the number of clusters, significantly influence the results.\n\nIn the vast field of machine learning, clustering algorithms hold a pivotal role. Used as a form of unsupervised learning, they uncover hidden patterns and structures within unlabeled data, making them instrumental for exploratory data analysis. The essence of clustering algorithms lies in their ability to sift through unstructured data and bring together similar items, forming distinct groups or ‚Äòclusters‚Äô based on defined criteria.\nThese algorithms have found their place in a myriad of real-world applications. From customer segmentation in marketing strategies, to image segmentation in computer vision, anomaly detection in cybersecurity, and dimensionality reduction in high-dimensional data‚Äîclustering algorithms are at the heart of insightful data-driven decision making.\nThe world of clustering algorithms is diverse, housing various types such as K-means, hierarchical, DBSCAN, and Gaussian Mixture Models, each coming with its unique strengths, limitations, and suitability for certain types of data.\nA crucial aspect of working with these algorithms is the specification of parameters. While some algorithms like K-means require the user to define the number of clusters in advance, others, like DBSCAN, determine the number of clusters based on the data itself. The choice and tuning of these parameters can significantly steer the algorithm‚Äôs effectiveness, making this a critical skill for any data practitioner.\nIn this blog post, we delve deeper into the intriguing world of clustering algorithms, demystifying their workings, exploring their varieties, and discussing how to choose and fine-tune parameters for optimal results. Stay tuned as we embark on this insightful journey."
  },
  {
    "objectID": "posts/cluster/cluster.html#data-preprocessing",
    "href": "posts/cluster/cluster.html#data-preprocessing",
    "title": "Cluster",
    "section": "2 Data Preprocessing",
    "text": "2 Data Preprocessing\nIn this initial stage, the goal is to prepare the data for analysis. This involves cleaning the data by removing or filling in missing values, which could be done through various strategies like dropping the missing rows, filling them with mean/median/mode, or using a prediction model. It‚Äôs also crucial to handle outliers and potentially normalize features if they‚Äôre on different scales. This stage might also involve dealing with categorical variables using encoding techniques. Effective preprocessing is crucial for reliable results in the subsequent stages.\n#TODO add intro to the dataset dataset can be found here\n\n\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\nUnique ID assigned to the customer\nGender of the customer\nAge of the customer\nAnnual income of the customer\nScore assigned by the mall based on customer behavior and spending nature\n\n\n\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nCustomerID\nUnique ID assigned to the customer\n\n\nGender\nGender of the customer\n\n\nAge\nAge of the customer\n\n\nAnnual Income (k$)\nAnnual income of the customer\n\n\nSpending Score (1-100)\nScore assigned by the mall based on customer behavior and spending nature\n\n\n\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\n\n\n\n\nCode\n# Load the customer dataset to analyze shopping patterns\ndf_mall = pd.read_csv(\"artifacts/Mall_Customers.csv\")\n\n# rename columns to be lowercase, for easy typing\ndf_mall = df_mall.rename(\n    columns={\n        \"CustomerID \": \"id\",\n        \"Gender \": \"gender\",\n        \"Age \": \"age\",\n        \"Annual Income (k$) \": \"income\",\n        \"Spending Score (1-100)\": \"spending\",\n    }\n)\ndf_mall[\"gender\"] = df_mall[\"gender\"].str.lower()\ndf_mall[\"gender\"] = df_mall[\"gender\"].str.strip()\n\n\n# look at a random sample to validate the contents\ndisplay(df_mall.sample(6))\n\n\n\n\n\n\n\n\n\nid\ngender\nage\nincome\nspending\n\n\n\n\n103\n104\nmale\n26\n62\n55\n\n\n177\n178\nmale\n27\n88\n69\n\n\n43\n44\nfemale\n31\n39\n61\n\n\n81\n82\nmale\n38\n54\n55\n\n\n169\n170\nmale\n32\n87\n63\n\n\n99\n100\nmale\n20\n61\n49\n\n\n\n\n\n\n\nUpon examination of the dataset, it appears that the ‚Äògender‚Äô column is the only non-numeric feature. Specifically, it contains string values categorizing customers as either ‚ÄúMale‚Äù or ‚ÄúFemale‚Äù. All other columns - ‚Äòid‚Äô, ‚Äòage‚Äô, ‚Äòincome‚Äô, and ‚Äòspending‚Äô - are numeric data types.\nThe ‚Äòid‚Äô column seems to be a unique identifier for each customer. While numeric, this feature has very high cardinality (a distinct value for every customer). Features with high cardinality tend to dominate distance calculations in clustering algorithms and obscure more meaningful patterns in the data. Therefore, we should exclude the customer ID column from the feature set used for clustering.\nFor the remaining features to function effectively in clustering algorithms, the input features generally need to be numeric rather than categorical strings. Distance-based algorithms like k-means rely on computing distances between data points across features. String categories do not have an inherent numeric ordering, so the distance computations would not be meaningful.\nAdditionally, when performing exploratory analysis, a numeric encoding often allows easier visualization of the impact of gender relative to the other numeric features. Box plots, scatter plots with color-coded points, and other techniques can provide better insights into how gender correlates with shopper age, income or spending habits when mapped to a numeric form.\nTherefore, to enable the application of clustering algorithms and more impactful analysis, we need to process the ‚Äògender‚Äô column by encoding the categories as numbers. Common encoding schemes include mapping ‚ÄúMale‚Äù and ‚ÄúFemale‚Äù to 0 and 1 respectively, or using one-hot encoding to create an additional binary feature column for each category. After encoding, gender will be represented numerically like the other existing features.\nIn summary, as gender is the only non-numeric column, and ID has high cardinality unsuitable for clustering, effectively preparing the data requires encoding the gender categorical data and excluding the customer ID column.\n\n\nCode\n# convert gender to a numerical value via one-hot-encoding\n# clustering models usally need numerical values\ndf_mall = pd.get_dummies(df_mall, columns=[\"gender\"], drop_first=True)\n\n# list with features for easy reference\nfeatures = [\"age\", \"income\", \"spending\", \"gender_male\"]\ndf_feature = df_mall[features]\n\n\n# look at a random sample to validate the contents\ndf_feature.sample(5)\n\n\n\n\n\n\n\n\n\nage\nincome\nspending\ngender_male\n\n\n\n\n65\n18\n48\n59\nTrue\n\n\n11\n35\n19\n99\nFalse\n\n\n184\n41\n99\n39\nFalse\n\n\n50\n49\n42\n52\nFalse\n\n\n159\n30\n78\n73\nFalse\n\n\n\n\n\n\n\n\n\nCode\ndf_feature.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype\n---  ------       --------------  -----\n 0   age          200 non-null    int64\n 1   income       200 non-null    int64\n 2   spending     200 non-null    int64\n 3   gender_male  200 non-null    bool \ndtypes: bool(1), int64(3)\nmemory usage: 5.0 KB"
  },
  {
    "objectID": "posts/cluster/cluster.html#exploratory-data-analysis",
    "href": "posts/cluster/cluster.html#exploratory-data-analysis",
    "title": "Cluster",
    "section": "3 Exploratory Data Analysis",
    "text": "3 Exploratory Data Analysis\nVisualize the data and derive initial insights.\n\n\nCode\ndisplay(df_feature.describe().T)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nage\n200.0\n38.85\n13.969007\n18.0\n28.75\n36.0\n49.0\n70.0\n\n\nincome\n200.0\n60.56\n26.264721\n15.0\n41.50\n61.5\n78.0\n137.0\n\n\nspending\n200.0\n50.20\n25.823522\n1.0\n34.75\n50.0\n73.0\n99.0\n\n\n\n\n\n\n\nThe dataset contains information 200 customers. The average (mean) age is 38.85 years. Ages range from 18 to 70, with 50% of customers aged 36 years or below.\nThe average annual income is $60,560, ranging from $15,000 to $137,000. 50% of customers earn $61,500 or less.\nFor the spending score (1-100), the average is 50.2. Half the customers have a spending score of 50 or below. The minimum is 1 and maximum 99, showing a wide range in spending habits.\nOverall, we see variation among customers in age, income levels, and purchasing patterns. Clustering algorithms can help segment customers into groups based on these attributes to develop targeted marketing approaches. let us first look at the distributions of the features.\n\n\nCode\nsns.pairplot(df_feature, hue=\"gender_male\", corner=True, diag_kind=\"hist\");\n\n\n/Users/JdeTheije/mambaforge/envs/hypershotgun/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/JdeTheije/mambaforge/envs/hypershotgun/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/JdeTheije/mambaforge/envs/hypershotgun/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):"
  },
  {
    "objectID": "posts/cluster/cluster.html#feature-engineering",
    "href": "posts/cluster/cluster.html#feature-engineering",
    "title": "Cluster",
    "section": "4 Feature Engineering",
    "text": "4 Feature Engineering\nBased on your EDA, create features that might help define customer segmentation, such as total purchase value, average purchase size, frequency of purchase, etc."
  },
  {
    "objectID": "posts/cluster/cluster.html#clustering",
    "href": "posts/cluster/cluster.html#clustering",
    "title": "Cluster",
    "section": "5 Clustering",
    "text": "5 Clustering\nUse a suitable clustering algorithm (like K-means or hierarchical clustering) to divide customers into distinct groups."
  },
  {
    "objectID": "posts/cluster/cluster.html#analysis-and-evaluation",
    "href": "posts/cluster/cluster.html#analysis-and-evaluation",
    "title": "Cluster",
    "section": "6 Analysis and Evaluation",
    "text": "6 Analysis and Evaluation\nAnalyze each customer group‚Äôs traits, like average age or buying habits. Use metrics like Silhouette Score or Dunn Index to assess clustering quality, evaluating cluster cohesion and separation. A successful clustering result scores well on these metrics and provides actionable business insights."
  },
  {
    "objectID": "posts/cluster/cluster.html#insights-and-business-applications",
    "href": "posts/cluster/cluster.html#insights-and-business-applications",
    "title": "Cluster",
    "section": "7 Insights and Business Applications",
    "text": "7 Insights and Business Applications\nExplain how the results could be used to tailor marketing strategies towards each segment for improved customer engagement and retention."
  }
]