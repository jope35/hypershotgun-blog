{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Pyspark ⚡️ style guide\"\n",
    "author: \"Joost de Theije + LLM\"\n",
    "subtitle: \"appear in style and sparkly \"\n",
    "date: \"01/25/2025\"\n",
    "image: \"artifacts/splash.png\"\n",
    "abstract: \"PySpark is a wrapper language that allows users to interface with an Apache Spark backend to quickly process data. \n",
    "Spark can operate on massive datasets across a distributed network of servers, providing major performance and reliability benefits when utilized correctly. \n",
    "It presents challenges, even for experienced Python developers, as the PySpark syntax draws on the JVM heritage of Spark and therefore implements code patterns that may be unfamiliar.\n",
    "This opinionated guide to PySpark code style presents common situations encountered and the associated best practices based on the most frequent recurring topics across PySpark repos.\n",
    "adapted from [here](https://github.com/palantir/pyspark-style-guide) and [here](https://github.com/axelearning/pyspark-style-guide)\"\n",
    "format:\n",
    "  html: default\n",
    "draft: false\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Standard practise aka no-brainers 🧠\n",
    "## import Pyspark modules with default aliases\n",
    "\n",
    "Importing these will enable you as a developer to use the types and functions without importing each item seperatly, resulting in speedy workflow. Also this will eliminate the oopsie when you are applying the sum from the python lib instead of the Pyspark lib. Using the `F` prefix for all Pyspark functions, `T` prefix for all Pyspark types, and `W` to specify everything surounding Windows.\n",
    "\n",
    "\n",
    "```python\n",
    "from pyspark.sql import DataFrame as df, functions as F, types as T, Window as W#noqa\n",
    "\n",
    "F.sum() \n",
    "T.Stringtype()\n",
    "```\n",
    "\n",
    "\n",
    "## Type annotaion and docstrings in functions\n",
    "Type annotations or type hints so that you as a developer can clearly indicate what your intended datatype is. Also this integrates nicely with editors and IDE. static type checkers such as [mypy](https://www.mypy-lang.org/) utilize it to help you catch errors early.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# bad\n",
    "def calculate_daily_scoops(df, flavor_col, date_col):\n",
    "    return df.groupBy(date_col, flavor_col).agg(F.sum(\"scoops_sold\").alias(\"total_scoops\"))\n",
    "\n",
    "# good\n",
    "def calculate_daily_scoops(df: DataFrame, flavor_col: str, date_col: str) -> DataFrame:\n",
    "    \"\"\"Calculate total scoops sold per day and flavor.\n",
    "\n",
    "    Groups the input DataFrame by date and flavor, aggregating the total number\n",
    "    of scoops sold for each unique combination.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Input DataFrame containing sales data.\n",
    "    flavor_col : str\n",
    "        Name of the column containing ice cream flavor information.\n",
    "    date_col : str\n",
    "        Name of the column containing date information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        DataFrame with total scoops sold, grouped by date and flavor.\n",
    "        Contains columns for date, flavor, and total scoops.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pyspark.sql.functions as F\n",
    "    >>> sales_df = spark.createDataFrame(...)\n",
    "    >>> daily_scoops = calculate_daily_scoops(sales_df, 'flavor', 'sale_date')\n",
    "    >>> daily_scoops.show()\n",
    "    \"\"\"\n",
    "    return df.groupBy(date_col, flavor_col).agg(\n",
    "        F.sum(\"scoops_sold\").alias(\"total_scoops\")\n",
    "    )\n",
    "```\n",
    "\n",
    "## Formatting\n",
    "[black](https://github.com/psf/black), just use black \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<!-- ::: {.callout-note title=\"Tip with Title\" callout-appearance=\"simple\" icon=false collapse=\"true\"}\n",
    "Note that there are five types of callouts, including:\n",
    "`note`, `warning`, `important`, `tip`, and `caution`.\n",
    "::: -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit column selection over direct access, except to avoid disambiguation\n",
    "\n",
    "```python\n",
    "# bad\n",
    "df = df.select(F.lower(df1.colA), F.upper(df2.colB))\n",
    "\n",
    "# good\n",
    "df = df.select(F.lower(F.col('colA')), F.upper(F.col('colB')))\n",
    "\n",
    "# better - since Spark 3.0\n",
    "df = df.select(F.lower('colA'), F.upper('colB'))\n",
    "```\n",
    "\n",
    "Most situations favor avoiding the first and second styles in favor of referencing columns by name using strings, as shown in the third example. Spark 3.0 [greatly expanded](https://issues.apache.org/jira/browse/SPARK-26979) the scenarios where this approach works. However, when string references aren't feasible, a more verbose method is necessary.\n",
    "\n",
    "While the first style may appear simpler and less cluttered in some cases, it has notable drawbacks:\n",
    "\n",
    "- Long dataframe variable names lead to cumbersome expressions.\n",
    "- Column names with spaces or special characters require bracket notation, which isn't consistent with other methods. -> `df1['colA']` is just as difficult to write as `F.col('colA')`\n",
    "- Expressions involving dataframes names are not reusable and can not define abstract functions.\n",
    "- Renaming a dataframe variable demands updating all column references, increasing the risk of errors.\n",
    "\n",
    "Additionally, the dot syntax encourages the use of short and non-descriptive variable names for dataframes, which harms maintainability. Descriptive names help establish clear expectations about the data contained within dataframes.\n",
    "\n",
    "In contrast, `F.col('colA')` consistently references a column named `colA` in the supplied dataframe (`df`). This method eliminates the need to track other dataframes' states, making code more localized and less prone to \"spooky interaction at a distance,\" a common source of debugging challenges.\n",
    "\n",
    "## Caveats\n",
    "\n",
    "In some contexts there may be access to columns from more than one dataframe, and there may be an overlap in names. A common example is in matching expressions like `df.join(df2, on=(df.key == df2.key), how='left')`. In such cases it is fine to reference columns by their dataframe directly. You can also disambiguate joins using dataframe aliases (see more in the [Joins](#joins)  section in this guide).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Refactor complex logical operations\n",
    "\n",
    "Logical operations, which often reside inside `.filter()` or `F.when()`, need to be readable. Keep logic expressions inside the same code block to **three (3) expressions at most**. If they grow longer, it is often a sign that the code can and should be simplified or extracted out. Extracting out complex logical operations into variables makes the code easier to read and reason about, which also reduces bugs.\n",
    "\n",
    "```python\n",
    "# bad\n",
    "F.when( (F.col('prod_status') == 'Delivered') | (((F.datediff('deliveryDate_actual', 'current_date') < 0) & ((F.col('currentRegistration') != '') | ((F.datediff('deliveryDate_actual', 'current_date') < 0) & ((F.col('originalOperator') != '') | (F.col('currentOperator') != '')))))), 'In Service')\n",
    "```\n",
    "\n",
    "The code above can be simplified in different ways. To start, focus on grouping the logic steps in named variables. PySpark requires that expressions are wrapped with parentheses. This, mixed with actual parenthesis to group logical operations, can hurt readability. For example the code above has a redundant `(F.datediff(df.deliveryDate_actual, df.current_date) < 0)` that it very hard to spot.\n",
    "\n",
    "```python\n",
    "# better\n",
    "has_operator = ((F.col('originalOperator') != '') | (F.col('currentOperator') != ''))\n",
    "delivery_date_passed = (F.datediff('deliveryDate_actual', 'current_date') < 0)\n",
    "has_registration = (F.col('currentRegistration').rlike('.+'))\n",
    "is_delivered = (F.col('prod_status') == 'Delivered')\n",
    "\n",
    "F.when(is_delivered | (delivery_date_passed & (has_registration | has_operator)), 'In Service')\n",
    "```\n",
    "\n",
    "The above example drops the redundant expression and is easier to read. We can improve it further by reducing the number of operations.\n",
    "\n",
    "```python\n",
    "# good\n",
    "has_operator = (F.col(\"originalOperator\") != \"\") | (F.col(\"currentOperator\") != \"\")\n",
    "delivery_date_passed = F.datediff(\"deliveryDate_actual\", \"current_date\") < 0\n",
    "has_registration = F.col(\"currentRegistration\").rlike(\".+\")\n",
    "is_delivered = F.col(\"prod_status\") == \"Delivered\"\n",
    "is_active = has_registration | has_operator\n",
    "\n",
    "F.when(is_delivered | (delivery_date_passed & is_active), \"In Service\")\n",
    "\n",
    "```\n",
    "\n",
    "Note how the `F.when` expression is now concise and readable and the desired behavior is clear to anyone reviewing this code. The reader only needs to visit the individual expressions if they suspect there is an error. It also makes each chunk of logic easy to test if you have unit tests in your code, and want to abstract them as functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Use `select` statements to specify a schema contract\n",
    "\n",
    "Doing a select at the beginning of a PySpark transform, or before returning, specifies the contract with both the reader and the code about the expected dataframe schema for inputs and outputs.\n",
    "\n",
    "Keep select statements as simple as possible. Apply only *one* function from `spark.sql.function` to be used per selected column, plus an optional `.alias()` to give it a meaningful name. \n",
    "\n",
    "Expressions involving more than one dataframe, or conditional operations like `.when()` are discouraged to be used in a select, unless required for performance reasons.\n",
    "\n",
    "\n",
    "```python\n",
    "# bad\n",
    "aircraft = aircraft.select(\n",
    "    'aircraft_id',\n",
    "    'aircraft_msn',\n",
    "    F.col('aircraft_registration').alias('registration'),\n",
    "    'aircraft_type',\n",
    "    F.avg('staleness').alias('avg_staleness'),\n",
    "    F.col('number_of_economy_seats').cast('long'),\n",
    "    F.avg('flight_hours').alias('avg_flight_hours'),\n",
    "    'operator_code',\n",
    "    F.col('number_of_business_seats').cast('long'),\n",
    ")\n",
    "```\n",
    "\n",
    "Unless order matters to you, try to cluster together operations of the same type, reducing the cognitve load on the reader of the code.\n",
    "\n",
    "```python\n",
    "# good\n",
    "aircraft = aircraft.select(\n",
    "    \"aircraft_id\",\n",
    "    \"aircraft_msn\",\n",
    "    \"aircraft_type\",\n",
    "    \"operator_code\",\n",
    "    F.col(\"aircraft_registration\").alias(\"registration\"),\n",
    "    F.col(\"number_of_economy_seats\").cast(\"long\"),\n",
    "    F.col(\"number_of_business_seats\").cast(\"long\"),\n",
    "    F.avg(\"staleness\").alias(\"avg_staleness\"),\n",
    "    F.avg(\"flight_hours\").alias(\"avg_flight_hours\"),\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "The `select()` statement redefines the schema of a dataframe, so it naturally supports the inclusion or exclusion of columns, old and new, as well as the redefinition of pre-existing ones. By centralising all such operations in a single statement, it becomes much easier to identify the final schema, which makes code more readable. It also makes code more concise.\n",
    "\n",
    "Instead of calling `withColumnRenamed()`, use aliases:\n",
    "\n",
    "\n",
    "```python\n",
    "#bad\n",
    "df.select('key', 'comments').withColumnRenamed('comments', 'num_comments')\n",
    "\n",
    "# good\n",
    "df.select(\"key\", F.col(\"comments\").alias(\"num_comments\"))\n",
    "\n",
    "```\n",
    "\n",
    "Instead of using `withColumn()` to redefine type, cast in the select:\n",
    "```python\n",
    "# bad\n",
    "df.select('comments').withColumn('comments', F.col('comments').cast('double'))\n",
    "\n",
    "# good\n",
    "df.select(F.col(\"comments\").cast(\"double\"))\n",
    "\n",
    "```\n",
    "\n",
    "But keep it simple:\n",
    "```python\n",
    "# bad\n",
    "df.select(\n",
    "    ((F.coalesce(F.unix_timestamp('closed_at'), F.unix_timestamp())\n",
    "    - F.unix_timestamp('created_at')) / 86400).alias('days_open')\n",
    ")\n",
    "\n",
    "# good\n",
    "df.withColumn(\n",
    "    \"days_open\",\n",
    "    (\n",
    "        F.coalesce(F.unix_timestamp(\"closed_at\"), F.unix_timestamp())\n",
    "        - F.unix_timestamp(\"created_at\")\n",
    "    )\n",
    "    / 86400,\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "Avoid including columns in the select statement if they are going to remain unused and choose instead an explicit set of columns - this is a preferred alternative to using `.drop()` since it guarantees that schema mutations won't cause unexpected columns to bloat your dataframe. However, dropping columns isn't inherently discouraged in all cases; for instance, it is commonly appropriate to drop columns after joins since it is common for joins to introduce redundant columns. \n",
    "\n",
    "Finally, instead of adding new columns via the select statement, using `.withColumn()` is recommended instead for single columns. When adding or manipulating tens or hundreds of columns, use a single `.select()` for performance reasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Empty columns\n",
    "\n",
    "If you need to add an empty column to satisfy a schema, **always** use `F.lit(None)` for populating that column. Never use an empty string or some other string signalling an empty value (such as `NA`, this can be interpeted as 'North America').\n",
    "\n",
    "Beyond being semantically correct, one practical reason for using `F.lit(None)` is preserving the ability to use utilities like `isNull`, instead of having to verify empty strings, nulls, and `'NA'`, etc.\n",
    "\n",
    "\n",
    "```python\n",
    "# bad\n",
    "df = df.withColumn('foo', F.lit(''))\n",
    "\n",
    "# also bad\n",
    "df = df.withColumn('foo', F.lit('NA'))\n",
    "\n",
    "# good\n",
    "df = df.withColumn(\"foo\", F.lit(None))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Using comments\n",
    "\n",
    "While comments can provide useful insight into code, it is often more valuable to refactor the code to improve its readability and only use comments to explain the why or provide more context.\n",
    "\n",
    "```python\n",
    "# bad\n",
    "\n",
    "# Cast the timestamp columns\n",
    "cols = ['start_date', 'delivery_date']\n",
    "for c in cols:\n",
    "    df = df.withColumn(c, F.from_unixtime(F.col(c) / 1000).cast(T.TimestampType()))\n",
    "```\n",
    "\n",
    "In the example above, we can see that those columns are getting cast to Timestamp. The comment doesn't add much value. Moreover, a more verbose comment might still be unhelpful if it only provides information that already exists in the code. For example:\n",
    "\n",
    "```python\n",
    "# bad\n",
    "\n",
    "# Go through each column, divide by 1000 because millis and cast to timestamp\n",
    "cols = ['start_date', 'delivery_date']\n",
    "for c in cols:\n",
    "    df = df.withColumn(c, F.from_unixtime(F.col(c) / 1000).cast(T.TimestampType()))\n",
    "```\n",
    "\n",
    "Instead of leaving comments that only describe the logic you wrote, aim to leave comments that give context, that explain the **why** of decisions you made when writing the code. This is particularly important for PySpark, since the reader can understand your code, but often doesn't have context on the data that feeds into your PySpark transform. Small pieces of logic might have involved hours of digging👷 through data to understand the correct behavior, in which case comments explaining the rationale are especially valuable.\n",
    "\n",
    "```python\n",
    "# good\n",
    "\n",
    "# The consumer of this dataset expects a timestamp instead of a date, and we need\n",
    "# to adjust the time by 1000 because the original datasource is storing these as millis\n",
    "# even though the documentation says it's actually a date.\n",
    "cols = [\"start_date\", \"delivery_date\"]\n",
    "for c in cols:\n",
    "    df = df.withColumn(c, F.from_unixtime(F.col(c) / 1000).cast(T.TimestampType()))\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# UDFs (user defined functions)\n",
    "\n",
    "It is highly recommended to avoid UDFs in all situations, as they are dramatically less performant than native PySpark. In most situations, logic that seems to necessitate a UDF can be refactored to use only native PySpark functions.\n",
    "\n",
    "#TODO add a topic about pandas udf and how to structure\n",
    "\n",
    "\n",
    "If you are in the position that you would need to create a [`pandas_udf()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html) as they tend to be more performant than the [`udf()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html) variant.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "#bad\n",
    "F.udf()\n",
    "\n",
    "# good\n",
    "@F.pandas_udf(T.IntegerType())\n",
    "def slen(s: pd.Series) -> pd.Series:\n",
    "    return s.str.len()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Joins\n",
    "\n",
    "Be careful with joins! If you perform a left join, and the right side has multiple matches for a key, that row will be duplicated as many times as there are matches. This is called a \"join explosion\" and can dramatically bloat the output of your transforms job. Always double check your assumptions to see that the key you are joining on is unique, unless you are expecting the multiplication.\n",
    "\n",
    "Bad joins are the source of many tricky-to-debug issues. __Always__ pass the values by name, even if you are using the default values such as `(inner)`.\n",
    "\n",
    "\n",
    "```python\n",
    "# bad\n",
    "flights = flights.join(aircraft, 'aircraft_id')\n",
    "\n",
    "# also bad\n",
    "flights = flights.join(aircraft, 'aircraft_id', 'inner')\n",
    "\n",
    "# good\n",
    "flights = flights.join(other=aircraft, on=\"aircraft_id\", how=\"inner\")\n",
    "```\n",
    "\n",
    "Avoid `right` joins. If you are about to use a `right` join, switch the order of your dataframes and use a `left` join instead. It is more intuitive since the dataframe you are doing the operation on is the one that you are centering your join around.\n",
    "\n",
    "```python\n",
    "# bad\n",
    "flights = aircraft.join(flights, 'aircraft_id', how='right')\n",
    "\n",
    "# good\n",
    "flights = flights.join(other=aircraft, on=\"aircraft_id\", how=\"left\")\n",
    "\n",
    "```\n",
    "\n",
    "Avoid renaming all columns to avoid collisions. Instead, give an alias to the\n",
    "whole dataframe, and use that alias to select which columns you want in the end.\n",
    "\n",
    "```python\n",
    "# bad\n",
    "columns = ['start_time', 'end_time', 'idle_time', 'total_time']\n",
    "for col in columns:\n",
    "    flights = flights.withColumnRenamed(col, 'flights_' + col)\n",
    "    parking = parking.withColumnRenamed(col, 'parking_' + col)\n",
    "\n",
    "flights = flights.join(parking, on='flight_code', how='left')\n",
    "\n",
    "flights = flights.select(\n",
    "    F.col('flights_start_time').alias('flight_start_time'),\n",
    "    F.col('flights_end_time').alias('flight_end_time'),\n",
    "    F.col('parking_total_time').alias('client_parking_total_time')\n",
    ")\n",
    "\n",
    "# good\n",
    "flights = flights.alias(\"flights\")\n",
    "parking = parking.alias(\"parking\")\n",
    "\n",
    "flights = flights.join(other=parking, on=\"flight_code\", how=\"left\")\n",
    "\n",
    "flights = flights.select(\n",
    "    F.col(\"flights.start_time\").alias(\"flight_start_time\"),\n",
    "    F.col(\"flights.end_time\").alias(\"flight_end_time\"),\n",
    "    F.col(\"parking.total_time\").alias(\"client_parking_total_time\"),\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "In such cases, keep in mind:\n",
    "\n",
    "- It is a better idea to drop overlapping columns *before* joining if you don't need both\n",
    "- In case you do need both, it might be best to rename one of them prior to joining, signaling the difference between the two cols.\n",
    "- You should always resolve ambiguous columns before outputting a dataset. After the transform is finished running you can no longer distinguish them.\n",
    "\n",
    "\n",
    "## `.dropDuplicates()` and `.distinct()` to \"clean\" joins\n",
    "don't think about using `.dropDuplicates()` or `.distinct()` as a quick fix.  If unexpected duplicate rows are in your dataframe, there is always an underlying reason for why those duplicate rows appear. Adding `.dropDuplicates()` only masks this problem and adds unneccesary cpu cycles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Window Functions\n",
    "Always explicitly define three things when working with window functions:\n",
    "- `partitionBy`: The partitions or groups over which the window function will be applied.\n",
    "- `orderBy`: The ordering of rows within the partition, which can be based on time or rank.\n",
    "- [`rowsBetween`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.rowsBetween.html) or [`rangeBetween`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.rangeBetween.html#pyspark.sql.Window.rangeBetween): Defines the scope in rows or ranges considered when applying a function over the ordered partition.\n",
    "\n",
    "By specifying these three components, you ensure that your windows are properly defined.\n",
    "\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F, Window as W\n",
    "df = spark.createDataFrame(\n",
    "    [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"a\", 4)],\n",
    "    [\"key\", \"num\"],\n",
    ")\n",
    "\n",
    "# bad\n",
    "w1 = W.partitionBy('key')\n",
    "w2 = W.partitionBy('key').orderBy('num')\n",
    " \n",
    "df.select('key', F.sum('num').over(w1).alias('sum')).collect()\n",
    "# => [Row(key='a', sum=10), Row(key='a', sum=10), Row(key='a', sum=10), Row(key='a', sum=10)]\n",
    "\n",
    "df.select('key', F.sum('num').over(w2).alias('sum')).collect()\n",
    "# => [Row(key='a', sum=1), Row(key='a', sum=3), Row(key='a', sum=6), Row(key='a', sum=10)]\n",
    "\n",
    "df.select('key', F.first('num').over(w2).alias('first')).collect()\n",
    "# => [Row(key='a', first=1), Row(key='a', first=1), Row(key='a', first=1), Row(key='a', first=1)]\n",
    "\n",
    "df.select('key', F.last('num').over(w2).alias('last')).collect()\n",
    "# => [Row(key='a', last=1), Row(key='a', last=2), Row(key='a', last=3), Row(key='a', last=4)]\n",
    "```\n",
    "\n",
    "It is much safer to always specify an explicit window with the three components:\n",
    "```python\n",
    "# good\n",
    "w3 = (\n",
    "    W.partitionBy(\"key\")\n",
    "    .orderBy(\"num\")\n",
    "    .rowsBetween(\n",
    "        start=W.unboundedPreceding,\n",
    "        end=0, #<- zero means the current row, -1 is the row before the current\n",
    "    )\n",
    ")\n",
    "w4 = (\n",
    "    W.partitionBy(\"key\")\n",
    "    .orderBy(\"num\")\n",
    "    .rowsBetween(\n",
    "        start=W.unboundedPreceding,\n",
    "        end=W.unboundedFollowing,\n",
    "    )\n",
    ")\n",
    "\n",
    "df.select(\"key\", F.sum(\"num\").over(w3).alias(\"sum\")).collect()\n",
    "# [Row(key=\"a\", sum=1),\n",
    "#  Row(key=\"a\", sum=3),\n",
    "#  Row(key=\"a\", sum=6),\n",
    "#  Row(key=\"a\", sum=10),]\n",
    "\n",
    "\n",
    "df.select(\"key\", F.sum(\"num\").over(w4).alias(\"sum\")).collect()\n",
    "# [Row(key=\"a\", sum=10),\n",
    "#  Row(key=\"a\", sum=10),\n",
    "#  Row(key=\"a\", sum=10),\n",
    "#  Row(key=\"a\", sum=10),]\n",
    "\n",
    "\n",
    "df.select(\"key\", F.first(\"num\").over(w4).alias(\"first\")).collect()\n",
    "# [Row(key=\"a\", first=1),\n",
    "#  Row(key=\"a\", first=1),\n",
    "#  Row(key=\"a\", first=1),\n",
    "#  Row(key=\"a\", first=1),]\n",
    "\n",
    "\n",
    "df.select(\"key\", F.last(\"num\").over(w4).alias(\"last\")).collect()\n",
    "# [Row(key=\"a\", last=4),\n",
    "#  Row(key=\"a\", last=4),\n",
    "#  Row(key=\"a\", last=4),\n",
    "#  Row(key=\"a\", last=4),]\n",
    "```\n",
    "\n",
    "## Dealing with nulls\n",
    "\n",
    "While nulls are ignored for aggregate functions (like `F.sum()` and `F.max()`), they will impact the result of analytic functions (like `F.first()/F.last()` and `F.lead()/F.lag()`)\n",
    "\n",
    "```python\n",
    "df_nulls = spark.createDataFrame(\n",
    "    [(\"a\", None), (\"a\", 1), (\"a\", 2), (\"a\", None)],\n",
    "    [\"key\", \"num\"],\n",
    ")\n",
    "\n",
    "df_nulls.select(\"key\", F.first(\"num\").over(w4).alias(\"first\")).collect()\n",
    "# [Row(key=\"a\", first=None),\n",
    "#  Row(key=\"a\", first=None),\n",
    "#  Row(key=\"a\", first=None),\n",
    "#  Row(key=\"a\", first=None),]\n",
    "\n",
    "\n",
    "df_nulls.select(\"key\", F.last(\"num\").over(w4).alias(\"last\")).collect()\n",
    "# [Row(key=\"a\", last=None),\n",
    "#  Row(key=\"a\", last=None),\n",
    "#  Row(key=\"a\", last=None),\n",
    "#  Row(key=\"a\", last=None),]\n",
    "```\n",
    "\n",
    "Best to avoid this problem by enabling the `ignorenulls` flag:\n",
    "```python\n",
    "df_nulls.select(\n",
    "    \"key\", F.first(\"num\", ignorenulls=True).over(w4).alias(\"first\")\n",
    ").collect()\n",
    "# [Row(key=\"a\", first=1),\n",
    "#  Row(key=\"a\", first=1),\n",
    "#  Row(key=\"a\", first=1),\n",
    "#  Row(key=\"a\", first=1),]\n",
    "\n",
    "\n",
    "df_nulls.select(\"key\", F.last(\"num\", ignorenulls=True).over(w4).alias(\"last\")).collect()\n",
    "# [Row(key=\"a\", last=2),\n",
    "#  Row(key=\"a\", last=2),\n",
    "#  Row(key=\"a\", last=2),\n",
    "#  Row(key=\"a\", last=2),]\n",
    "```\n",
    "\n",
    "Also be mindful of explicit ordering of nulls to make sure the expected results are obtained:\n",
    "```python\n",
    "w5 = (\n",
    "    W.partitionBy(\"key\")\n",
    "    .orderBy(F.asc_nulls_first(\"num\"))\n",
    "    .rowsBetween(W.currentRow, W.unboundedFollowing)\n",
    ")\n",
    "w6 = (\n",
    "    W.partitionBy(\"key\")\n",
    "    .orderBy(F.asc_nulls_last(\"num\"))\n",
    "    .rowsBetween(W.currentRow, W.unboundedFollowing)\n",
    ")\n",
    "\n",
    "df_nulls.select(\"key\", F.lead(\"num\").over(w5).alias(\"lead\")).collect()\n",
    "# [ Row(key=\"a\", lead=None),\n",
    "#   Row(key=\"a\", lead=None),\n",
    "#   Row(key=\"a\", lead=1),\n",
    "#   Row(key=\"a\", lead=2),]\n",
    "\n",
    "\n",
    "df_nulls.select(\"key\", F.lead(\"num\").over(w6).alias(\"lead\")).collect()\n",
    "# [ Row(key=\"a\", lead=1),\n",
    "#   Row(key=\"a\", lead=2),\n",
    "#   Row(key=\"a\", lead=None),\n",
    "#   Row(key=\"a\", lead=None),]\n",
    "```\n",
    "\n",
    "## Empty `partitionBy()`\n",
    "\n",
    "Spark window functions can be applied over all rows, using a global frame. This is accomplished by specifying zero columns in the partition by expression (i.e. `W.partitionBy()`).\n",
    "\n",
    "Code like this should be avoided, however, as it forces Spark to combine all data into a single partition, which can be extremely harmful for performance.\n",
    "\n",
    "Prefer to use aggregations whenever possible:\n",
    "\n",
    "```python\n",
    "# bad\n",
    "w = W.partitionBy()\n",
    "df = df.select(F.sum('num').over(w).alias('sum'))\n",
    "\n",
    "# good\n",
    "df = df.agg(F.sum(\"num\").alias(\"sum\"))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Micro structure \n",
    "This is a style guide with an opinion, and here are some recommendation on how to strucutre your scripts and what is the best way of applying transformations to your data.\n",
    "\n",
    "1. start with loading the specific data that you want, by combining `select` and `where` -> repeat for all data sources \n",
    "1. select a base table(this should be the largest table) and **left** join all other tables\n",
    "1. perform aggregations or window transformations\n",
    "1. post-processing transformations\n",
    "\n",
    "If you utilize a functional approach, then you can use [`transform()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.transform.html?highlight=transform#pyspark.sql.DataFrame.transform) method to chain multiple operations together, keep in mind that the function supplied to `transform()` should accept and return a dataframe. this structure will force you to write function that are ready for unit testing. \n",
    "\n",
    "## making transformation functions \n",
    "this can be achieved in three ways:\n",
    "\n",
    "### wrapper function\n",
    "```py\n",
    "def transform_wrapper(df):\n",
    "    return f(df, specific_arg1, specific_arg2)\n",
    "df = df.transform(transform_wrapper)\n",
    "```\n",
    "### lambda function\n",
    "```py\n",
    "df = df.transform(lambda df: f(df, specific_arg1, specific_arg2))\n",
    "```\n",
    "### partial function\n",
    "```py\n",
    "from functools import partial\n",
    "f_par = partial(f, arg1=value1, arg2=value2)\n",
    "df = df.transform(f_par)\n",
    "```\n",
    " \n",
    "\n",
    "\n",
    "Chaining expressions is a contentious topic, however, since this is an opinionated guide, we are opting to recommend some limits on the usage of chaining. See the conclusion of this section for a discussion of the rationale behind this recommendation.\n",
    "\n",
    "Avoid chaining of expressions into multi-line expressions with different types, particularly if they have different behaviours or contexts. For example- mixing column creation or joining with selecting and filtering.\n",
    "\n",
    "```python\n",
    "# bad\n",
    "df = (\n",
    "    df\n",
    "    .select('a', 'b', 'c', 'key')\n",
    "    .filter(F.col('a') == 'truthiness')\n",
    "    .withColumn('boverc', F.col('b') / F.col('c'))\n",
    "    .join(df2, 'key', how='inner')\n",
    "    .join(df3, 'key', how='left')\n",
    "    .drop('c')\n",
    ")\n",
    "\n",
    "# better (seperating into steps)\n",
    "# first: we select and trim down the data that we need\n",
    "# second: we create the columns that we need to have\n",
    "# third: joining with other dataframes\n",
    "\n",
    "df = df.select(\"a\", \"b\", \"c\", \"key\").filter(F.col(\"a\") == \"truthiness\")\n",
    "df = df.withColumn(\"boverc\", F.col(\"b\") / F.col(\"c\"))\n",
    "df = df.join(df2, \"key\", how=\"inner\").join(df3, \"key\", how=\"left\").drop(\"c\")\n",
    "\n",
    "```\n",
    "\n",
    "Having each group of expressions isolated into its own logical code block improves legibility and makes it easier to find relevant logic.\n",
    "For example, a reader of the code below will probably jump to where they see dataframes being assigned `df = df...`.\n",
    "\n",
    "```python\n",
    "# bad\n",
    "df = (\n",
    "    df\n",
    "    .select('foo', 'bar', 'foobar', 'abc')\n",
    "    .filter(F.col('abc') == 123)\n",
    "    .join(another_table, 'some_field')\n",
    ")\n",
    "\n",
    "# better\n",
    "df = df.select(\"foo\", \"bar\", \"foobar\", \"abc\").filter(F.col(\"abc\") == 123)\n",
    "df = df.join(other=another_table, on=\"some_field\", how=\"inner\")\n",
    "\n",
    "```\n",
    "\n",
    "There are legitimate reasons to chain expressions together. These commonly represent atomic logic steps, and are acceptable. Apply a rule with a maximum of number chained expressions in the same block to keep the code readable.\n",
    "We recommend chains of no longer than 3 statements.\n",
    "\n",
    "If you find you are making longer chains, or having trouble because of the size of your variables, consider extracting the logic into a separate function:\n",
    "\n",
    "```python\n",
    "# bad\n",
    "customers_with_shipping_address = (\n",
    "    customers_with_shipping_address\n",
    "    .select('a', 'b', 'c', 'key')\n",
    "    .filter(F.col('a') == 'truthiness')\n",
    "    .withColumn('boverc', F.col('b') / F.col('c'))\n",
    "    .join(df2, 'key', how='inner')\n",
    ")\n",
    "\n",
    "# also bad\n",
    "customers_with_shipping_address = customers_with_shipping_address.select('a', 'b', 'c', 'key')\n",
    "customers_with_shipping_address = customers_with_shipping_address.filter(F.col('a') == 'truthiness')\n",
    "customers_with_shipping_address = customers_with_shipping_address.withColumn('boverc', F.col('b') / F.col('c'))\n",
    "customers_with_shipping_address = customers_with_shipping_address.join(df2, 'key', how='inner')\n",
    "\n",
    "# better\n",
    "def join_customers_with_shipping_address(customers, df_to_join):\n",
    "    customers = customers.select(\"a\", \"b\", \"c\", \"key\").filter(\n",
    "        F.col(\"a\") == \"truthiness\"\n",
    "    )\n",
    "    customers = customers.withColumn(\"boverc\", F.col(\"b\") / F.col(\"c\"))\n",
    "    return customers.join(other=df_to_join, on=\"key\", how=\"inner\")\n",
    "```\n",
    "\n",
    "Chains of more than 3 statement are prime candidates to factor into separate, well-named functions since they are already encapsulated, isolated blocks of logic.\n",
    "\n",
    "The rationale for why we've set these limits on chaining:\n",
    "\n",
    "- Differentiation between PySpark code and SQL code. Chaining is something that goes against most, if not all, other Python styling. You don’t chain in Python, you assign.\n",
    "- Discourage the creation of large single code blocks. These would often make more sense extracted as a named function.\n",
    "- It doesn’t need to be all or nothing, but a maximum of five lines of chaining balances practicality with legibility.\n",
    "- If you are using an IDE, it makes it easier to use automatic extractions or do code movements \n",
    "- Large chains are hard to read and maintain, particularly if chains are nested.\n",
    "\n",
    "\n",
    "##TODO \n",
    "# Good\n",
    "flavor_analysis = (\n",
    "    ice_cream_sales\n",
    "    .transform(clean_sales_data)\n",
    "    .transform(add_price_category)\n",
    "    .transform(calculate_flavor_metrics)\n",
    ")\n",
    "\n",
    "# Avoid: Procedural approach with intermediate DataFrames\n",
    "clean_sales = clean_sales_data(ice_cream_sales)\n",
    "categorized_sales = add_price_category(clean_sales)\n",
    "flavor_analysis = calculate_flavor_metrics(categorized_sales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Other Considerations and Recommendations\n",
    "\n",
    "1. Be wary of functions that grow too large. As a general rule, a file\n",
    "    should not be over 250 lines, and a function should not be over 70 lines.\n",
    "2. Try to keep your code in logical blocks. For example, if you have\n",
    "    multiple lines referencing the same things, try to keep them\n",
    "    together. Separating them reduces context and readability.\n",
    "3. Test your code! If you *can* run the local tests, do so and make\n",
    "    sure that your new code is covered by the tests. If you can't run\n",
    "    the local tests, build the datasets on your branch and manually\n",
    "    verify that the data looks as expected.\n",
    "4. Avoid `.otherwise(value)` as a general fallback. If you are mapping\n",
    "    a list of keys to a list of values and a number of unknown keys appear,\n",
    "    using `otherwise` will mask all of these into one value.\n",
    "5. Do not keep commented out code checked in the repository. This applies\n",
    "    to single line of codes, functions, classes or modules. Rely on git\n",
    "    and its capabilities of branching or looking at history instead.\n",
    "6. When encountering a large single transformation composed of integrating multiple different source tables, split it into the natural sub-steps and extract the logic to functions. This allows for easier higher level readability and allows for code re-usability and consistency between transforms.\n",
    "7. Try to be as explicit and descriptive as possible when naming functions\n",
    "    or variables. Strive to capture what the function is actually doing\n",
    "    as opposed to naming it based the objects used inside of it.\n",
    "8. Think twice about introducing new import aliases, unless there is a good\n",
    "    reason to do so. Some of the established ones are `types` and `functions` from PySpark `from pyspark.sql import types as T, functions as F`.\n",
    "9. Avoid using literal strings or integers in filtering conditions, new\n",
    "    values of columns etc. Instead, to capture their meaning, extract them into variables, constants,\n",
    "    dicts or classes as suitable. This makes the\n",
    "    code more readable and enforces consistency across the repository.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
