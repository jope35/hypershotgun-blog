{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Advanced Pyspark ⚡️ style guide\"\n",
    "author: \"Joost de Theije + LLM\"\n",
    "subtitle: \"  \"\n",
    "date: \"\"\n",
    "image: \"artifacts/splash.png\"\n",
    "abstract: \"PySpark is a wrapper language that allows users to interface with an Apache Spark backend to quickly process data.\n",
    "Spark can operate on massive datasets across a distributed network of servers, providing major performance and reliability benefits when utilized correctly.\n",
    "It presents challenges, even for experienced Python developers, as the PySpark syntax draws on the JVM heritage of Spark and therefore implements code patterns that may be unfamiliar.\n",
    "Adapted from [here](https://github.com/palantir/pyspark-style-guide) and [here](https://github.com/axelearning/pyspark-style-guide)\"\n",
    "format:\n",
    "  html: default\n",
    "draft: true\n",
    "code-line-numbers: false\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "# Micro structure \n",
    "This is a style guide with an opinion, and here are some recommendation on how to strucutre your scripts and what is the best way of applying transformations to your data.\n",
    "\n",
    "1. start with loading the specific data that you want, by combining `select` and `where` -> repeat for all data sources \n",
    "1. select a base table(this should be the largest table) and **left** join all other tables\n",
    "1. perform aggregations or window transformations\n",
    "1. post-processing transformations\n",
    "\n",
    "If you utilize a functional approach, then you can use [`transform()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.transform.html?highlight=transform#pyspark.sql.DataFrame.transform) method to chain multiple operations together, keep in mind that the function supplied to `transform()` should accept and return a dataframe. this structure will force you to write function that are ready for unit testing. \n",
    "\n",
    "## making transformation functions \n",
    "this can be achieved in three ways:\n",
    "\n",
    "### wrapper function\n",
    "```py\n",
    "def transform_wrapper(df):\n",
    "    return f(df, specific_arg1, specific_arg2)\n",
    "df = df.transform(transform_wrapper)\n",
    "```\n",
    "### lambda function\n",
    "```py\n",
    "df = df.transform(lambda df: f(df, specific_arg1, specific_arg2))\n",
    "```\n",
    "### partial function\n",
    "```py\n",
    "from functools import partial\n",
    "f_par = partial(f, arg1=value1, arg2=value2)\n",
    "df = df.transform(f_par)\n",
    "```\n",
    " \n",
    "\n",
    "\n",
    "Chaining expressions is a contentious topic, however, since this is an opinionated guide, we are opting to recommend some limits on the usage of chaining. See the conclusion of this section for a discussion of the rationale behind this recommendation.\n",
    "\n",
    "Avoid chaining of expressions into multi-line expressions with different types, particularly if they have different behaviours or contexts. For example- mixing column creation or joining with selecting and filtering.\n",
    "\n",
    "```python\n",
    "# bad\n",
    "df = (\n",
    "    df\n",
    "    .select('a', 'b', 'c', 'key')\n",
    "    .filter(F.col('a') == 'truthiness')\n",
    "    .withColumn('boverc', F.col('b') / F.col('c'))\n",
    "    .join(df2, 'key', how='inner')\n",
    "    .join(df3, 'key', how='left')\n",
    "    .drop('c')\n",
    ")\n",
    "\n",
    "# better (seperating into steps)\n",
    "# first: we select and trim down the data that we need\n",
    "# second: we create the columns that we need to have\n",
    "# third: joining with other dataframes\n",
    "\n",
    "df = df.select(\"a\", \"b\", \"c\", \"key\").filter(F.col(\"a\") == \"truthiness\")\n",
    "df = df.withColumn(\"boverc\", F.col(\"b\") / F.col(\"c\"))\n",
    "df = df.join(df2, \"key\", how=\"inner\").join(df3, \"key\", how=\"left\").drop(\"c\")\n",
    "\n",
    "```\n",
    "\n",
    "Having each group of expressions isolated into its own logical code block improves legibility and makes it easier to find relevant logic.\n",
    "For example, a reader of the code below will probably jump to where they see dataframes being assigned `df = df...`.\n",
    "\n",
    "```python\n",
    "# bad\n",
    "df = (\n",
    "    df\n",
    "    .select('foo', 'bar', 'foobar', 'abc')\n",
    "    .filter(F.col('abc') == 123)\n",
    "    .join(another_table, 'some_field')\n",
    ")\n",
    "\n",
    "# better\n",
    "df = df.select(\"foo\", \"bar\", \"foobar\", \"abc\").filter(F.col(\"abc\") == 123)\n",
    "df = df.join(other=another_table, on=\"some_field\", how=\"inner\")\n",
    "\n",
    "```\n",
    "\n",
    "There are legitimate reasons to chain expressions together. These commonly represent atomic logic steps, and are acceptable. Apply a rule with a maximum of number chained expressions in the same block to keep the code readable.\n",
    "We recommend chains of no longer than 3 statements.\n",
    "\n",
    "If you find you are making longer chains, or having trouble because of the size of your variables, consider extracting the logic into a separate function:\n",
    "\n",
    "```python\n",
    "# bad\n",
    "customers_with_shipping_address = (\n",
    "    customers_with_shipping_address\n",
    "    .select('a', 'b', 'c', 'key')\n",
    "    .filter(F.col('a') == 'truthiness')\n",
    "    .withColumn('boverc', F.col('b') / F.col('c'))\n",
    "    .join(df2, 'key', how='inner')\n",
    ")\n",
    "\n",
    "# also bad\n",
    "customers_with_shipping_address = customers_with_shipping_address.select('a', 'b', 'c', 'key')\n",
    "customers_with_shipping_address = customers_with_shipping_address.filter(F.col('a') == 'truthiness')\n",
    "customers_with_shipping_address = customers_with_shipping_address.withColumn('boverc', F.col('b') / F.col('c'))\n",
    "customers_with_shipping_address = customers_with_shipping_address.join(df2, 'key', how='inner')\n",
    "\n",
    "# better\n",
    "def join_customers_with_shipping_address(customers, df_to_join):\n",
    "    customers = customers.select(\"a\", \"b\", \"c\", \"key\").filter(\n",
    "        F.col(\"a\") == \"truthiness\"\n",
    "    )\n",
    "    customers = customers.withColumn(\"boverc\", F.col(\"b\") / F.col(\"c\"))\n",
    "    return customers.join(other=df_to_join, on=\"key\", how=\"inner\")\n",
    "```\n",
    "\n",
    "Chains of more than 3 statement are prime candidates to factor into separate, well-named functions since they are already encapsulated, isolated blocks of logic.\n",
    "\n",
    "The rationale for why we've set these limits on chaining:\n",
    "\n",
    "- Differentiation between PySpark code and SQL code. Chaining is something that goes against most, if not all, other Python styling. You don’t chain in Python, you assign.\n",
    "- Discourage the creation of large single code blocks. These would often make more sense extracted as a named function.\n",
    "- It doesn’t need to be all or nothing, but a maximum of five lines of chaining balances practicality with legibility.\n",
    "- If you are using an IDE, it makes it easier to use automatic extractions or do code movements \n",
    "- Large chains are hard to read and maintain, particularly if chains are nested.\n",
    "\n",
    "\n",
    "##TODO \n",
    "# Good\n",
    "flavor_analysis = (\n",
    "    ice_cream_sales\n",
    "    .transform(clean_sales_data)\n",
    "    .transform(add_price_category)\n",
    "    .transform(calculate_flavor_metrics)\n",
    ")\n",
    "\n",
    "# Avoid: Procedural approach with intermediate DataFrames\n",
    "clean_sales = clean_sales_data(ice_cream_sales)\n",
    "categorized_sales = add_price_category(clean_sales)\n",
    "flavor_analysis = calculate_flavor_metrics(categorized_sales)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
