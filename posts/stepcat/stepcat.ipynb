{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"some points are more or less random than others\"\n",
    "author: \"Joost de Theije + LLM\"\n",
    "subtitle: \"stepwise tuning of gradient boosting models\"\n",
    "date: \"03/14/2024\"\n",
    "image: \"artifacts/ice_lines.gif\"\n",
    "abstract: \"i am an abstract piece of arts\"\n",
    "format:\n",
    "  html: default\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: hide\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from scipy.stats import qmc\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: hide\n",
    "\n",
    "set_matplotlib_formats(\"svg\")\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_context(context=\"notebook\", font_scale=1.5)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "FIGSIZE = (12, 6)\n",
    "N_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "We have all encountered this predicament. The moment when we believe our features are satisfactory, and we're ready to train our first model. Initially, we begin with the default settings of the model, which yields acceptable performance. However, to achieve superior results, we determine that hyperparameter tuning is necessary. Referring to the documentation of [Scikit-Learn](https://scikit-learn.org/stable/modules/classes.html#module-model_selection), there are several options: GridSearchCV and RandomizedSearchCV are the most common choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch\n",
    "\n",
    "GridSearchCV is a hyperparameter tuning method in Scikit-Learn that uses exhaustive search to find the best combination of estimator parameters. It tries all possible combinations of specified parameter values using cross-validation to evaluate each set. Pros include exhaustive exploration and handling various methods, providing detailed performance information. However, it is time-consuming and memory-intensive, as it tests all combinations. The choice of cross-validation method and number of folds significantly impacts results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearch\n",
    "\n",
    "RandomizedSearchCV is a hyperparameter tuning method in Scikit-Learn that uses random sampling to search for the best combination of estimator parameters. Unlike GridSearchCV, which tries all possible combinations, it samples a fixed number of parameter settings from specified distributions. It is more efficient than exhaustive search but may not find the absolute best set of parameters. Cross-validation is used to evaluate each sampled set, and continuous or categorical distributions can be specified. The choice of distributions and iterations impacts results, as well as the cross-validation method and number of folds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something in between\n",
    "\n",
    "There are techniques called Latin Hypercube Sampling (LHS) or Poisson Discs sampling that mix grid and random sampling together.\n",
    "\n",
    "Latin Hypercube Sampling is a deterministic, space-filling design technique that selects a representative sample from the parameter space by ensuring that every hyperrectangle enclosing each unique combination of parameters contains at least one sample. In simpler terms, LHS aims to provide a representative and uniform distribution of samples across the entire range of possible combinations.\n",
    "\n",
    "Consider the analogy of a Sudoku puzzle. Each square on the board represents a unique combination of parameters. While you may be tempted to reveal all the numbers in one row or column, this would be an unwise move as it would limit the overall usefulness of your picks. LHS offers a framework for reducing the number of picks while maximizing their effectiveness, if you have selected a square from one row it would favor other rows more, the same logic applies to the columns and sub-squares.\n",
    "\n",
    "![](artifacts/sudoku.png)\n",
    "\n",
    "In conclusion, when choosing between hyperparameter tuning methods, it is essential to consider factors like computational resources, time constraints, and the complexity of the problem at hand. GridSearchCV provides exhaustive exploration but comes with higher computational costs, while RandomizedSearchCV offers flexibility and efficiency but may not find the absolute best combination of parameters. Latin Hypercube Sampling sampling offer a balance between these two extremes by providing representative samples while being more computationally efficient than GridSearchCV for large search spaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visual represantation\n",
    "\n",
    "To illustrate the distinction between the three sampling methods, let's consider a hypothetical scenario in a two-dimensional space, spanning from 0 to 1 inclusively and apply the three sampling strategies random, lhs, and grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: show\n",
    "np.random.seed(42)\n",
    "\n",
    "uni_sample = np.random.uniform(0, 1, (100, 2))\n",
    "\n",
    "sampler = qmc.LatinHypercube(d=2, optimization=\"lloyd\")\n",
    "lhs_sample = sampler.random(n=100)\n",
    "\n",
    "grid_sample = np.array(\n",
    "    [[i, j] for i in np.linspace(0, 1, 10) for j in np.linspace(0, 1, 10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: hide\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "\n",
    "curr_ax = ax[0]\n",
    "sns.histplot(\n",
    "    x=uni_sample[:, 0], y=uni_sample[:, 1], bins=10, ax=curr_ax, cbar=True, thresh=None\n",
    ")\n",
    "sns.scatterplot(x=uni_sample[:, 0], y=uni_sample[:, 1], ax=curr_ax, color=\"tab:orange\")\n",
    "curr_ax.set_xticks([])\n",
    "curr_ax.set_yticks([])\n",
    "curr_ax.set_title(\"uniform sampling\")\n",
    "\n",
    "\n",
    "curr_ax = ax[1]\n",
    "sns.histplot(\n",
    "    x=lhs_sample[:, 0], y=lhs_sample[:, 1], bins=10, ax=curr_ax, cbar=True, thresh=None\n",
    ")\n",
    "sns.scatterplot(x=lhs_sample[:, 0], y=lhs_sample[:, 1], ax=curr_ax, color=\"tab:orange\")\n",
    "curr_ax.set_xticks([])\n",
    "curr_ax.set_yticks([])\n",
    "curr_ax.set_title(\"latin hypercube sampling\")\n",
    "\n",
    "\n",
    "curr_ax = ax[2]\n",
    "sns.histplot(\n",
    "    x=grid_sample[:, 0],\n",
    "    y=grid_sample[:, 1],\n",
    "    bins=10,\n",
    "    ax=curr_ax,\n",
    "    cbar=True,\n",
    "    thresh=None,\n",
    ")\n",
    "sns.scatterplot(\n",
    "    x=grid_sample[:, 0], y=grid_sample[:, 1], ax=curr_ax, color=\"tab:orange\"\n",
    ")\n",
    "curr_ax.set_xticks([])\n",
    "curr_ax.set_yticks([])\n",
    "curr_ax.set_title(\"grid sampling\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply lhs to a real application\n",
    "\n",
    "Lets put this knowledge into pratice by performing some hyper-parameter tuning of a catboost model on a [dataset](https://www.openml.org/search?type=data&status=active&id=1461)\n",
    "\n",
    "The dataset is about direct marketing campaigns of a Portuguese banking institution, focusing on phone calls to promote term deposits. The data includes 17 input variables and one output variable. Input attributes consist of client demographics such as age, job type, marital status, education level, credit default status, average yearly balance, housing loan status, personal loan status, contact communication type, last contact day and month, duration of contact, number of contacts during the current campaign, number of days since last contact from a previous campaign, number of contacts before the current campaign, and the outcome of the previous marketing campaign. The goal is to predict if a client will subscribe to a term deposit based on these features.\n",
    "\n",
    "This dataset is a binary classification problem where the goal is to predict if a client will subscribe to a term deposit based on multiple categorical and numerical features, making it an excellent use case for CatBoost, a gradient boosting library specifically designed to handle categorical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset\n",
    "\n",
    "We can utilize the [`fetch_openml`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml) function from scikit-learn to automatically fetch the appropriate dataset. Following this, the data will be split into train and test sets with stratification controlled on the target variable. That is, the distribution of the target variable (y) is maintained equal for both the training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: show\n",
    "\n",
    "# fetch data set\n",
    "ID = 1461\n",
    "X, y = fetch_openml(\n",
    "    data_id=ID,\n",
    "    data_home=f\"openml_download_{ID}\",\n",
    "    return_X_y=True,\n",
    ")\n",
    "\n",
    "# downsample and create a stratified train test split\n",
    "X = X.sample(frac=0.25)  # control the size of the dataset\n",
    "X = X.dropna(axis=0, how=\"any\")\n",
    "\n",
    "# y = y.astype(int) - 1\n",
    "y = y.loc[X.index]  # align the datasets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## info on dataset\n",
    "\n",
    "As mentioned earlier, the dataset comprises 9 categorical and 7 numerical features. This configuration makes the dataset well-suited for using CatBoost. CatBoost employs a specialized technique called \"ordered target encoding\" that converts categorical features into numerical values, rendering them amenable to gradient boosting models. This approach can result in enhanced model performance and accuracy compared to other machine learning algorithms, particularly when managing datasets replete with a substantial number of categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: show\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating data pools\n",
    "\n",
    "CatBoost provides an option to create pools of data. This feature allows CatBoost to optimize the handling of large files internally. In this specific case, defining categorical features in one place and being able to reuse them later is a benefit for improving reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: show\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    "    cat_features=list(X.select_dtypes(include=\"category\").columns),\n",
    ")\n",
    "\n",
    "test_pool = Pool(\n",
    "    data=X_test,\n",
    "    label=y_test,\n",
    "    cat_features=list(X.select_dtypes(include=\"category\").columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Three Sampling Techniques: Grid, Random, and LHS\n",
    "\n",
    "In the following section, we apply and compare the results of the three sampling techniques: grid search, random search, and Latin Hypercube Sampling (LHS). The objective is to demonstrate that LHS explores more of the design space than random search while maintaining enough randomness to mitigate the symmetry issues inherent in grid search.\n",
    "\n",
    "The search space is defined by the following parameters.\n",
    "\n",
    "- depth $[1 \\dotsc 15]$\n",
    "- iterations $[1 \\dotsc 1024]$\n",
    "- subsample $[0.1,1]$\n",
    "- bagging_temperatur $[1 \\dotsc 1e6]$\n",
    "\n",
    "The number of samples used is 100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search\n",
    "\n",
    "To adhere to the limit of 100 samples, we use $100^{1/4}\\approx 3$ this results in $3^4=81$ samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: show\n",
    "\n",
    "n_grid_samples = np.power(N_SAMPLES, 1 / 4)\n",
    "n_grid_samples = np.floor(n_grid_samples).astype(int)\n",
    "\n",
    "param_grid = {\n",
    "    \"depth\": np.linspace(1, 15, n_grid_samples, dtype=int),\n",
    "    \"iterations\": np.linspace(1, 1024, n_grid_samples, dtype=int),\n",
    "    \"subsample\": np.linspace(0.1, 1, n_grid_samples, endpoint=False),\n",
    "    \"bagging_temperature\": np.linspace(1, 1000000, n_grid_samples, dtype=int),\n",
    "}\n",
    "\n",
    "param_combo = list(ParameterGrid(param_grid=param_grid))\n",
    "\n",
    "cube_search = []\n",
    "for param in tqdm(param_combo):\n",
    "    cbc = CatBoostClassifier(\n",
    "        **param,\n",
    "        eval_metric=\"F1\",\n",
    "    )\n",
    "    cbc.fit(train_pool, eval_set=(test_pool), verbose=False)\n",
    "\n",
    "    score = cbc.get_best_score().get(\"validation\").get(\"F1\")\n",
    "    param[\"score\"] = score\n",
    "    param[\"iterations\"] = cbc.get_best_iteration()\n",
    "\n",
    "    cube_search.append(param)\n",
    "grid_results = pd.DataFrame(cube_search).sort_values(by=\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search\n",
    "\n",
    "for depth and iterations the model excpects integers, so `randint` is used and for subsample and bagging_temperature `uniform` is used, as the model accepts float values for those.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: show\n",
    "random_sample = np.hstack(\n",
    "    (\n",
    "        np.random.randint(1, 15, (N_SAMPLES, 1)),\n",
    "        np.random.randint(1, 1024, (N_SAMPLES, 1)),\n",
    "        np.random.uniform(0, 1, (N_SAMPLES, 1)),\n",
    "        np.random.uniform(1, 1_000_000, (N_SAMPLES, 1)),\n",
    "    )\n",
    ")\n",
    "\n",
    "param_combo = list(\n",
    "    pd.DataFrame(\n",
    "        data=random_sample,\n",
    "        columns=[\"depth\", \"iterations\", \"subsample\", \"bagging_temperature\"],\n",
    "    )\n",
    "    .to_dict(\"index\")\n",
    "    .values()\n",
    ")\n",
    "\n",
    "print(param_combo)\n",
    "\n",
    "cube_search = []\n",
    "for param in tqdm(param_combo):\n",
    "    cbc = CatBoostClassifier(\n",
    "        **param,\n",
    "        eval_metric=\"F1\",\n",
    "    )\n",
    "    cbc.fit(train_pool, eval_set=(test_pool), verbose=False)\n",
    "\n",
    "    score = cbc.get_best_score().get(\"validation\").get(\"F1\")\n",
    "    param[\"score\"] = score\n",
    "    param[\"iterations\"] = cbc.get_best_iteration()\n",
    "\n",
    "    cube_search.append(param)\n",
    "random_results = pd.DataFrame(cube_search).sort_values(by=\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: show\n",
    "\n",
    "random_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LHS search\n",
    "\n",
    "when applying the LHS technqiue you need to specify how many dimensions the resulting sample should have and how many samples you want. these are on a different scale than the searchspace that you want to explore, so a scaling is applied to get it to the desired space. afterwhich a type conversion is applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: show\n",
    "sampler = qmc.LatinHypercube(d=4, optimization=\"lloyd\")\n",
    "sample = sampler.random(n=N_SAMPLES)\n",
    "\n",
    "l_bounds = [1, 1, 0, 1]\n",
    "u_bounds = [15, 1024, 1, 1000000]\n",
    "sample_scaled = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "# convert to int\n",
    "sample_scaled[:, :2] = sample_scaled[:, :2].astype(int)\n",
    "\n",
    "\n",
    "param_combo = list(\n",
    "    pd.DataFrame(\n",
    "        data=sample_scaled,\n",
    "        columns=[\"depth\", \"iterations\", \"subsample\", \"bagging_temperature\"],\n",
    "    )\n",
    "    .to_dict(\"index\")\n",
    "    .values()\n",
    ")\n",
    "\n",
    "cube_search = []\n",
    "for param in tqdm(param_combo):\n",
    "    cbc = CatBoostClassifier(\n",
    "        **param,\n",
    "        eval_metric=\"F1\",\n",
    "    )\n",
    "    cbc.fit(train_pool, eval_set=(test_pool), verbose=False)\n",
    "\n",
    "    score = cbc.get_best_score().get(\"validation\").get(\"F1\")\n",
    "    param[\"score\"] = score\n",
    "    param[\"iterations\"] = cbc.get_best_iteration()\n",
    "\n",
    "    cube_search.append(param)\n",
    "lhs_results = pd.DataFrame(cube_search).sort_values(by=\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: show\n",
    "\n",
    "lhs_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare the three results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: show\n",
    "\n",
    "df_all_sample = pd.concat(\n",
    "    [\n",
    "        lhs_results.assign(sampling=\"lhs\"),\n",
    "        random_results.assign(sampling=\"random\"),\n",
    "        grid_results.assign(sampling=\"grid\"),\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "df_all_sample = df_all_sample.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "df_all_sample.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Optuna hyperparameter optimization framework\n",
    "\n",
    "[Optuna](https://optuna.readthedocs.io/en/stable/)\n",
    "bayes hyperparameter tuning, tradeoff between explotation and exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: show\n",
    "\n",
    "\n",
    "def obj_all(trial) -> float:\n",
    "    params = {\n",
    "        \"depth\": trial.suggest_int(name=\"depth\", low=1, high=15),\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 1, 1024),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0, 1.0),\n",
    "        \"bagging_temperature\": trial.suggest_float(\n",
    "            \"bagging_temperature\", 1e-10, 1_000_000, log=True\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    cbc = CatBoostClassifier(\n",
    "        **params,\n",
    "        eval_metric=\"F1\",\n",
    "    )\n",
    "    cbc.fit(train_pool, eval_set=(test_pool), verbose=False)\n",
    "\n",
    "    return cbc.get_best_score().get(\"validation\").get(\"F1\")\n",
    "\n",
    "\n",
    "print(\"-\" * 10)\n",
    "\n",
    "start = time.time()\n",
    "study = optuna.create_study(study_name=\"all-in\", direction=\"maximize\")\n",
    "study.optimize(\n",
    "    obj_all,\n",
    "    n_trials=N_SAMPLES,\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(\"-\" * 10)\n",
    "print(f\"time all-in: {end - start:.2f} sec\")\n",
    "print(f\"{study.best_params=}\")\n",
    "print(f\"{study.best_value=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypershotgun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
